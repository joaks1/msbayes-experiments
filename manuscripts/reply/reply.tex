%&<latex>
\documentclass[letterpaper,12pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% preamble %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{../utils/preamble.tex}
\input{../utils/macros.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\doublespacing
\raggedright
\setlength{\parindent}{0.5in}
\begin{linenumbers}

\begin{titlepage}
    \begin{flushleft}
        \sffamily

        \MakeUppercase{\large\bfseries Why you should not fix a biased
        model-choice method by adding an additional dimension of model choice:
        A reply to Hickerson et al.}

        \vspace{12pt}
        \textbf{Running head:} \MakeUppercase{Approximate Bayesian model
        choice}

        \vspace{12pt}
        Jamie R.\ Oaks$^{1,2}$, Jeet Sukumaran$^{3}$, Jacob A.\
        Esselstyn$^{4}$, Charles W.\ Linkem$^{5}$, Cameron D.\
        Siler$^{6}$, Mark T.\ Holder$^{1}$ and Rafe M.\ Brown$^{1}$

        \bigskip
        $^1$\emph{Department of Ecology and Evolutionary Biology,
            Biodiversity Institute,
            University of Kansas,
            Lawrence, Kansas 66045}\\[.1in]
        $^3$\emph{Department of Biology,
            Duke University,
            Durham, North Carolina 27708} \\[.1in]
        $^4$\emph{Museum of Natural Science,
            Louisiana State University,
            119 Foster Hall,
            Baton Rouge, Louisiana 70803}\\[.1in]
        $^5$\emph{Department of Biology,
            University of Washington,
            Seattle, Washington 98195}\\[.1in]
        $^6$\emph{Sam Noble Museum,
            Department of Biology,
            University of Oklahoma,
            Norman, Oklahoma 73072}\\[.1in]
        $^2$\emph{Corresponding author} (\href{mailto:joaks1@gmail.com}{\tt
        joaks1@gmail.com})\\

    \end{flushleft}
\end{titlepage}

{\sffamily
    \noindent\textbf{ABSTRACT} \\
    \noindent Recently, \citet{Oaks2012} presented a simulation-based
    assessment of the approximate Bayesian phylogeographical model choice
    method implemented in \msb, and found the method to lack robustness and
    power to detect temporal variation in divergences that are randomly
    distributed over relatively broad periods of evolutionary history.
    They conclude that a likely mechanism behind the bias toward models with
    few divergence events is the small marginal likelihoods of models with more
    divergence time parameters due to the integration over vast parameter space
    imposed by broad uniform priors characterized by low likelihood and high
    prior density.
    This phenomenon is often referred to as Lindley's paradox
    \citep{Lindley1957}.  \citet{Hickerson2013} have responded to this work,
    criticising the priors used by \citet{Oaks2012} and presenting an
    approximate Bayesian model-averaging approach that uses narrow, empirically
    informed uniform priors.
    \citet{Hickerson2013} conclude this approach circumvents the issues
    revealed by \citet{Oaks2012} and yields robust inference of divergence
    models.
    Furthermore, they argue that the biases revealed by \citet{Oaks2012} are
    inherent to the inefficient rejection algorithm implemented in \msb, rather
    than a result of Lindley's paradox.
    Here we show that \cwlnote{their}{the} methodology \cwlnote{suffered}{of \citet{Hickerson2013}} suffers from a fundamental error that
    renders some of their results invalid and the remainder difficult to
    interpret.
    Furthermore, we demonstrate how \cwlnote{their}{the} approach \cwlnote{}{of \citet{Hickerson2013}} still suffers from a bias
    toward models with \cwlnote{lesser}{less} parameter space, which manifests in a dangerously
    strong tendency to sample predominantly from models that exclude the true
    values of the model's parameters.
    Our results suggest that their model-averaged results are dominated by
    models that likely exclude at least some of the true divergence times
    across the 22 pairs of Philippine vertebrates, which adds to the
    difficulty in interpreting their results.
    We also show that in addition to this dangerous behavior, the approach of
    \citet{Hickerson2013} is still strongly biased toward inferring
    simultaneous divergences even when the true divergences are randomly
    distributed over millions of generations.
    Our results, coupled with those of \citet{Oaks2012}, strongly suggest that
    Lindley's paradox is a primary cause of the bias of \msb toward models with
    lesser parameter space.
    We discuss ways to mitigate this bias without resorting to adding an
    additional dimension of model choice to the model or using dangerous
    empirically informed uniform priors.

    \vspace{12pt}
    \noindent\textbf{KEY WORDS: Approximate Bayesian computation; Bayesian
        model choice; empirical Bayes} 

}

\newpage

\highLight{The goal of this paper needs to be to highlight the major differences of opinion on how ABC can be used in comparative phylogeography and how these changes to msbayes can be made that will improve the inference in the future. We should avoid nit-picking the Hickerson paper and try to stay positive.}


\noindent Recently, this journal has served as a medium for a
discussion of the potential pitfalls of approximate Bayesian methods of
comparative phylogeographical model choice and prior selection.
The discussion has centered around the approximate Bayesian model choice method,
\msb, which estimates the temporal distribution of divergences among
co-distributed pairs of taxa \citep{Huang2011} by sampling over all
unordered models of divergence.
\citet{Oaks2012} published results of simulation-based power analyses that
revealed that \msb can often be biased towards inferring models of temporally
clustered divergence times among taxon pairs.
\citet{Hickerson2013} published a response to this paper where they present
an empirical Bayes model-averaging approach that they conclude circumvents the
poor behavior of the method revealed by \citet{Oaks2012}.
These papers are largely in agreement with \citet{Hickerson2013} reiterating a lot of the discussion of
\citet{Oaks2012} regarding the impact of broad uniform priors on Bayesian model
choice.
The main difference in perspectives are centered around
(1) the mechanism by which broad uniform priors cause the poor behavior of
\msb, and
(2) how to potentially ameliorate this issue.

Regarding the mechanism behind the poor behavior of the method,
\citet{Oaks2012} suggest the primary cause is likely the low marginal
likelihoods of parameter-rich models integrated over vast parameter space with
low probability of producing the data, yet relatively high prior density
\citep[this is often referred to as Lindley's paradox;][]{Lindley1957}.
Note, this suggests the bias is extrinsic to \msb, and the numerical
approximation machinery of \msb could be sound.
\citet{Hickerson2013} suggest it is intrinsic to \msb, concluding the method's rejection algorithm is inefficient and will be
increasingly biased as the overall space of the model increases, either as a
function of the number of taxon pairs or the width of the uniform priors on
nuisance parameters.
\citet{Hickerson2013} support their position by giving a probabilistic argument that focuses on
only one of the three prior models used by \citet{Oaks2012}.
We show that the empirical Bayesian approach used by \citet{Hickerson2013} is based on questionable assumptions, and does not explain the
bias of the method found in many of the analyses of \citet{Oaks2012}. 
Furthermore, we show that the model-averaging approach suggested by \citet{Hickerson2013} does not correct the bias towards small priors and we present results of additional analyses, which strongly suggest that
Lindley's paradox is playing a large role in the poor behavior of \msb.

%This paragraph seems too aggressive. It also seems to repeat some of the previous points. I removed it for now.

%To mitigate the bias, \citet{Oaks2012} suggest that more cogent
%prior probability distributions on divergence models and nuisance parameters
%could reduce the effect of Lindley's paradox.
%They also layout a set of simulation-based procedures for determining power,
%accuracy, and robustness of the method given a dataset, and recommend any
%application of \msb should be accompanied by such procedures, especially if any
%biological conclusions are drawn from the results.
%\citet{Hickerson2013} present an approximate Bayesian model-averaging approach
%for accommodating uncertainty in selecting among empirically guided priors, and
%champion the method as a means of avoiding the pitfalls raised by
%\citet{Oaks2012}.
%Unfortunately, we show that fundamental errors in this approach render some of
%their results invalid and leave the remaining results difficult to interpret.
%Furthermore, we follow the advice of \citet{Oaks2012} and present
%simulation-based assessments of Hickerson et al.'s (\citeyear{Hickerson2013})
%approach.
%Our results demonstrate that the new method of \citet{Hickerson2013} is biased and dangerous.  The approach
%merely provides another means by which the model can ``escape'' large parameter
%space, and bias toward smaller models remains.
%This manifestation of the bias can be dangerous and often excludes the true
%parameter space due to the use of narrow uniform priors.
%Furthermore we discuss the potential theoretical and practical problems of
%empirical Bayesian model choice, which is used and encouraged by
%\citet{Hickerson2013} without any mention of its potential complications.


%It may be important to point out the error made in their paper, but it can also be nit-picky, it depends on how the error impacts the overall message of our response. 
%We should strive to keep the response short and directed at the big picture items. If there is a way to still point out the error, then maybe some of this section should be used
\iffalse
\section*{An error in Hickerson et al.'s re-analysis of the Philippines data}
\citet{Hickerson2013} re-analyzed the dataset of \citet{Oaks2012} using a
model-averaging approach, where they placed a discrete uniform prior over eight
different prior models (see Table 1 of \citet{Hickerson2013}).
In our initial attempts to explore the behavior of this approach, we found
a fundamental error in the methodology of \citet{Hickerson2013}:
their model mixes different time units.

Each of the eight prior models used in the re-analysis by \citet{Hickerson2013}
has one of two priors on the mean size of the descendant populations of each
taxon pair:
$\meanDescendantTheta{} \sim U(0.0001, 0.1)$ or
$\meanDescendantTheta{} \sim U(0.0005, 0.04)$.
As described in \citet{Oaks2012}, the divergence time parameters of the model
implemented in \msb are scaled relative to a constant reference population
size, \myTheta{C}.
This reference population size is defined in terms of the upper limit of the
uniform prior on the mean size of the descendant populations,
\meanDescendantTheta{}, such that for the prior $\meanDescendantTheta{} \sim
U(\uniformMin{\meanDescendantTheta{}},\uniformMax{\meanDescendantTheta{}})$,
$\myTheta{C} = \uniformMax{\meanDescendantTheta{}}/2$.
Thus, the model used by \citet{Hickerson2013} mixes two different units of
time.
In other words, some of their prior and posterior samples are in units of
$0.05/\mutationRate$ generations, whereas others are in units of
$0.02/\mutationRate$ generations.

The fact that their posterior samples are in different units makes the results
of \citet{Hickerson2013} difficult to interpret, and renders their
regression-adjusted results invalid; a fundamental assumption of regression is
that all of the values of the response variable are in the same units.
Thus, the results in sections ``Using ABC Model Comparison to Weight
Alternative Priors for the Philippine Vertebrate Data'' and ``Improved Sampling
Efficiency by Prior Weighting Supports Asynchronous and Recent Divergence for
the Philippines Vertebrate Data'' and presented in Figure 2 of
\citet{Hickerson2013} should be disregarded.
The error is easily illustrated by re-plotting their results with the different
time units indicated (Figure~\ref{figJointPosteriorHickerson}).

\cwlnote{}{This part seems important, but not related to the error in units discussed above}
Another important characteristic of the approach used by \citet{Hickerson2013}
that makes their results difficult to interpret in addition to this error is
their use of narrow, empirically guided uniform priors in an approximate
Bayesian model choice framework.
We discuss the dangers of this approach in the next section, and show how the
results of \citet{Hickerson2013} likely exclude true values of the model's
parameters.
\fi

% In addition to this error, we were not able to reproduce the results of
% \citet{Hickerson2013}.
% We followed their methods, generating $5\e6$ prior samples for each of the
% eight models in their Table 1.
% We retained the 10,000 samples from each model with the smallest Euclidean
% distance to the observed summary statistics, and subsequently retained 10,000
% samples from the remaining 80,000.


\iffalse
\section*{The dangers of empirical Bayesian model choice}

\citet{Hickerson2013} repeatedly refer to the prior distributions used by
\citet{Oaks2012} as ``poorly selected''. 
However, this is misleading, as \citet{Oaks2012} discuss in detail how they
selected their priors (see their section ``Specifying and simulating the joint
prior'').
They detail that the limitation in using uniform prior distributions in \msb
necessitates awkward broad priors to avoid excluding the truth, \textit{a priori}.
What \citet{Hickerson2013} really mean by ``poorly selected'' is that
\citet{Oaks2012} did not use their data to inform their initial prior
distributions (i.e., they did not use an empirical Bayesian approach).
\citet{Oaks2012} did use empirically informed priors when assessing both the
prior sensitivity of the method and determining the power of \msb under
real-world conditions with highly informative priors.  
\citet{Oaks2012} discuss
the potential dangers of taking an empirical Bayesian approach to model choice
(see the last paragraph of ``Assessing prior sensitivity of \msb'' in
\citet{Oaks2012}), and we expand on this here by exploring both the theoretical
and practical implications of using empirically informed priors for Bayesian
model choice.
\fi

\section*{Theoretical implications of empirical priors for Bayesian model
choice}
\begin{linenomath}

Bayesian inference is a rational method of inductive learning in which Bayes'
rule is used to update our beliefs about a model as new information becomes
available.
If we let \allParameterValues represent the set of all possible parameter
values, we can define a prior distribution for all $\theta \in
\allParameterValues$ such that $p(\theta)$ describes our belief \cwlnote{}{is belief the right term in this section? Would probability or likelihood be better?}
 that any given
\myTheta{} is the true value of the parameter.
If we let \allDatasets represent all possible datasets then we can 
define a sampling model for all $\theta \in
\allParameterValues$ and $\alignment{}{} \in \allDatasets$ such that
$p(\alignment{}{} | \theta)$ measures our belief that any dataset \alignment{}{}
will be generated by any model state \myTheta{}.
After collecting a new dataset \alignment{i}{}, we can use Bayes' rule to
calculate the posterior distribution
\begin{equation}
    p(\myTheta{} \given \alignment{i}{}) = \frac{p(\alignment{i}{} \given 
    \myTheta{})p(\myTheta{})}{p(\alignment{i}{})},
    \label{eq:bayesrule}
\end{equation}
where
\begin{equation}
    p(\alignment{i}{}) = \int_{\myTheta{}} p(\alignment{i}{} \given
    \myTheta{})p(\myTheta{}) d\myTheta{}.
\end{equation}
The posterior distribution is a measure of our beliefs after seeing the new
information.
\end{linenomath}

This is an elegant method of updating our beliefs as data are accumulated.
However, this all hinges on the fact that the prior ($p(\myTheta{})$) is
defined for all possible parameter values independently of the new data being
analyzed.
Any other datasets or external information can safely be used to inform our
beliefs about $p(\myTheta{})$.
However, if the same data are used to both inform the prior and calculate the
posterior, the prior becomes conditional on the data, and Bayes' rule breaks
down.

This is not to say that empirical Bayesian approaches are not useful.
Empirical Bayes is a well studied branch of Bayesian statistics that
has given rise to many powerful inference methods.
Unfortunately, empirical Bayes methods have an uncertain theoretical basis and
do not yield a valid posterior distribution from Bayes' rule \citep[e.g.,
empirical Bayesian estimates of the posterior are often too narrow, off-center,
and misshapen;][]{Morris1983,Laird1987,Carlin1990,Efron2013}.
Nevertheless, empirical Bayes methods can provide a powerful means of parameter
estimation that often exhibit favorable frequentist properties.
Furthermore, many post-hoc correction methods have been developed for
estimating confidence-intervals from empirical Bayes estimates of posterior
distributions that often exhibit well-behaved frequentist coverage
probabilities
\citep{Morris1983,Laird1987,Laird1989, Carlin1990,Hwang2009}.

Whereas empirical Bayes approaches can provide powerful methods for parameter
estimation, a theoretical justification for empirical Bayes approaches to model
choice is unclear.
In Bayesian model choice, the goal is not to estimate parameters, but to
estimate the relative probabilities of candidate models.
Unlike parameter estimates, posterior probabilities represent a summary of the
entire posterior distribution.
Thus, given that empirical Bayesian posterior distributions are not accurate,
there is no guarantee regarding the accuracy of probabilities summarized from
them.

%I like the example, but it may be too cute. It is also too long. I think a contrived example is good, but if there could be an example that is more relevant to model choice and comparison of inaccurate posterior distributions, I think that would be better. This is an example of empirical Bayes, but not the potential problem with the use in msbayes. Unless I am interpreting wrong. I would be careful about making it to cute as well. You may intend to make it easy to understand for a general audience, but it can also seem like talking down to Hickerson et al, who we are responding to. It just depends on how you read it.
\iffalse
This can be demonstrated with a simple, albeit contrived, example.
Let us say that principal investigator Mary and her new postdoc Will are
interested in the hypothesis that the mass of George Washington's periwig
renders the quarter dollar of the United States unfair.
That is to say their null hypothesis is that the probability of a US
quarter landing heads when tossed is less than 0.5 ($\theta < 0.5$).
They can certainly evaluate this hypothesis, they can have undergraduate
worker, Joe, flip the coin for them while they tabulate the results.
But being Bayesians, before they call Joe into the lab, they agree on a
prior probability to place on the set of all possible probabilities that
the quarter will land heads when it is flipped.
Given that neither of them have a quarter, and their prior knowledge that Joe
moonlights as a magician, and is notorious for performing coin and card tricks
in the lab, they suspect there is a good chance that the quarter Joe uses will
be either two-headed or two-tailed.
So, knowing that the beta distribution is the conjugate prior for a binomial
likelihood, they decide to use a $beta(a=0.5, b=0.5)$ prior distribution
(Figure~\ref{figCoinFlip}).

Mary calls Joe into the lab, confirms that he has a quarter, and tells him to
begin flipping it.
After five tosses, four of which was heads, Joe decides that academics are
crazy and leaves the lab to pursue a major in theatre.  Mary and Will, both
being computational biologists, are satisfied with their
empirical dataset of $y = 4$ heads out of $N = 5$ trials.
They know from the conjugacy of the beta prior, that the posterior distribution
has the nice analytical form $\theta|y,N \sim beta(a + y, b + N - y)$, which
in this case is simply $beta(4.5, 1.5)$; this is the true posterior distribution
of $\theta$ given their prior belief and data (Figure~\ref{figCoinFlip}).
This allows them to plug these values into the beta cumulative distribution
function to determine that the posterior probability of their hypothesis is
$p(\theta < 0.5 | y=1, N=5) = 0.088$.
Given their prior belief and dataset, this indeed is the correct posterior
probability of the hypothesis, and Mary and Will should now update their
posterior belief accordingly.

However, reflecting upon the results of their experiment
(Figure~\ref{figCoinFlip}), Mary and Will regret their choice of prior.
Their prior looks very ``poorly selected,'' and if they had only known that the
coin was fair before the data were collected, they would have selected a much
better prior.
Clearly, from their results, they should have used a prior centered
around 0.2; their data suggest a prior of $beta(4.5, 1.5)$ would have been
much ``better.''
Rather than resort to flipping the coin five more times, Mary and Will decide
to redo their analysis using the much better ``prior'' of $p(\theta) \sim
beta(4.5, 1.5)$.
This gives a ``posterior'' of $\theta|y,N \sim beta(8.5, 2.5)$, and a
probability of their hypothesis of $p(\theta < 0.5 | y=1, N=5) = 0.026$.
Now convinced that the US quarter is unfair, albeit not due to the mass of
President Washington's head as they hypothesized, Will begins composing an
e-mail of complaint addressed to the U.S.\ Mint.

If Joe's flips were a representative sample, Mary and Will's empirical Bayes
estimate might very well be a better point estimate for the parameter
\myTheta{}.
However, their empirical Bayes estimate of the probability of their hypothesis
is incorrect and biased.
This simple example shows how parameter estimation is fundamentally different
from estimating the probability of a model.
While empirically informed priors can be used to obtain well-behaved parameter
estimators, using them for model choice is much less certain.
In addition to these theoretical concerns there are practical dangers to
using narrow, empirical, uniform priors for a method that has already
been shown to be biased toward models with lesser parameter space.
\fi

\subsection*{Dangers of narrow uniform priors for Bayesian model
choice} \cwlnote{}{Should this be part of the empirical Bayes section, or something else?}

\cwlnote{}{It seems like this section can be boiled down to ``empirical Bayes approaches for model choice when using uniform priors can exclude the $truth$ from the prior, which is a violation of Bayes theorem" or something like that. Picking apart what they calculated and we calculated seems like too much detail. I would try to simplify how this is being presented. This can lead into how the use of more appropriate prior distributions should be used.}
The results of \citet{Hickerson2013} reanalysis of the Philippine dataset strongly favored models with the narrowest, empirically
informed prior on divergence time, and thus their model-averaged posterior
estimates are dominated by models $M_1$ and $M_2$ (see Table 1 of
\citet{Hickerson2013}).
The narrowest \divt{} prior used by
\citet{Hickerson2013} ($\divt{} \sim U(0,0.1)$) likely excludes the true
divergence times for at least some of the Philippine taxa, a major problem when using uniform priors.
\citet{Hickerson2013} set this prior to match the 95\% highest posterior density (HPD) interval for
the mean divergence time estimated under one of the priors used by
\citet{Oaks2012} (see Tables 2 and 3 of \citet{Oaks2012}).
Given this interval estimate is for the \emph{mean} divergence time across all
22 taxa, it may be inappropriate to set this as the prior, because many of the taxon pairs
are expected to have diverged at times older than the upper limit.
Furthermore, this prior is \emph{excluded} from the 95\% HPD interval estimates
of the mean divergence time under the other two priors explored by
\citet{Oaks2012} (under these priors the 95\% HPD is approximately 0.3--0.6;
see Table~6 of \citet{Oaks2012}).
Thus, the results of \citet{Hickerson2013} indicate
Lindley's paradox is biasing the method toward models with smaller
parameter space, and as a consequence is biasing the method toward models that
exclude the truth (i.e., toward estimating model-averaged posteriors
dominated by models that exclude true values of parameters of the model).


%This should be in the section on model-averaging, not empirical Bayes limitations
We explored the propensity of the model-averaging approach of
\citet{Hickerson2013} to exclude the truth in two ways.
First, we re-analyzed the Philippines dataset using the model-averaging approach
of \citet{Hickerson2013}, but set one of prior models with a uniform prior on
divergence times that is unrealistically narrow, and almost certainly excludes
most, if not all, of the true divergence times of the 22 taxon pairs.
If Lindley's paradox causes the method to prefer models with less parameter
space, we expect \msb will preferentially sample from this incorrect model
yielding a marginal posterior that is incorrect (i.e., the model-averaged
posterior will be dominated by an incorrect model that excludes the truth).
Second, we generated simulated datasets for which the divergence times are
drawn from an exponential distribution and applied the approach of
\citet{Hickerson2013} to each of them to see how often the method excludes the
truth.

For our re-analysis of the Philippines dataset we used the model-averaging
approach of \citet{Hickerson2013}, but with a reduced set of prior models to
avoid their error of mixing time units.
We used five prior models, all of which had priors on population sizes of
$\meanDescendantTheta{} \sim U(0.0001, 0.1)$ and $\ancestralTheta{} \sim
U(0.0001, 0.05)$.
Following \citet{Hickerson2013}, each of these models had the following
priors on divergence time parameters:
$M_1$, $\divt{} \sim U(0, 0.1)$;
$M_2$, $\divt{} \sim U(0, 1)$;
$M_3$, $\divt{} \sim U(0, 5)$;
$M_4$, $\divt{} \sim U(0, 10)$; and
$M_5$, $\divt{} \sim U(0, 20)$.
For our prior, we simulated $1\e6$ random samples from each of the models
for a total sample of $5\e6$.
For each model, we retained the 10,000 samples with the smallest Euclidean
distance from the observed summary statistics, standardizing the statistics
using the prior means and standard deviations of the given model.
From the remaining 50,000 samples, we then retained the 10,000 samples with the
smallest Euclidean distance from the observed summary statistics, this time
standardizing the statistics using the prior means and standard deviations
across all five models.
We then repeated this analysis twice, replacing the $M_1$ with
$M_{1A}$ and $M_{1B}$, which differ only by having priors on divergence
times of $\divt{} \sim U(0, 0.01)$ and $\divt{} \sim U(0, 0.001)$,
respectively.
While we suspect the prior of $\divt{} \sim U(0, 0.1)$ used by
\citet{Hickerson2013} likely excludes the true divergence times of at least
some of the 22 taxa, we are nearly certain that these narrower priors are
incorrect and exclude most, if not all, of the divergence times of the
Philippine taxa.

Our results show that the model-averaging approach of \citet{Hickerson2013}
does not reduce the method's bias toward models with less parameter space,
but rather allows it to manifest in a very dangerous way.
The method strongly prefers the prior model with the narrowest distribution on
divergence times across all three of our analyses, even when this model is
almost certainly incorrect and excludes the true divergence times of the
Philippine taxa (Table~\ref{tabModelChoiceEmpirical}).
Unfortunately, ``checking'' the priors by plotting the summary statistics from
1000 random samples of each prior model along the first two orthogonal axes of
a principle component analysis, as recommended by \citet{Hickerson2013},
provides no warning of a problem (Figure~\ref{figPCA}).
Given that the results of \citet{Hickerson2013} strongly prefer the models with
the narrowest prior on divergence times, it seems quite likely that their
model-averaged results are dominated by models that exclude at least some of
the 22 true divergence times, making their results difficult to interpret
(in addition to the error of time units).

To better quantify the propensity of Hickerson et al.'s
(\citeyear{Hickerson2013}) approach to exclude the truth, we simulated
1000 pseudo-observed datasets in which the divergence times for the
22 population pairs are drawn randomly from an exponential distribution
with a mean of 0.5 ($\divt{} \sim Exp(2)$).
All other parameters were identically distributed as the $M_1$--$M_5$ models
(Table~\ref{tabModelChoiceEmpirical}).
We then repeated the analysis described above, using $5\e6$ prior samples from
$M_1$, $M_2$, $M_3$, $M_4$, and $M_5$, and retaining 1000 posterior samples
for each of the 1000 pseudo-observed datasets.

\highLight{Add how we determined exclusion.}
Our results show that the model-averaging approach of \citet{Hickerson2013}
excludes the true values of parameters in 97\% of the replicates (90\% with
GLM-regression adjustment), excluding up to 21 of the 22 true divergence times
(Figure~\ref{figExclusionSimTau}).
Furthermore, the posterior probability of excluding at least one true parameter
value is very high in nearly all of the replicates
(Figure~\ref{figExclusionSimProb}).
We used a Bayes factor (comparing models that exclude the truth to those that
do not) calculated for each replicate of greater than 10 as strong support for
excluding the truth.
With this criterion, 66\% of the replicates (87\% with GLM-regression adjustment)
strongly support the exclusion of true values (Figure~\ref{figExclusionSimProb}).

The results of our empirical and simulation-based analyses clearly demonstrate
the danger of using narrow, empirically guided uniform priors in a Bayesian
model-averaging framework.
This is especially dangerous in \msb, which has already been demonstrated to be
biased away from models with more parameter space \citep{Oaks2012}.
The consequence of this approach is a high risk of obtaining a model-averaged
posterior estimate that is heavily weighted toward incorrect models that
exclude true values of model parameters.


\subsection*{Final thoughts on empirical priors in Bayesian model choice}
Given all of the theoretical and practical issues with empirical Bayes
approaches to Bayesian model choice discussed in the proceeding sections, it is
quite clear why one should use caution before overly criticizing an
investigator's choice of priors after having seen the resulting posterior.
%In a strictly Bayesian world, this is cheating.
As discussed by \citet{Oaks2012}, prior to analyzing the data, there was a large
amount of uncertainty regarding the divergence times of the 22 population pairs
under study.
Two of these pairs represent distinct species, and the taxonomy of many groups
in the Philippines has repeatedly been shown to mask deeply divergent lineages
\citep{RafeDiesmosAlcala2008,Linkem2010,Siler2010,Welton2010,Siler2011HerpMonographs,
Siler2011,Siler2012,RafeStuart2012,LinkemBrown2013}.
\citet{Oaks2012} also discuss how the sole use of uniform priors in \msb makes
it very difficult for investigators to express their prior uncertainty without
putting a lot of prior density in regions of improbable parameter space.
The alternative, as shown above, is to risk excluding the truth before
performing the analysis.






\section*{Assessing the power of the model-averaging approach of
    \citet{Hickerson2013}}
\cwlnote{}{I think the empirical use of the model-averaging should go here, instead of above in the empirical Bayes section}


As recommended by \citet{Oaks2012}, we perform simulation-based analyses
to explore the power of the approximate Bayesian model-averaging approach
proposed by \citet{Hickerson2013}.
Following \citet{Oaks2012}, we simulated 1000 pseudo-replicate datasets with
\divt{} for each of the 22 population pairs randomly drawn from a uniform
distribution, $U(0, \divt{max})$, where \divt{max} was set to: 0.2, 0.4, 0.6,
0.8, 1.0, and 2.0, in \globalcoalunit generations.
All other parameters were identically distributed as the prior.
Following \citet{Hickerson2013} (but avoiding mixing time units), we used five
prior models, all of which had priors on population sizes of
$\meanDescendantTheta{} \sim U(0.0001, 0.1)$ and $\ancestralTheta{} \sim
U(0.0001, 0.05)$.
Each of these priors had the following priors on divergence time parameters:
$M_1$, $\divt{} \sim U(0, 0.1)$;
$M_2$, $\divt{} \sim U(0, 1)$;
$M_3$, $\divt{} \sim U(0, 5)$;
$M_4$, $\divt{} \sim U(0, 10)$; and
$M_5$, $\divt{} \sim U(0, 20)$.
For our prior, we simulated $1\e6$ random samples from each of the models
for a total sample of $5\e6$.
For each pseudo-replicate dataset, we retained 1000 samples from each prior
model with the smallest Euclidean distance from the pseudo-observed summary
statistics, standardizing the statistics using the prior means and standard
deviations of the given model.
Then, for each replicate, we retained 1000 samples with the overall
smallest Euclidean distance from the pseudo-observed summary statistics, this
time standardizing the statistics using the prior means and standard deviations
across all five models.
In total, we analyzed 6000 replicate datasets, retaining 1000 model-averaged
posterior samples for each of them.

We find that the approach of \citet{Hickerson2013} struggles to estimate the
dispersion index of divergence times (\vmratio{}) across most of the \divt{max}
we simulated, whether evaluating the unadjusted
(Figure~\ref{figPowerAccOmegaMedian}) or GLM-adjusted
(Figure~\ref{figPowerAccOmegaModeGLM}) posterior estimates.
The method only estimates \vmratio{} relatively well when the simulated
distribution of divergence times is identical to one of the prior models
(Figures~\ref{figPowerAccOmegaMedian}E \& \ref{figPowerAccOmegaModeGLM}E).
This is consistent with the conclusion of \citet{Oaks2012} that \msb lacks
robustness and is highly sensitive to the prior distribution deviating from the
true, underlying distribution of the data.

Furthermore, our results demonstrate that the approach of \citet{Hickerson2013}
consistently infers highly clustered divergences across all the \divt{max} we
simulated (Figure~\ref{figPowerNumExcluded}).
The method is most likely to infer the extreme case of a single divergence event
when populations diverged randomly over the past \globalcoalunit generations.
Even when divergences are random over the past $8\globalpopsize$ generations,
the most likely inference is only two divergence events, and a single 
divergence is still estimated in more than 10\% of the replicates.
It is very interesting to note that as \divt{max} increases, but before the
estimates are finally pulled away from $\numt{} = 1$, the distribution of
\numt{} estimates closely mirror the odd U-shaped prior on divergence models
used by \msb (see Figure~\ref{figPowerNumExcluded}E and Oaks et al.'s
(\citeyear{Oaks2012}) Figure 5B).
This supports the conclusions of \citet{Oaks2012} that this U-shaped
prior coupled with the poor marginal likelihoods of models with many
\divt{} parameters, is a major cause of the method's bias toward
clustered divergence models.

Looking at our simulation results in terms of the posterior probability of the
dispersion index of divergence times supporting the extreme case of one
divergence event (i.e., $p(\vmratio{} < 0.01 \given \ssSpace)$), we find the
method strongly supports one divergence in greater than 27\% of the replicates
across all the \divt{max} we simulated (Figure~\ref{figPowerOmegaProb}).
Following \citet{Hickerson2013}, we use a Bayes factor of greater than 10 as
the criterion for incorrect inference of a single divergence event.
There is strong support for a single divergence event in more than 90\% of the
replicates when divergences are random over the past $2.4\globalpopsize$
generations, and more than 60\% when over the past $3.2\globalpopsize$
generations or less (Figure~\ref{figPowerOmegaProb}).

Contrary to Hickerson et al.'s (\citeyear{Hickerson2013}) claim that their
model-averaging approach ``can discriminate complex multispecies histories and
correctly reject synchronous divergence, even when discrete divergence times
differ by much less than \ldots millions of generations,'' our results show
that method is biased toward supporting the extreme case of a single
divergence event when populations diverged randomly over the last
$8\globalpopsize$ generations.
To put this on the scale roughly consistent with a vertebrate mitochondrial
locus, assuming a mutation rate of $2\e{-8}$ per site per generation, this
translates to 5 million generations.
Assuming a mutation rate consistent with nuclear loci of $1\e{-9}$, this is 100
million generations.

The results of our power analysis further demonstrate the propensity of
Hickerson et al.'s (\citeyear{Hickerson2013}) approach to exclude true parameter
values.
Across all but one of the \divt{max} we simulated, the method excludes the
truth in large proportion of replicates, and across many of the \divt{max} it
will exclude a large proportion of the true divergence time values
(Figure~\ref{figPowerNumExcluded}).
The posterior probability of excluding at least on true divergence value is
also quite high across many of the \divt{max}
(Figure~\ref{figPowerProbExclusion}).
Only when the data are identically distributed as one of the prior models does
the method avoid excluding the truth more than 5\% of the time
(Figure~\ref{figPowerNumExcluded}E).
Again, this demonstrates the method's lack of robustness.

Given our results, we conclude that the approach of
\citet{Hickerson2013} excludes regions of parameter
space containing the $truth$, and lacks power to detect random variation in
divergence times over the scale of $8\globalpopsize$ generations or more.
This roughly translates to millions of generations for mitochondrial loci, and
hundreds of millions of generations for nuclear loci.


%I would suggest removing this section
\iffalse
\section*{The power analysis of \citet{Hickerson2013}}
\citet{Hickerson2013} present a power analysis in which they find that
\msb has the power to infer multiple divergence events when divergence times
are random over hundreds of thousands of generations (rather than millions as
demonstrated by \citet{Oaks2012}).
It is not surprising that under limited parameter space the method has
increased power to detect temporal variation in divergences over narrower time
windows than millions of generations.
However, what is important to consider is whether those conditions are relevant
to real-world applications of the method.
\citet{Oaks2012} explore the behavior of the method under three divergence time
priors, as narrow as 0--5 coalescent units. This is quite narrow considering
that this expresses the prior belief that all 22 taxon pairs diverged within
this window.
\citet{Hickerson2013} limit their power analysis to a single prior of 0--1
coalescent unit.
This prior setting assumes that there is enough a priori information to
be 100\% certain that all taxa (18 for their simulations) diverged within
the last 4$\globalpopsize$ generations.
This does not seem applicable to most empirical systems.
Furthermore, even when such extensive prior information is available, to only
be able to detect multiple divergences on the scale of hundreds of thousands of
generations does not seem very powerful.

Also, \citet{Hickerson2013} present information only on the extreme case of a
single divergence event.
As discussed in \citet{Oaks2012}, it seems odd to consider the estimation
of two divergence events as a success when divergence is random over hundreds
of thousands (or millions) of generations.
In an empirical system such as the Philippines, where island fragmentation has
occurred at least \highLight{XX} times over the past several millions of years,
an estimate where 22 taxa share even a handful of divergence events would be of
biogeographic interest.

Lastly, \citet{Hickerson2013} translate their results from units of
\globalcoalunit generations to generations assuming a mutation rate of
$1.92\e{-8}$ mutations per site per generation.
If we translate their results to a scale more consistent with rates of mutation
of nuclear loci ($1\e{-9}$), even under the very optimistic prior settings used
by \citet{Hickerson2013} the method can only reliably reject the extreme case
of one divergence event when divergences are random over a period of more than
3 million generations.
Again, considering \citet{Hickerson2013} are assuming prior knowledge that
all population pairs diverged within the last coalescent unit, this
does not seem particularly powerful.
\fi

%This seems to specific, not sure about including
\section*{Validation analyses}
\highLight{Not sure where to put this section}.
\highLight{This justifies presenting the unadjusted results in this paper.}
\highLight{Is it worth including just for this purpose?}
Following \citet{Oaks2012}, we characterize the model-choice behavior of the
model-averaging approach of \citet{Hickerson2013} under the ideal conditions
where the prior is correct (i.e., the data are generated from parameters drawn
from the same prior distributions used in the analysis).
We used the same prior models as above ($M_1$--$M_5$;
Table~\ref{tabModelChoiceEmpirical}), and generated 50,000 pseudo-replicate
datasets under this prior (10,000 from each model).
We used a simulated data structure of eight population pairs, with a single
1000 bp locus sampled from 10 individuals from each population.
We then analyzed each of these replicate datasets using the same prior,
retaining 1000 posterior samples.
Our results are very similar to \citet{Oaks2012}, but we note that they
are not directly comparable as our simulations contained eight population
pairs rather than 10 (Figure~\ref{figValidationMCBehavior}).
We find that the method does a decent job of estimating the posterior
probability of divergence models when all assumptions of the method
are met, the prior is correct, and the unadjusted posterior estimates
are used.
Similar to \cite{Oaks2012}, we find that the regression-adjusted estimates of
the model probabilities are biased.




\section*{Lindley's paradox versus insufficient prior sampling}
One of the points of disagreement between \citet{Hickerson2013} and
\citet{Oaks2012} is the underlying mechanism causing the bias toward models
with clustered divergences (i.e., models with few divergence events).
\citet{Oaks2012} present simulation-based analyses that suggest the broad
uniform prior on divergence time parameters hinder the marginal likelihoods of
models with more divergence time parameters, whereas \citet{Hickerson2013}
argue that the bias is inherent to the inefficient rejection algorithm
implemented in \msb.

\citet{Hickerson2013} present an interesting probabilistic argument
 to show that insufficient prior sampling is to blame for the bias.
They argue the widest of the three priors used by \citet{Oaks2012} was very
unlikely to produce any prior samples with large numbers of divergence times
that were consistent with the Philippine data.
There are a few issues with their argument.
First, the probabilities they present assume the gene divergence times
estimated by \citet{Oaks2012} are correct.
The sole purpose of the estimates of ultrametric gene trees presented by
\citet{Oaks2012} was to provide a very rough comparison of the gene divergence
times across the 22 taxa.  These analyses assumed an arbitrary strict clock of
$2\e{-8}$ for all taxa, and are, of course, subject to estimation error.
Furthermore, the branch-length units of the gene trees are in millions of
years, whereas the divergence time prior of \msb is in generations, thus the
logic of \citet{Hickerson2013} requires the additional assumption that all 22
Philippine taxa have a generation time of one year.
Thus, the argument of \citet{Hickerson2013} that divergence time estimates of
\citet{Oaks2012} ``should set an upper bound on their prior for \divt{}'' seems
dangerous, especially given our findings on the behavior of empirically
informed priors presented above.

Finally, even \emph{if} we make all of these assumptions and assume the gene
divergence times are estimated without error, the probabilistic argument only
applies to one of the three different priors used by \citet{Oaks2012}.
The narrowest prior on divergence times used by \citet{Oaks2012} closely mirrors
the range of gene divergence time estimates, and applying Hickerson et al.'s
(\citeyear{Hickerson2013}) probability equations demonstrates that the prior
is densely populated with samples with large numbers of divergence parameters
that are consistent with the gene divergence estimates.
Thus, according to the argument of \citet{Hickerson2013}, if insufficient prior
sampling is to blame for the bias, it should be much reduced under the narrow
prior on \divt{}.
However, the magnitude of the bias is very similar across all three priors
explored by \citet{Oaks2012}.
\citet{Hickerson2013} point out a case where the narrow prior performs
slightly better (panel L of Figures S32, S37, and S38 of \citet{Oaks2012}).
However, it is important to note that these results suffered from a bug
in \msb, and there are many cases after \citet{Oaks2012} corrected the 
bug where the narrow prior performs slightly worse (see panels D--J of
Figures 3 and S12).

To disentangle whether Lindely's paradox or insufficient prior sampling is
to blame for the biases revealed by \citet{Oaks2012}, we must look at
the different predictions made by these two phenomena.
One example, as discussed by \citet{Oaks2012}, is that insufficient prior
sampling should create higher variance in posterior estimates, and thus it
should cause analyses to be sensitive to the number of samples drawn from the
prior.
\citet{Oaks2012} do not see such sensitivity when they compare prior sample
sizes of $2\e6$, $5\e6$, and $10^7$.

To explore this prediction further, we repeat the analysis of the Philippines
dataset under the intermediate prior used by \citet{Oaks2012} ($\divt{} \sim U(0,
10)$, $\meanDescendantTheta{} \sim (0.0005, 0.04)$, $\ancestralTheta{} \sim
(0.0005, 0.02)$), using a very large prior sample size of $10^8$.
When we look at the trace of the estimates of the dispersion index of
divergence times (\vmratio{}) as the prior samples accumulate
(Figure~\ref{figSamplingError}) we see no trend in either the unadjusted or
GLM-regression-adjusted estimates.
This suggests that insufficient prior sampling did not play a large
role in the bias found by \citet{Oaks2012}.

Our results presented above that demonstrate the bias of the model-averaging
approach of \citet{Hickerson2013} both toward models with narrower \divt{}
priors (Table~\ref{tabModelChoiceEmpirical} and
Figs.~\labelcref{figExclusionSimTau,figExclusionSimProb,figPowerNumExcluded,figPowerProbExclusion})
and models with fewer \divt{} parameters (Figs.~\ref{figPowerPsiMode} \&
\ref{figPowerOmegaProb}) strongly implicate Lindley's paradox as a primary
cause.
In all of our model-averaging analyses, all prior models have the same number
of samples.
Thus, it is difficult to explain the method's propensity towards the model with
the smallest parameter space via insufficient prior sampling.
While analyses that sample each model proportional to their relative parameter
space could be explored, it seems much more likely that the broad uniform
priors are simply inhibiting the marginal likelihoods of spacious models.

As discussed by \citet{Oaks2012}, a straightforward prediction of Lindley's
paradox is that it should disappear as the model generating the data converges
to the prior.
\citet{Oaks2012} test this prediction by performing 100,000 simulations to
assess the model-choice behavior of \msb when the prior model is correct.
They find that the bias actually switches direction (at least for the
regression-adjusted estimates) and the method tends to underestimate the
probability of the model with one divergence event (see Figure 4 of
\citet{Oaks2012}).
The same prediction is not as straightforward for the insufficient-sampling
hypothesis.
Even when the prior is correct, due to the discrete uniform prior on
\numt{} implemented in \msb, models with larger numbers of divergence
events (and thus greater parameter space) will still be less densely
sampled than those with fewer divergence events \citep{Oaks2012}.
Thus, the results of the simulations of \citet{Oaks2012} are more consistent
with Lindley's paradox causing the bias toward models with fewer divergence
time parameters.

Taken together, the results presented here and in \citet{Oaks2012} support
Lindley's paradox as a primary mechanism by which broad uniform priors
cause biases in \msb.
Nonetheless, posterior sampling error will always be present in any numerical
Bayesian approximation method.
Thus, insufficient sampling of the prior will contribute to the error of all
approximate Bayesian estimates.
However, there is no strong evidence that it is playing a large role in
the biases revealed herein and by \citet{Oaks2012}.




\section*{Differing utilities of \numt{} and \vmratio{} in \msb}
The primary parameter of the \msb model is the vector of divergence
times for each of the taxon pairs,
$\divtvector = \{\divt{1}, \ldots, \divt{Y}\}$
\citep{Oaks2012}.
\citet{Hickerson2013} argue that the dispersion index of this vector,
\vmratio{}, is a better model-choice estimator than the vector's cardinality,
\numt{}.
They present a plot of \numt{} against \vmratio{} (Fig.~S1 of
\citet{Hickerson2013}), which is simply a plot of sample size versus variance.
This plot shows, not surprisingly, that \vmratio{} has essentially no
information about the number of divergences among taxa.
Nonetheless, \citet{Hickerson2013} conclude \vmratio{} more informative and
biogeographically relevant than \numt{}.
We struggle to follow this logic.
Certainly the maximum information is contained within the divergence time
vector that \vmratio{} is summarizing.
The temporal information contained in this vector coupled with its cardinality
(\numt{}) is certainly more informative than its variance (i.e., the dispersion
index is not a sufficient statistic for \divtvector).

\citet{Hickerson2013} also argue that ``\msb can estimate \vmratio{} much
better than \numt{}.''
However, \msb is a model-choice estimate, and hence the goal is to estimate the
posterior probability of divergence models.
\citet{Oaks2012} demonstrate that even when all assumptions of the model are
met, \vmratio is a poor model-choice estimator (see plots B, D \& F of Figure
4), whereas \numt{} performs better.
Furthermore, \vmratio{} is limited to estimating the probability of only a
single divergence model (the one divergence model), and thus its utility for
model choice is extremely limited.

The model-choice utility of \vmratio{} is limited to the probability that
this continuous statistic, which can range from zero to infinity, is at its lower
limit of zero. In theory, this point density will always be zero, thus an
arbitrary threshold (0.01 is used throughout the \msb literature) must 
be chosen to make the probability estimable.
However, it is still not surprising to see that it is numerically difficult to
obtain reliable estimates of the probability that the continuous \vmratio{}
statistic is ``near'' its limit of zero.
It is much easier, less subjective, and more interpretable to estimate
the probability of the discrete parameter of the model, \numt{}, is
at its lower limit of one.
Thus, it is not surprising to see this estimator of model probability
perform better.


\section*{Some general thoughts on the model of \msb}
It is not too surprising that the model-averaging approach proposed by
\citet{Hickerson2013} behaves poorly.
While this approach is trivial to implement, it is not a trivial change
to the basic \msb model.
To better understand why this is, it might help to step back and get a
sense of the scale of the \msb model.
To do this, we will use the dataset of 22 vertebrate taxon pairs of
\citet{Oaks2012} as an example.

Following the model description and notation of \citet{Oaks2012}, let us tally
up all of the free parameters in the \msb model.  Under the simplest model in
\msb (i.e., assuming no migration and
no intra-locus recombination), the number of parameters for each
taxon pair include:
The population sizes of the ancestral and descendant populations
(\ancestralTheta{}, \descendantTheta{1}{}, \descendantTheta{2}{}),
the magnitude of population contraction in each of the descendant
populations (\bottleScalar{1}{} and \bottleScalar{2}{}) and the
timing of these contractions (\bottleTime{}), and the $N-1$ node heights
(coalescent times) of the gene tree that gives rise to the $N$ gene
copies sampled from both populations of the pair.
Because we only have a single locus for each taxon, there are no locus-specific
\myTheta{}-scaling parameters (\locusMutationRateScalar{}) or
\locusRateHetShapeParameter shape parameter for the gamma prior distribution on
\locusMutationRateScalar{}.
Lastly, there are between one and 22 divergence time parameters \divt{} in
the vector \divtvector.
Overall, for our Philippines data under the simplest model in \msb, there are
581--602 free parameters (depending on the cardinality of \divtvector).
Under this rich model, the method is sampling over 1002 divergence models
\citep[i.e., the number of integer partitions of $Y=22$;][]{Oaks2012}.

This is a very tall order, and the method only uses four summary statistics
calculated from the sequence alignment of each taxon pair:
$\pi$ \citep{Tajima1983}, $\theta_W$
\citep{Watterson1975}, $\pi_{net}$ \citep{Takahata1985}, and
$SD(\pi-\theta_W)$ \citep{Tajima1989}.
That gives us a total of 88 summary statistics (four for each of the 22 taxon
pairs), which contain minimal information about many of the $\approx 600$
parameters in the model.
More summary statistics can be used in \msb, but most are highly correlated
with these four statistics (which are even highly correlated among themselves),
and thus contribute little additional information about the data.

This very large number of parameters and divergence models relative to 
summary statistics is undoubtedly a key reason the method is so sensitive
to the prior distributions.
It also likely contributes to the method's lack of robustness.
We use robustness here as a measure of a method's insensitivity to model
violations.
Robustness is an extremely important characteristic of a method to gauge its
applicability to real-world data, because we know the model and priors will be
wrong to some degree.

The approach of \citet{Hickerson2013} adds an additional dimension of model
choice to the model. They expand the model to sample over eight prior models.
This extends the original model to having 582-603 free parameters and, more
importantly, sampling over 8016 unique model states across both divergence
models and prior models.
This certainly is a non-trivial extension of the model and, given the method's
very large number of parameters and models relative to the information used
from the data, likely plays a major role in the poor behavior of this approach.
% Certainly, such a large modification of the model demands simulation-based
% assessments of the behavior of the new model, as we provide here.

In theory, the approach of \citet{Hickerson2013} is very appealing.  It sums
over multiple candidate prior models to produce a posterior estimate
marginalized over the uncertainty in prior choice.
In general, Bayesian model-averaging is a powerful approach that leverages a
great strength of Bayesian statistical procedures, namely the ability to
obtain marginalized estimates that incorporate uncertainty in nuisance
parameters.
However, given the basic \msb model is already struggling to estimate
a huge number of parameters and model probabilities with scant information
from the data, it is not surprising that adding an additional layer of
model choice to the method causes problems.

The recommendations of \citet{Oaks2012} for mitigating the bias and lack of
robustness of \msb are actually similar to those of \citet{Hickerson2013}, but
avoid the need for imposing additional model choice.
\citet{Oaks2012} suggest that uniform priors are inappropriate for many
parameters of the \msb model, and recommend the use of more cogent probability
distributions from the exponential family.
If we look at the prior distribution on the divergence time parameter \divt{}
imposed by the model-averaging prior approach of \citet{Hickerson2013} we see
it is a mixture of overlapping uniforms with lower limits of zero
(Figure~\ref{figMCTauPrior}).
This looks very much like an exponential distribution.
Thus, it would be much more natural and easier to simply place a gamma prior
(the exponential being a special case of the gamma) on divergence times.
This would allow the investigator to much better capture their prior
uncertainty in model parameters while avoiding awkward broad uniform
distributions, and without the need of costly model-averaging.
This seems like a much more natural solution to the problem of Lindley's
paradox and sampling error, which are both certainly exacerbated by the fact
that uniform distributions are inappropriately the only choice of prior for
many of the parameters of the \msb model \citep[\divt{}, \ancestralTheta{},
\descendantTheta{1}{}, \descendantTheta{2}{}, \bottleTime{},
\bottleScalar{1}{}, \bottleScalar{2}{}, \locusRateHetShapeParameter,
\migrationRate{}, \recombinationRate;][]{Oaks2012}.

Importantly, there is no cost to this strategy.
Given that ABC methods merely have to draw random values from prior
distributions (hence there are no difficult proposal probability ratios to
calculate, etc.) there is no reason for not using the most appropriate
distributions on parameters with conjugacy in mind.
Also, this approach reduces the temptation of empirically guided priors.
The investigator can place the majority of the prior density in regions
of parameters space they believe, a priori, are most plausible, but still
capture uncertainty in the tails of distributions with low density.
As discussed in \citet{Oaks2012} the use of inflexible and unforgiving uniform
priors necessitate the need to use broad priors to avoid excluding the truth a
priori.
Thus, to avoid the behavior of the method under these uniform priors, using the
data is a very tempting, albeit dangerous, option \citep{Hickerson2013}.
More appropriate prior distributions would alleviate this issue.



%This seems to nit-picky. Probably not worth including it
\section*{Other clarifications}

\subsection*{Graphical checks of priors}
\citet{Hickerson2013} advocate the use of what they call graphical checks of
prior models.
This entails generating a small number of random samples from the prior (1000)
and plotting the resulting summary statistics in comparison to the observed
statistics to see if they coincide (see Figure 1 of \citet{Hickerson2013}).
As we show above, this is a dangerous strategy for choosing priors.
It is not surprising that the resulting plots of this approach have little
correlation with the appropriateness of priors.
Given the richness of the \msb model ($\approx 600$ parameters for the Philippine
dataset analyzed by \citet{Hickerson2013}), we do not expect that 1000
\emph{random} draws from the vast prior parameter space will yield data and
summary statistics consistent with the observed data.
In fact, when it does, this can be an obvious warning sign that the prior is
over-fit, as we show above (Table~\ref{tabModelChoiceEmpirical} and
Figure~\ref{figPCA}).
Thus, interpreting such plots should be avoided, and the use of posterior
predictive analyses would be much more informative about the overall fit of
models.

\subsection*{The validity of \msb estimates}
\citet{Hickerson2013} claim that \citet{Oaks2012} ``assume that all previous
\msb results are invalid.''
With the exception of \citet{Hickerson2013}, we do not claim any previous
results of \msb are invalid.
Rather, we conservatively recommend that the common inference of temporally
clustered divergences
\citep{Barber2010, Bell2012, Carnaval2009, Chan2011, Daza2010, Hickerson2006,
    Huang2011, Lawson2010, Leache2007, Plouviez2009, Stone2012, Voje2009},
when not accompanied with the necessary simulation-based analyses to guide the
interpretation of such results, should be treated with caution, because the
method has been shown to spuriously infer clustered divergences over a range of
prior conditions.

\subsection*{Saturation of summary statistics}
\citet{Hickerson2013} claim that our priors ``cause much of the explored
parameter space to be beyond the threshold of saturation in most mtDNA genes.''
To explore this possibility, we simulated datasets under prior settings that
match two of the three priors used by \citet{Oaks2012}: $\meanDescendantTheta{}
\sim U(0.0005, 0.04)$ and $\ancestralTheta{} \sim U(0.0005, 0.02)$.
Under this prior, we draw divergence time parameters from a uniform
distribution of $U(0, 20)$, simulate datasets, and plot the \divt{} values
against the summary statistics calculated from the resulting datasets
(Figure~\ref{figSaturationPlot}).
Clearly, the priors used by \citet{Oaks2012} with upper limits on \divt{} of five
and 10 suffered from little to no effect from saturation.
Even at divergence times of 20 coalescent units, there is still signal in the
summary statistics used by \msb (Figure~\ref{figSaturationPlot}).
Thus, the assertion of \citet{Hickerson2013} does not apply to at least
two of the priors used by \citet{Oaks2012} and, as a result, does not
explain the bias they found.



\section*{Conclusions}
We demonstrate how the approximate Bayesian model choice method implemented in
\msb can be strongly biased away from models with greater parameter space.
We find that this is likely caused by the inappropriate
use of uniform priors on most of the model's parameters.
These priors necessitate the use of broad priors that place high prior density
in unlikely regions of parameter space, less the risk of excluding the truth
\emph{a priori}.
This likely hinders the marginal likelihoods of models with greater parameter
space, either due to more divergence time parameters or broader prior
distributions on those parameters.
We show that the empirical Bayesian model-averaging approach of
\citet{Hickerson2013} does not mitigate this bias, but rather causes it to
manifest by sampling predominantly from models that may exclude the true values
of the parameters.
Exploring cogent prior probability distributions on most of the model's
parameters, in addition to different priors over divergence models, should
help mitigate the methods biases.

The work presented herein follows the principles of Open Notebook Science.
All aspects of the work were recorded in real-time via version-control
software and are publicly available at
\href{https://github.com/joaks1/msbayes-experiments}{https://github.com/joaks1/msbayes-experiments}.
All information necessary to reproduce our results are provided there.




\section*{Acknowledgments}
We thank the National Science Foundation for supporting this work (DEB
1011423).
J.\ Oaks was also supported by the University of Kansas (KU) Office of Graduate
Studies, Society of Systematic Biologists, Sigma Xi Scientific Research
Society, KU Department of Ecology and Evolutionary Biology, and the KU
Biodiversity Institute.

\bibliography{../bib/references}
% \bibliography{references}

%% LIST OF FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\singlespacing

\renewcommand\listfigurename{Figure Captions}
\cftsetindents{fig}{0cm}{2.2cm}
\renewcommand\cftdotsep{\cftnodots}
\setlength\cftbeforefigskip{10pt}
\cftpagenumbersoff{fig}
\listoffigures


\end{linenumbers}

%% TABLES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\singlespacing

%:TABLE-re-analysis
% \begin{table}[htbp]
%     \sffamily
%     \footnotesize
%     \addtolength{\tabcolsep}{-0.08cm}
%     \rowcolors{2}{}{myGray}
%     %\captionsetup{font=footnotesize}
%     \caption{Results of the model-averaging approach of \citet{Hickerson2013}
%         applied to the Philippines dataset of \citet{Oaks2012} using eight
%         prior models (see \citet{Hickerson2013} Table 1).  The results of (1)
%         \citet{Hickerson2013}, (2) our full re-analysis, and of (3) repeating
%         the final rejection step of \citet{Hickerson2013} using the prior means and
%         standard deviations to normalize the summary statistics, are all compared.}
%     \centering
%     \begin{tabular}{ l l l l }
%         \toprule
%          & Hickerson et al. (2013) & Re-analysis & blah \\
%         $\hat{\numt{}}$ & 2 & 1 & 1 \\
%         $p(\numt{} = 1 | \ssSpace)$ & 0.062 & 0.206 & 0.146 \\
%         $BF_{\numt{} = 1, \numt{} \neq 1}$ 1.39 & 5.45 & 3.59 \\
%         $p(\vmratio{} < 0.01 | \ssSpace)$ & 0.298 & 0.416 & 0.361 \\
%         $BF_{\vmratio{} < 0.01, \vmratio{} \geq 0.01}$ 6.65 & 11.16 & 8.85 \\
%         $\hat{M}$ & 1 & 2 & 2 \\
%         $p(M_1 | \ssSpace)$ & 0.513 & 0.358 & 0.376 \\
%         $p(M_2 | \ssSpace)$ & 0.286 & 0.369 & 0.389 \\
%         $p(M_3 | \ssSpace)$ & 0.036 & 0.086 & 0.089 \\
%         $p(M_4 | \ssSpace)$ & 0.002 & 0.012 & 0.011 \\
%         $p(M_5 | \ssSpace)$ & 0.142 & 0.129 & 0.107 \\
%         $p(M_6 | \ssSpace)$ & 0.020 & 0.043 & 0.024 \\
%         $p(M_7 | \ssSpace)$ & 0.001 & 0.003 & 0.004 \\
%         $p(M_8 | \ssSpace)$ & 0.000 & 0.000 & 0.001 \\
%         \bottomrule
%     \end{tabular}
%     \label{tabModelChoiceEmpirical}
% \end{table}

%:TABLE-model-choice-empirical
\begin{table}[htbp]
    \sffamily
    % \footnotesize
    \addtolength{\tabcolsep}{-0.08cm}
    \rowcolors{2}{}{myGray}
    %\captionsetup{font=footnotesize}
    \caption{Results of the model-averaging approach of \citet{Hickerson2013}
        applied to the Philippines dataset of \citet{Oaks2012} using three sets
        of prior models. All models used priors on population size of
        $\meanDescendantTheta{} \sim U(0.0001,0.1)$ and $\ancestralTheta{} \sim
        U(0.0001, 0.05)$, and differ only in their prior on divergence time
        (\divt{}) parameters.  Each set of five models differ only in the
        divergence time prior used for the model with the narrowest prior:
        $M_1$ ($\divt{} \sim U(0, 0.1)$), $M_{1A}$ ($\divt{} \sim U(0, 0.01)$),
        or $M_{1B}$ ($\divt{} \sim U(0, 0.001)$). The approximate posterior
        probability of each model ($p(M_i \given \ssSpace)$) is given for each
        of the three analyses.  The posterior estimates are based on 10,000
        samples retained from $1\e6$ prior samples
    from each model.}
    \centering
    \begin{tabular}{ l l l l l }
        \toprule
        & & \multicolumn{3}{c}{$p(M_i \given \ssSpace)$} \\
        \cmidrule(){3-5}
        Model & \divt{} prior & $M_{*}=M_1$ & $M_{*}=M_{1A}$ & $M_{*}=M_{1B}$ \\
        \midrule
        $M_*$ & --        & 0.899 & 0.821 & 0.673 \\
        $M_2$ & $U(0,1)$  & 0.079 & 0.136 & 0.251 \\
        $M_3$ & $U(0,5)$  & 0.013 & 0.026 & 0.044 \\
        $M_4$ & $U(0,10)$ & 0.006 & 0.012 & 0.022 \\
        $M_5$ & $U(0,20)$ & 0.003 & 0.005 & 0.010 \\
        \bottomrule
    \end{tabular}
    \label{tabModelChoiceEmpirical}
\end{table}

\clearpage

%% FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\mFigure{../../response-redux/hickerson-et-al-posterior/hickerson-posterior-1k/mean_by_dispersion.pdf}{
    The joint posterior of the mean (\meant{}{}) and dispersion index ($\vmratio{} = 
    \vart{}{}/\meant{}{}$) of divergence times for 22 vertebrate taxon pairs as
    estimated by \citet{Hickerson2013} (see Figure 2B of \citet{Hickerson2013}).
    The posterior samples are color-coded to indicate the erroneous mixture of
    timescales in the analysis of \citet{Hickerson2013};
    grey = $0.05/\mutationRate$ generations and
    black = $0.02/\mutationRate$ generations.
}{figJointPosteriorHickerson}

\mFigure{../images/coin_flip.pdf}{
    A plot of three beta probability density functions that represent a prior
    (black; $beta(0.5, 0.5)$), true posterior (blue; $beta(4.5, 1.5)$), and
    empirical Bayes density (red; $beta(8.5, 2.5)$) for a dataset of five
    Bernoulli trials, four of which are successes.
}{figCoinFlip}

\mFigure{../../model-choice/priors-for-pc-plot/pc-plots.pdf}{
    The graphical checks recommended by \citet{Hickerson2013} for three prior
    models: (A) $M_1$ ($\divt{} \sim U(0, 0.1)$), (B) $M_{1A}$ ($\divt{} \sim
    U(0, 0.01)$), and (C) $M_{1B}$ ($\divt{} \sim U(0, 0.001)$).
    The plots project the summary statistics from 1000 random samples from each
    model onto the first two orthogonal axes of a principle component analysis,
    with the blue dot representing the observed summary statistics from the 22
    population pairs of Philippine vertebrates.
}{figPCA}

%% Exclusion simulation results
\mFigure{../../model-choice/results/m1-1-sim/pymsbayes-results/num_tau_excluded.pdf}{
    The propensity of the model-averaging approach of \citet{Hickerson2013} to
    exclude the truth.
    The plots illustrate the number of true \divt{} parameters excluded from analyses of
    simulated datasets where \divt{} for 22 pairs of populations is drawn from an
    exponential distribution, $\divt{} \sim Exp(2)$.
    The plots represent (A) unadjusted and (B) GLM-adjusted estimates from 1000
    simulation replicates analyzed using $5\e6$ samples from the prior.
    The proportion of simulation replicates in which at least one true
    parameter value is excluded from the model preferred by a Bayes factor
    ($p(\divt{} \notin \hat{M})$) is also given.
}{figExclusionSimTau}

\mFigure{../../model-choice/results/m1-1-sim/pymsbayes-results/prob_of_exclusion.pdf}{
    The propensity of the model-averaging approach of \citet{Hickerson2013} to
    exclude the truth.
    The plots illustrate the estimated probability of excluding at least one
    true \divt{} value from analyses of simulated datasets where \divt{} for 22
    pairs of populations is drawn from an exponential distribution, $\divt{}
    \sim Exp(2)$.
    The plots represent (A) unadjusted and (B) GLM-adjusted estimates from 1000
    simulation replicates analyzed using $5\e6$ samples from the prior.
    The proportion of simulation replicates in which there is strong support
    for at least one true parameter value being excluded from the model
    ($p(BF_{\divt{} \notin M, \divt{} \in M} > 10)$) is also given.
}{figExclusionSimProb}

%% Power results
\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_accuracy_omega_median.pdf}{
    The accuracy of the model-averaging approach of \citet{Hickerson2013} to
    estimate the dispersion index of divergenct times (\vmratio{}) from
    analyses of simulated datasets where \divt{} for 22 pairs of populations is
    drawn from a series of uniform distributions, $\divt{} \sim U(0,
    \divt{max})$.
    The proportion of estimates less than the true value of,
    $p(\hat{\vmratio{}} < \vmratio{})$, is given for each \divt{max}.
    Each plot represents unadjusted median estimates from 500 simulation
    replicates analyzed using $5\e6$ samples from the prior.
}{figPowerAccOmegaMedian}

\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_accuracy_omega_mode_glm.pdf}{
    The accuracy of the model-averaging approach of \citet{Hickerson2013} to
    estimate the dispersion index of divergenct times (\vmratio{}) from
    analyses of simulated datasets where \divt{} for 22 pairs of populations is
    drawn from a series of uniform distributions, $\divt{} \sim U(0,
    \divt{max})$.
    The proportion of estimates less than the true value of,
    $p(\hat{\vmratio{}} < \vmratio{})$, is given for each \divt{max}.
    Each plot represents GLM-regression-adjusted mode estimates from 500
    simulation replicates analyzed using $5\e6$ samples from the prior.
}{figPowerAccOmegaModeGLM}

\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_psi_mode.pdf}{
    The bias of the model-averaging approach of \citet{Hickerson2013} to infer
    clustered divergences in our simulation-based power analyses.
    The plots illustrate the estimated number of divergence events ($\hat{\numt{}}$)
    from analyses of simulated datasets where \divt{} for 22 pairs of populations is drawn from a series of
    uniform distributions, $\divt{} \sim U(0, \divt{max})$.
    The estimated probability of the method inferring on divergecne event, $p(\numt{} = 1)$,
    is given for each \divt{max}.
    Each plot represents 500 simulation replicates analyzed using $5\e6$
    samples from the prior.
}{figPowerPsiMode}

\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_omega_prob.pdf}{
    The bias of the model-averaging approach of \citet{Hickerson2013} to
    support one divergence event in our simulation-based power analyses.
    The plots illustrate histograms of the estimated posterior probability that
    the dispersion index of divergence times is less than 0.01 ($p(\vmratio{} <
    0.01 | \ssSpace)$) from analyses of simulated datasets where \divt{} for 22
    pairs of populations is drawn from a series of uniform distributions,
    $\divt{} \sim U(0, \divt{max})$.
    The proportion of simulation replicates that strongly support one
    divergence event, $p(BF_{\vmratio{} < 0.01, \vmratio{} \geq 0.01} > 10)$,
    is given for each \divt{max}.
    Each plot represents 500 simulation replicates analyzed using $5\e6$
    samples from the prior.
}{figPowerOmegaProb}

\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_num_excluded.pdf}{
    The propensity of the model-averaging approach of \citet{Hickerson2013} to exclude
    the truth in our simulation-based power analyses.
    The plots illustrate the number of true \divt{} parameters excluded from analyses of
    simulated datasets where \divt{} for 22 pairs of populations is drawn from a series of
    uniform distributions, $\divt{} \sim U(0, \divt{max})$. The proportion
    of simulation replicates in which at least one true parameter value is excluded from
    the model preferred by a Bayes factor ($p(\divt{} \notin \hat{M})$) is
    given for each \divt{max}.
    Each plot represents 500 simulation replicates analyzed using $5\e6$
    samples from the prior.
}{figPowerNumExcluded}

\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_prob_exclusion.pdf}{
    The propensity of the model-averaging approach of \citet{Hickerson2013} to
    exclude the truth in our simulatoin-based power analyses.  The plots
    illustrate the estimated posterior probability of excluding at least one
    true \divt{} value from analyses of simulated datasets where \divt{} for 22
    pairs of populations is drawn from a series of uniform distributions,
    $\divt{} \sim U(0, \divt{max})$. The proportion of simulation replicates in
    which there is strong support for at least one true parameter value being
    excluded from the model ($p(BF_{\divt{} \notin M, \divt{} \in M} > 10)$) is
    given for each \divt{max}.
    Each plot represents 500 simulation replicates analyzed using $5\e6$
    samples from the prior.
}{figPowerProbExclusion}

%% Validation results
\mFigure{../../model-choice/results/validation/results/pymsbayes-results/plots/mc_behavior.pdf}{
    An assessment of the approximate Bayesian model-averageing approach of
    \citet{Hickerson2013} under the ideal conditions when the prior model
    is correct (i.e., the pseudo-replicate datasets are simulated from
    parameters drawn from the same prior distributions used in the
    analysis).
    The plots show the relationship between the estimated posterior and true
    probability of (A \& C) $\numt{}=1$ and (B \& D) $\vmratio{} < 0.01$, based
    on 50,000 simulations.
    The results summarize the (A \& B) unadjusted and (C \& D) GLM-adjusted
    posterior estimate from each simulation replicate.
    The prior settings for all replicates included five prior models with
    $\meanDescendantTheta{} \sim U(0.0001, 0.1)$ and $\ancestralTheta{} \sim
    U(0.0001, 0.05)$ for all five models, and
    $M_1: \divt{} \sim U(0, 0.1)$,
    $M_2: \divt{} \sim U(0, 1)$,
    $M_3: \divt{} \sim U(0, 5)$,
    $M_4: \divt{} \sim U(0, 10)$, and
    $M_5: \divt{} \sim U(0, 20)$.
    The number of samples from the prior was $2.5\e6$.
    The simulated data structure was 8 population pairs, with a single 1000
    bp locus sampled from 10 individuals from each population.  The 50,000
    estimates of the posterior probability of one divergence event were
    assigned to 20 bins of width 0.05.
    The estimated posterior probability of each bin is plotted against the
    proportion of replicates in that bin with a true value consistent with
    one divergence event (i.e., $\numt{}=1$ or $\vmratio{} < 0.01)$.
}{figValidationMCBehavior}

\mFigure{../../response-redux/results/sampling-error/pymsbayes-results/omega_over_sampling.pdf}{
    Traces of the estimated lower and upper limits of the 95\% highest posterior density (HPD)
    interval of \vmratio{} (the dispersion index of divergence times) as 100 million prior
    samples are accumulated. Each pair of points is based on 1000 posterior samples retained
    from the prior. Both (A) unadjusted and (b) GLM-regression-adjusted estimates are shown.
    Prior settings were $\divt{} \sim U(0,10)$, $\meanDescendantTheta{} \sim U(0.0005, 0.04)$,
    and $\ancestralTheta{} \sim U(0.0005, 0.02)$.
}{figSamplingError}

%% Model-choice tau prior
\mFigure{../../model-choice/tau-prior/tau_prior.pdf}{
    The prior distribution on divergence times imposed by the model-averaging prior
    comprised of five models with different uniform priors on \divt{}:
    $M_1$ ($\divt{} \sim U(0, 0.1)$), $M_2$ ($\divt{} \sim U(0, 1)$), $M_3$
    ($\divt{} \sim U(0, 5)$), $M_4$ ($\divt{} \sim U(0, 10)$), $M_5$ ($\divt{}
    \sim U(0, 20)$).
}{figMCTauPrior}

%:FIGURE-saturation plot
\mFigure{../../saturation/saturation-plot.pdf}{
    The summary statistics $\pi$ \citep{Tajima1983} and $\pi_{net}$
    \citep{Takahata1985} as a function of divergence time between populations.
    Each plot represents 1100 pairs of parameter draws and summary statistics
    calculated from the simulated data.
    Prior settings for the simulations were $\divt{} \sim U(0, 20)$,
    $\meanDescendantTheta{} \sim U(0.0005, 0.04)$, and $\ancestralTheta{} \sim
    U(0.0005, 0.02)$.
}{figSaturationPlot}

% \mFigure{../../response-redux/results/hickerson/pymsbayes-results/pymsbayes-output/d1/m12345678-combined/mean_by_dispersion.pdf}{
%     Results of reanalysis.
% }{figJointPosterior}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SUPPORTING INFO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{page}{1}

\singlespacing

\renewcommand{\refname}{\noindent\MakeUppercase{\LARGE\sffamily\upshape supporting information}}

% PUT MAIN TEXT CITATION HERE
% \begin{thebibliography}{1}
% \providecommand{\natexlab}[1]{#1}
% \providecommand{\url}[1]{\texttt{#1}}
% \providecommand{\urlprefix}{URL }

% \bibitem

% \end{thebibliography}


%% SUPPL TABLES %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setcounter{table}{0}


\end{document}

