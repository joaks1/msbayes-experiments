%&<latex>
\documentclass[letterpaper,12pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% preamble %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{../utils/preamble.tex}
\input{../utils/macros.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\doublespacing
\raggedright
\setlength{\parindent}{0.5in}
\begin{linenumbers}

\begin{titlepage}
    \begin{flushleft}
        \sffamily

        \MakeUppercase{\large\bfseries Why you should not fix a biased
        model-choice method by adding an additional dimension of model choice:
        A reply to Hickerson et al.}

        \vspace{12pt}
        \textbf{Running head:} \MakeUppercase{Approximate Bayesian model
        choice}

        \vspace{12pt}
        Jamie R. Oaks$^{1,2,5}$, Jeet Sukumaran$^{1,2}$, Jacob A.
        Esselstyn$^{3}$, Charles W. Linkem$^{1,2}$, Cameron D.
        Siler$^{4}$, Mark T. Holder$^{2}$ and Rafe M. Brown$^{1,2}$

        \bigskip
        $^1$\emph{Biodiversity Institute\\
            $^2$Department of Ecology and Evolutionary Biology\\ 
            University of Kansas\\
            %1345 Jayhawk Blvd\\
            Lawrence, KS 66045\\
            USA}\\[.1in]
        $^3$\emph{Biology Department\\
            McMaster University\\
            % Life Sciences Building Rm 328\\
            Hamilton, Ontario L8S4K1\\
            Canada}\\[.1in]
        $^4$\emph{Department of Biology\\
            University of South Dakota\\
            Vermillion, SD 57069\\
            USA}\\[.1in]
        $^5$\emph{Corresponding author} (\href{mailto:joaks1@ku.edu}{\tt
        joaks1@ku.edu})\\

    \end{flushleft}
\end{titlepage}

{\sffamily
    \noindent\textbf{ABSTRACT} \\
    \noindent Abstract here \ldots

    \vspace{12pt}
    \noindent\textbf{KEY WORDS: } 
}

\newpage
\noindent Recently, this journal has served as a venue for discussion of the
potential pitfalls of approximate Bayesian methods of comparative
phylogeographical model choice.
The discussion is centered around the approximate Bayesian model choice method,
\msb, which estimates the temporal distribution of divergences among
co-distributed pairs of taxa.
\citet{Oaks2012} published their findings that the method \msb can often be
biased towards inferring models of temporally clustered divergence times
among taxon pairs.
\citet{Hickerson2013} has published a response to this paper where they
present a model-averaging approach that they conclude circumvents the poor
behavior of the method revealved by \citet{Oaks2012}.
Both of these papers are largely in agreement.
In fact, \citet{Hickerson2013} reiterate a lot of the discussion of
\citet{Oaks2012} regarding the impact of broad uniform priors on Bayesian model
choice.
The main differences in their perspectives are centered around
(1) the means by which broad uniform priors cause the poor behavior of \msb,
and
(2) how to potentially ameliorate this issue.

\citet{Oaks2012} conclude the primary mechanism by which broad priors cause the
poor behavior of the method is likely the low marginal likelihoods of
parameter-rich models integrated over vast parameter space with low probability
of producing the data, yet relatively high prior density \citep[this is often
referred to as Lindley's paradox;][]{Lindley1957}.
Note, this suggests the bias is extrinsic to \msb, and the numerical
approximation machinery of the method could be sound.
\citet{Hickerson2013} take a more pessimistic view of the bias, and suggest it
is intrinsic to \msb, i.e., the method's rejection algorithm is
inefficient and will be increasingly biased as the overall space of the model
increases, either as a function of the number of taxon pairs or the width of
the uniform priors on nuisance parameters.
They support their position by giving a probabilistic argument that focuses on
only one of the three prior models used by \citet{Oaks2012}.
We show that this argument is based on dubious assumptions, and does not
explain the bias of the method found in several analyses of \citet{Oaks2012}.
We reiterate several of the findings of \citet{Oaks2012} as well as present
results of additional analyses, which strongly suggest that Lindley's paradox
is playing a large role in the poor behavior of the method.

\citet{Oaks2012} suggest that more cogent prior probability distributions on
divergence models and nuisance parameters could mitigate the effect of
Lindley's paradox.
They also layout a set of simulation-based procedures for determining power,
accuracy, and robustness of the method given a dataset, and recommend any
application of \msb should be accompanied by such procedures, especially
if any biological conclusions are going to drawn from the results.
\citet{Hickerson2013} present an approximate Bayesian model-averaging approach
for accommodating uncertainty in selecting among empirically guided priors, and
champion the method as a means of avoiding the pitfalls raised by
\citet{Oaks2012}.
Unfortunately, we show that fundamental errors in this approach render some of
their results invalid and leave the remaining results difficult to interpret.
Furthermore, we follow the advice of \citet{Oaks2012} and present
simulation-based assessments of Hickerson et al.'s \citeyear{Hickerson2013}
approach.
Our results demonstrate that the method is biased and dangerous.
The approach merely provides another means by which the model can ``escape''
large parameter space, and the bias towards models with smaller space still
remains.
This bias can be dangerous and often excludes the true parameter space due to
the use of uniform priors.
Furthermore we discuss the potential theoretical and practical problems of
empirical Bayesian model choice.



\section*{Hickerson et al.'s re-analysis of the Philippines data}
\citet{Hickerson2013} re-analyze the dataset of \citet{Oaks2012} using a
model-averaging approach, where they place a discrete uniform prior over eight
different prior models (see Table 1 of \citet{Hickerson2013}).
In our initial attempts to explore the behavior of this approach, we found
a fundamental error in the methodology of \citet{Hickerson2013}:
their model mixes different time units.

Each of the eight prior models used in the re-analysis by \citet{Hickerson2013}
has one of two priors on the mean population size of the descendant populations
of each taxon pair:
$\meanDescendantTheta{} \sim U(0.0001, 0.1)$ or
$\meanDescendantTheta{} \sim U(0.0005, 0.04)$ or
As described in \citet{Oaks2012}, the divergence time parameters of the model
implemented in \msb are scaled relative to a constant reference population
size, \myTheta{C}.
This reference population size is defined in terms of the upper limit on the
uniform prior on the mean population size of the descendant populations,
\meanDescendantTheta{}, such that for the prior $\meanDescendantTheta{} \sim
U(\uniformMin{\meanDescendantTheta{}},\uniformMax{\meanDescendantTheta{}})$,
$\myTheta{C} = \uniformMax{\meanDescendantTheta{}}/2$.
Thus, the model used by \citet{Hickerson2013} mixes two different units of
time.
In other words, some of their prior and posterior samples are in units of
$0.05/\mutationRate$ generations, whereas other are in units of
$0.02/\mutationRate$ generations.

The fact that their posterior samples are in different units makes the results
of \citet{Hickerson2013} difficult to interpret, and renders their
regression-adjusted results invalid; a fundamental assumption of regression is
that all of the values of the response variable are in the same units.
Thus, the results in section ``Improved Sampling Efficiency by Prior Weighting
Supports Asynchronous and Recent Divergence for the Philippines Vertebrate
Data'' and presented in Figure 2 of \citet{Hickerson2013} should be
disregarded.
The error is easily illustrated by re-plotting their results with the different
time units indicated (Figure~\ref{figJointPosteriorHickerson}.

% In addition to this error, we were not able to reproduce the results of
% \citet{Hickerson2013}.
% We followed their methods, generating $5\e6$ prior samples for each of the
% eight models in their Table 1.
% We retained the 10,000 samples from each model with the smallest Euclidean
% distance to the observed summary statistics, and subsequently retained 10,000
% samples from the remaining 80,000.


\section*{The dangers of empirical Bayesian model choice}
\citet{Hickerson2013} repeatedly refer to the prior distributions used by
\citet{Oaks2012} as ``poorly selected''.
However, this is bit misleading, as \citet{Oaks2012} discuss in detail how they
selected their priors (see their section ``Specifying and simulating the joint
prior'').
They detail how the being limited to uniform prior distributions necessitates
awkward broad priors to avoid excluding the truth, a priori.
What \citet{Hickerson2013} really mean by ``poorly selected'' is that
\citet{Oaks2012} did not use their data to inform their initial prior
distributions (i.e., they did not use an empirical Bayesian approach).
\citet{Oaks2012} did use empirically informed priors when assessing
both the prior sensitivity of the method and determining the power of \msb
under the ``best'' possible real-world conditions.
\citet{Oaks2012} discuss the potential dangers of taking an empirical Bayesian
model choice (see last paragraph of ``Assessing prior sensitivity of \msb''),
and we expand on this here by exploring the theoretical and practical
implications of using an empirical Bayes approach to model choice.

\subsection*{Theoretical implications of empirical priors for Bayesian model
choice}
\begin{linenomath}
Bayesian inference is a rational method of inductive learning in which Bayes'
rule is used to update our beliefs about a model as new information becomes
available.
If we let \allParameterValues represent the set of all possible parameter
values, we can define a prior distribution for all $\theta \in
\allParameterValues$ such that $p(\theta)$ describes our belief that any given
\myTheta{} is the true value of the parameter.
If we let \allDatasets represent all possible datasets then we can 
define a sampling model for all $\theta \in
\allParameterValues$ and $\alignment{}{} \in \allDatasets$ such that
$p(\alignment{}{} | \theta)$ measures our belief that any dataset \alignment{}{}
will be generated by any model state \myTheta{}.
After collecting a new dataset \alignment{i}{}, we can use Bayes' rule to
calculate the posterior distribution
\begin{equation}
    p(\myTheta{} \given \alignment{i}{}) = \frac{p(\alignment{i}{} \given 
    \myTheta{})p(\myTheta{})}{p(\alignment{i}{})},
    \label{eq:bayesrule}
\end{equation}
where
\begin{equation}
    p(\alignment{i}{}) = \int_{\myTheta{}} p(\alignment{i}{} \given
    \myTheta{})p(\myTheta{}) d\myTheta{}.
\end{equation}
The posterior distribution is a measure of our beliefs after seeing the new
information.
\end{linenomath}

This is an elegant method of updating our beliefs as data are accumulated.
However, this all hinges on the fact that the prior ($p(\myTheta{})$) is
defined for all possible parameters independently of the new data being
analyzed.
Any other datasets or external information can safely be used to inform our
beliefs about $p(\myTheta{})$.
However, if the data are used to both inform the prior and calculate the
posterior, the prior becomes conditional on the data, and Bayes' rule breaks
down.

Now, this is not to say that empirical Bayesian approaches not useful.
Quite the contrary.
Empirical Bayes is a well studied branch of Bayesian statistics that
has given rise to a many powerful inference methods.
It is true that empirical Bayes methods have an uncertain theoretical basis and
do not yield a valid posterior distribution from Bayes' rule \citep[e.g.,
empirical Bayesian estimates of the posterior are often too narrow, off-center,
and misshapen;][]{Morris1983,Laird1987,Carlin1990,Efron2013}.
Nevertheless, empirical Bayes methods are often a powerful means of parameter
estimation that display favorable frequentist properties.
Furthermore, many post-hoc correction methods have been developed for
estimating confidence-intervals from empirical Bayes estimates of posterior
distributions that often display good frequentist coverage behavior
\citep{Morris1983,Laird1987,Laird1989, Carlin1990,Hwang2009}.

Whereas empirical Bayes approaches can provide powerful methods for parameter
estimation, a theoretical justification for empirical Bayes approaches to model
choice is much more dubious.
In Bayesian model choice, the goal is usually not to estimate parameters, but
to estimate the relative probabilities of candidate models.
Unlike parameter estimates, posterior probabilities represent a summary of the
entire posterior distribution.
Thus, given that empirical Bayesian posterior distributions are not accurate,
there is no guarantee regarding the accuracy of probabilities summarized from
them.

This can be demonstrated with a simple, albeit contrived, example.
Let us say that principal investigator Mary and her new postdoc Will are
interested in the hypothesis that the mass of George Washington's periwig
renders the United States' quarter dollar unfair.
That is to say that our null hypothesis is that the probability of a US quarter
landing heads when tossed is less than 0.5 ($\theta < 0.5$).
They can certainly evaluate this hypothesis, they can have undergraduate
worker, Joe, flip the coin for them while the tabulate the results.
But being Bayesians, before they call Joe into the lab, they agree on a
prior probability to place on the set of all possible probabilities that
the quarter will land heads when it is flipped.
Given their prior knowledge that Joe moonlights as a magician, and is notorious
for performing coin and card tricks in the lab, they suspect there is a good
chance that Joe will use his slight-of-hand skills to replace their quarter
with one of his two-headed or two-tailed coins he has up his sleeve at all
times.
So, knowing that the beta distribution is the conjugate prior for a binomial
likelihood, they decide to use a $beta(a=0.5, b=0.5)$ prior distribution
(Figure~\ref{figCoinFlip}).

Mary calls Joe into the lab, and hands him a regular-issue US quarter minted in
1992.
After five tosses, four of which was heads, Joe decides that academics are
crazy and leaves the lab to pursue a major in theatre.  Mary and Will, both
being computational biologists, are satisfied with their
empirical dataset of $y = 4$ heads out of $N = 5$ trials.
They know from the conjugacy of the beta prior, that the posterior distribution
has the nice analytical form $\theta|y,N \sim beta(a + y, b + N - y)$, which
in this case is simply $beta(4.5, 1.5)$; this is the true posterior distribution
of $\theta$ given their prior belief and data (Figure~\ref{figCoinFlip}).
This allows them to plug these values into the beta cumulative distribution
function to determine that the posterior probability of their hypothesis is
$p(\theta < 0.5 | y=1, N=5) = 0.088$.
Given their prior belief and dataset, this indeed is the correct posterior
probability of the hypothesis, and Mary and Will should now update their
posterior belief accordingly.

However, upon reflecting on the results of their experiment
(Figure~\ref{figCoinFlip}), Mary and Will regret not trusting Joe to refrain
from turning their experiment into a magic trick.
Their prior looks very ``poorly selected,'' and if they had only
had more faith in Joe's integrity, they would have selected a much better
prior. Clearly form their results, they should have used a prior centered
around 0.2; their data suggest a prior of $beta(4.5, 1.5)$ would have been
much more appropriate.
Rather than resort to flipping the coin five more times, Mary and Will decide
to redo their analysis using the much better ``prior'' of $p(\theta) \sim
beta(4.5, 1.5)$.
This gives a ``posterior'' of $\theta|y,N \sim beta(8.5, 2.5)$, and a
probability of their hypothesis of $p(\theta < 0.5 | y=1, N=5) = 0.026$.
Now convinced that the mass of President Washington's head renders the US
Quarter unfair, Will begins composing an e-mail of complaint for the U.S.\
Mint.

If Joe's flips were a representative sample, Mary and Will's empirical Bayes
estimate might very well be a better estimator for the parameter \myTheta{}.
However, their empirical Bayes estimate of the probability of their hypothesis
is incorrect and biased.
This simple example shows how parameter estimation is fundamentally different
from estimating the probability of a model.
While empirical informed priors can be used to obtain well-behaved parameter
estimators, using them for model choice is much less certain.


\subsection*{Practical dangers of narrow uniform priors for Bayesian model
choice}
Given that \msb can be biased toward models with smaller parameter space, and
Hickerson et al.'s \citeyear{Hickerson2013} use of narrow, empirically informed
uniform priors on divergence times, we sought to determine whether such an
approach will often exclude the truth (i.e., the model-averaged posterior will
be dominated by models that excude the true values of parameters of the model).
We explored this possiblity in two ways.
First, we reanalyzed the Philippines dataset using the model-averaging approach
of \citet{Hickerson2013}, but set one of prior models with a uniform prior on
divergence times that is unrealistically narrow, and almost certainly excludes
the truth.
If Lindley's paradox causes the method to prefer models with less parameter
space, we expect \msb will preferentially sample from this incorrect model
yielding a marginal posterior that is invalid (i.e., the model-averaged
posterior will be dominated by an incorrect model that excludes the truth).
Second, we generated simulated datasets for which the divergence times are
drawn form an exponential distribution and applied the approach of
\citet{Hickerson2013} to each of them to see how often the method excludes the
truth.

The empirically guided, narrowest prior of \citet{Hickerson2013} ($\divt{} \sim
U(0,0.1)$) likely excludes the true values for at least some of the Philippine
taxa.
They set this prior to match the 95\% highest posterior density (HPD) interval for
the mean divergence time estimated under one of the priors used by
\citet{Oaks2012} (see Table 2 and 3 of \citet{Oaks2012}).
Given this interval estimate is for the \emph{mean} divergence time across all
22 taxa, it seems odd to set this as the prior, as many of the taxon pairs
likely diverged at times older than the upper limit.
Furthermore, the prior is \emph{excluded} from the 95\% HPD interval estimates
of the mean divergence time under the other two prior analyses (under these
priors the 95\% HPD is approximately 0.3--0.6; see Table~6 of
\citet{Oaks2012}).

For our re-analysis of the Philippines dataset we used the model-averaging
approach of \citet{Hickerson2013}, but with different prior models to avoid
mixing time units. We used five priors, all of which had priors on population
sizes of $\meanDescendantTheta{} \sim U(0.0001, 0.1)$ and
$\ancestralTheta{} \sim U(0.0001, 0.05)$.
Following \citet{Hickerson2013}, each of these priors had the following
priors on divergence time parameters:
$M_1$, $\divt{} \sim U(0, 0.1)$;
$M_2$, $\divt{} \sim U(0, 1)$;
$M_3$, $\divt{} \sim U(0, 5)$;
$M_4$, $\divt{} \sim U(0, 10)$; and
$M_5$, $\divt{} \sim U(0, 20)$.
For our prior, we simulated $1\e6$ random samples from each of the models
for a total sample of $5\e6$.
For each model, we retained the 10,000 samples with the smallest Euclidean
distance from the observed summary statistics, standardizing the statistics
using the prior means and standard deviations of the given model.
We then retained the 10,000 samples with the smallest Euclidean distance from
the observed summary statistics, this time standardizing the statistics using
the prior means and standard deviations across all five models.
We then repeated this analysis twice, replacing the $M_1$ with
$M_1A$ and $M_1B$, which differ only by having priors on divergence
times of $\divt{} \sim U(0, 0.01)$ and $\divt{} \sim U(0, 0.001)$,
respectively.
While we suspect the prior of $\divt{} \sim U(0, 0.1)$ used by
\citet{Hickerson2013} likely excludes the true divergence times of
at least some of the 22 taxa, we are nearly certain that these
narrower priors are incorrect and exclude the truth.

Our results show that the model-averaging approach of \citet{Hickerson2013}
does not reduce the methods bias towards models with smaller parameter space,
but rather allow it to manifest in a very dangerous way.
The method strongly prefers the prior model with the narrowest distribution on
divergence times across all three of our analyses, even when this model is
almost certainly incorrect and excludes the true divergence times of the
Philippine taxa.
Unfortunately, ``checking'' the priors by plotting the summary statistics from
1000 random samples of each prior model along the first two orthogonal axes of
a principle component analysis, as recommended by \citet{Hickerson2013},
provides no warning of a problem (Figure~\ref{figPCA}.
Given that the results of \citet{Hickerson2013} strongly prefer the models with
the narrowest prior on divergence times, it seems quite likely that their
model-averaged results are dominated by models that exclude at least some of
the 22 true divergence times, making their results difficult to interpret.

To better quantify the propensity of Hickerson et al.'s
\citeyear{Hickerson2013} approach to exclude the truth, we simulated
1000 pseudo-observed datasets in which the divergence times for the
22 population pairs are drawn randomly from an exponential distribution
with a mean of 0.5 ($\divt{} \sim Exp(2)$). All other parameters were
identically distributed as in the $M_1$--$M_5$ models.
We then repeated the analysis described above, using $5\e6$ prior samples from
$M_1$, $M_2$, $M_3$, $M_4$, and $M_5$, and retaining 1,000 posterior samples
for each of the 1,000 pseudo-observed datasets.

\highLight{Add how we determined exclusion.}
Our results show that the model-averaging approach of \citet{Hickerson2013}
excludes the true values of parameters in 97\% of the replicates (90\% with
GLM-regression adjustment), excluding up to 21 of the 22 true divergence times
(Figure~\ref{figExclusionSimTau}.
Furthermore, the posterior probability of excluding at least one true parameter
value is very high in nearly all of the replicates
(Figure~\ref{figExclusionSimProb}).
We used a Bayes factor (comparing models that exclude the truth to those that
do not) calculated for each replicate of greater than 10 as strong support for
excluding the truth.
With that criterion, 66\% of the replicats (87\% with GLM-regression adjustment)
strongly support the exclusion of true values (Figure~\ref{figExclusionSimProb}).

The results of our empirical and simulation-based analyses clearly demonstrate
the danger of using narrow, empirically guided priors in a Bayesian
model-averaging framework \ldots

\subsection*{Final thought on EB}
Given all of the theoretical and practical issues with empirical Bayes
approaches to Bayesian model choice discussed in the proceeding sections, it is
quite clear why one should use caution before overly criticizing an
investigators choice of priors after having seen the posterior.
In a strictly Bayesian world, this is cheating.
As discussed by \citet{Oaks2012}, prior to analysing the data, there was a large
amount of uncertainty regarding the divergence times of the 22 population pairs
under study.
Two the these pairs represent distinct species, and the taxonomy of many groups
in the Philippines has repeatedly been shown to mask deeply divergent lineages
\citep{RafeDiesmosAlcala2008,Linkem2010,Siler2010,Welton2010,Siler2011HerpMonographs,
Siler2011,Siler2012,RafeStuart2012}.
\highLight{Any new stuff to add to this list of citations?}.
\citet{Oaks2012} also discuss how the sole use of uniform priors in \msb makes
it very difficult for investigators to express their prior uncertainty without
putting a lot of prior density in regions of improbable parameter space.
The alternative, as shown above, is to risk excluding the truth before even
conducting the analysis.






\section*{Our power analysis}
\citet{Hickerson2013} did not conduct a power analysis to accompany their
inference of clustered divergences from the Philippines data (as recommended by
\citet{Oaks2012}).
Thus, we perform perform simulation-based power analyses to explore the
power of the approximate Bayesian model-averaging approach championed
by \citet{Hickerson2013}.

Following \citet{Oaks2012}, we simulated 1000 pseudo-replicate datasets with
\divt{} for each of the 22 population pairs randomly drawn from a uniform
distribution, $U(0, \divt{max})$, where \divt{max} was set to: 0.2, 0.4, 0.6,
0.8, 1.0, and 2.0, in \globalcoalunit generations.
All other parameters were identically distributed as the prior.
Following \citet{Hickerson2013} (but avoiding mixing time units), we used five
priors, all of which had priors on population sizes of $\meanDescendantTheta{}
\sim U(0.0001, 0.1)$ and $\ancestralTheta{} \sim U(0.0001, 0.05)$.
Each of these priors had the following priors on divergence time parameters:
$M_1$, $\divt{} \sim U(0, 0.1)$;
$M_2$, $\divt{} \sim U(0, 1)$;
$M_3$, $\divt{} \sim U(0, 5)$;
$M_4$, $\divt{} \sim U(0, 10)$; and
$M_5$, $\divt{} \sim U(0, 20)$.
For our prior, we simulated $1\e6$ random samples from each of the models
for a total sample of $5\e6$.
For each pseudo-replicate dataset, we retained the 1000 samples from each prior
model with the smallest Euclidean distance from the replicates' summary
statistics, standardizing the statistics using the prior means and standard
deviations of the given model.
Then, for each replicate, we retained the 1000 samples with the smallest
Euclidean distance from the pseudo-observed summary statistics, this time
standardizing the statistics using the prior means and standard deviations
across all five models.
In total, we analyzed 6000 replicate datasets, retaining 1000 model-averaged
posterior samples from each of them.

We find that the approach of \citet{Hickerson2013} to estimate the dispersion
index of divergence times (\vmratio{}) across most of the \divt{max} we
simulated, whether evaluating the unadjusted
(Figure~\ref{figPowerAccOmegaMedian}) or GLM-adjusted
(Figure~\ref{figPowerAccOmegaModeGLM}) posterior estimates.
The method only estimates \vmratio{} relatively well when the simulated
distribution of divergence times is identical to one of the prior models
(Figures~\ref{figPowerAccOmegaMedian}E \& \ref{figPowerAccOmegaModeGLM}E).
This is consistent with the conclusion of \citet{Oaks2012} that \msb lacks
robustness and is highly sensitive to the prior distribution deviating from the
true, underlying distribution of the data.

Furthermore, our results demonstrate that the approach of \citet{Hickerson2013}
consistently infers highly clustered divergences across all the \divt{max} we
simulated (Figure~\ref{figPowerNumExcluded}).
The method is most likely to infer the extreme case of a single divergence event
when populations diverge randomly over the past \globalcoalunit generations.
Even when divergences are random over the past $8\globalpopsize$ generations,
the most likely inference is only two divergence events, and a single 
divergence is still estimated in more than 10\% of the replicates.
It is very interesting to note that as \divt{max} increases, but before the
mode inference is finally pulled away from $\numt{} = 1$, the distribution of
\numt{} estimates closely mirrors the odd U-shaped prior on divergence models
used by \msb (see Figure~\ref{figPowerNumExcluded}E and Oaks et al.'s
\citeyear{Oaks2012} Figure 5B).
This supports the conclusions of \citet{Oaks2012} that this U-shaped
prior coupled with the poor marginal likelihoods of models with many
\divt{} parameters, is a major cause of the method's bias toward
clustered divergence models.

Looking at our simulation results in terms of the posterior probability of the
divergence-time dispersion index supporting the extreme case of one divergence
event (i.e., $p(\vmratio{} < 0.01 \given \ssSpace)$), we find the method
strongly supports one divergence in greater than 27\%
of the replicates across all the \divt{max} we simulated
(Figure~\ref{figPowerOmegaProb}).
Following \citet{Hickerson2013}, We use a Bayes factor of greater than 10 as
the criterion for incorrect inference of a single divergence event.
There is strong support for a single divergence event in more than 90\% of the
replicates when divergences are random over the past $2.4\globalpopsize$
generations, and more than 60\% when over the past $3.2\globalpopsize$
generations or less (Figure~\ref{figPowerOmegaProb}).

Contrary to Hickerson et al.'s \citeyear{Hickerson2013} claim that their
model-averaging approach ``can discriminate complex multispecies histories and
correctly reject synchronous divergence, even when discrete divergence times
differ by much less than \ldots millions of generations,'' our results show
that method is biased toward strongly supporting the extreme case of a single
divergence event when populations diverged random over the last
$8\globalpopsize$ generations.
To roughly put that on the scale for a vertebrate mitochondrial locus, assuming
a mutation rate of $2\e{-8}$ per site per generation, this translates to 5
million generations.
Assuming a mutation rate consistent with nuclear loci of $1\e{-9}$, this is 100
million generations.

The results of our power analysis further demonstrate the
propensity of Hickerson et al.'s \citeyear{Hickerson2013} approach
to exclude true parameter values.
Across all but one of the \divt{max} we simulated, the method excludes the
truth in large proportion of replicates, and across many of the \divt{max} it
will exclude a large proportion of the true divergence time values
(Figure~\ref{figPowerNumExcluded}.
The posterior probability of excluding at least on true divergence value is
also quite high across many of the \divt{max}
(Figure~\ref{figPowerProbExclusion}).
Only when the data are identically distributed as one the prior models does
the method avoid excluding the truth more than 5\% of the time
(Figure~\ref{figPowerNumExcluded}E).

Given our results, we conclude that not only does is the approach of
\citet{Hickerson2013} dangerous in terms of excluding regions of parameter
space containing the true, but still lacks power to detect random variation in
divergence times over the scale of $8\globalpopsize$ generations or more.
This translates to millions of generations for mitochondrial loci, and hundreds
of millions of generations for nuclear loci.



\section*{Their power analysis}
\citet{Hickerson2013} present a power analysis in which they demonstrate that
\msb has the power to infer multiple divergence events when divergence times
are random over hundreds of thousands of generations (rather than millions
as demonstrated by \citet{Oaks2012}.
It is not surprising to find that within certain parameter space the method
has increased power to detect temporal variation in divergences over narrower
time windows than millions of generations.
However, what is important to consider is whether those conditions are relevant
to real world applications of the method.
\citet{Oaks2012} explore the behavior of the method under three divergence time
priors as narrow as 0--5 coalescent units, which is quite narrow considering
that this expresses the prior belief that all 22 taxon pairs diverged within
this window.
\citet{Hickerson2013} limit their power analysis to a single prior of 0--1
coalescent unit.
It seems that being 100\% certain a priori that all taxa under comparison
diverged within the last coalescent unit is not generally applicable to most
empirical systems.
Furthermore, even when such extensive prior information is available to only be
able to detect multiple divergences on the scale of hundreds of thousands of
generations does not seem very ``powerful.''

Also, \citet{Hickerson2013} present information only on the extreme case of a
single divergence event.
As discussed in \citet{Oaks2012}, it seems dubious to consider the estimation
of two divergence events as a success when divergence is random over hundreds
of thousands of generations.
Unlike the results of \citet{Oaks2012} it is impossible to assess the
performance of \msb based on the results presented by \cite{Hickerson2013}
other than to know whether or not it inferred the worst possible scenario of a
single event.
In an empirical system such as the Philippines, where island fragmentation
has occurred at least X times over the past several millions of years, an
estimate where 22 taxa share even a handful of divergence events would be
of biogeographic interest.
\citet{Hickerson2013} seem to suggest in their response that \msb should only
be used as a binary estimator; i.e., was there one event or more?
This is not consistent with how \msb has been presented, used, and interpreted
in the past.

\citet{Hickerson2013} translate their results from units of
\globalcoalunit generations to generations assuming a mutation rate of
$1.92\e{-8}$ mutations per site per generation.
If we tranlate there results to a scale more consistent with rates of mutation
of nuclear loci ($1\e{-9}$), even under the very optimistic prior settings used
by \citet{Hickerson2013} the method can only reliably reject the extreme case
of one divergence event when divergences are random over a period of more than
3 million generations.
Again, considering \citet{Hickerson2013} are assuming prior knowledge that
all population pairs diverged within the past coalescent unit, this
does not seem particularly powerful.



\section{Validation analyses}



\section*{Lindley's paradox versus sampling error}
One of the points of disagreement between \citet{Hickerson2013} and
\citet{Oaks2012} is the underlying mechanism causing the bias toward models
with clustered divergences (i.e., models with few divergence events).
\citet{Oaks2012} present simulations based analyses that suggest the broad
uniform prior on divergence time parameters hinder the marginal likelihoods of
models with more divergence time parameters, whereas \citet{Hickerson2013}
argue that the bias is inherent to the inefficient rejection algorithm
implemented in \msb.

\citet{Hickerson2013} present an interesting probabilistic argument in an
attempt to show that insufficient prior sampling is to blame for the bias.
They argue the widest of the three priors used by \citet{Oaks2012} was very
unlikely to produce any prior samples with high numbers of divergence times
that were consistent with the Philippine data.
There are a few issues with their argument.
First, the probabilities they present assume the gene divergence times
estimated by \citet{Oaks2012} are correct.
The sole purpose of the estimates of ultrametric gene trees presented by
\citet{Oaks2012} was to provide a very rough comparison of the gene divergence
times across the 22 taxa.  These analyses assumed an arbitrary strict clock of
$2\e{-8}$ for all taxa, and are, of course, subject to estimation error.
Furthermore, the units of the gene trees are in millions of year, whereas the
divergence time prior of \msb is in generations, thus the logic of
\citet{Hickerson2013} requires the additional assumption that all 22 Philippine
taxa have a generation time of one year.
Thus, the argument of \citet{Hickerson2013} that divergence time estimates of
\citet{Oaks2012} ``should set an upper bound on their prior for \divt{}'' seems
dangerous, especially given our findings presented above.

Finally, even \emph{if} we make all of these assumptions and assume the
gene divergence times are without error, the probabilistic argument only
applies to one of the three different priors used by \citet{Oaks2012}.
The narrowest prior on divergence times used by \citet{Oaks2012} closely mirrors
the range of gene divergence time estimates, and applying Hickerson et al.'s
\citeyear{Hickerson2013} probability equations demonstrates that the prior
is densely populated with samples with large number of divergence parameters
that are consistent with the gene divergence estimates
Thus, according to the argument of \citet{Hickerson2013}, if insufficient prior
sampling is to blame for the bias, it should be much reduced under the narrow
prior on \divt{}.
However, the magnitude of the bias is very similar across all three priors
\citet{Oaks2012}.
\citet{Hickerson2013} point out a case where the narrow prior performs
slightly better (panel L of Figures S32, S37, and S38 of \citet{Oaks2012}).
However, it is important to note that these results suffered from a bug
in \msb, and there are many cases after \citet{Oaks2012} corrected the 
bug where the narrow prior performs slightly worse (see panels D--J of
Figures 3 and S12).

To disentangle whether Lindely's paradox or insufficient prior sampling is
to blame for the biases revealed by \citet{Oaks2012}, we must look at
the different predictions made by these two phenomena.
One examples, as discussed by \citet{Oaks2012} is that insufficient
prior sampling should create higher variance in posterior estimates,
and thus the analyses should be sensitive to the number of samples
drawn from the prior.
\citet{Oaks2012} do not see such sensitivity when they compare prior sample
sizes of $2\e6$, $5\e6$, and $10\e7$.

To explore this prediction further, we repeat the analysis of the Philippines
dataset under the intermediate prior used by \citet{Oaks2012} ($\divt \sim U(0,
10)$, $\meanDescendantTheta{} \sim (0.0005, 0.04)$, $\ancestralTheta{} \sim
(0.0005, 0.02)$), using a very large prior sample size of $10\e8$.
When we look at the trace of the estimates of the dispersion index of
divergence times (\vmratio{}) as the prior samples accumulate
(Figure~\ref{figSamplingError}) we see no trend in either the unadjusted or GLM
regression adjusted estimates.
This strongly suggests that insufficient prior sampling did not play a large
role in the bias found by \citet{Oaks2012}.

Our results presented above that demonstrate the bias of the model-averaging
approach of \citet{Hickerson2013} both toward models with narrower \divt{}
priors (Table~\ref{tabModelChoiceEmpirical} and
Figs.~\labelcref{figExclusionSimTau,figExclusionSimProb,figPowerNumExcluded,figPowerProbExclusion})
and models with fewer \divt{} parameters (Figs.~\ref{figPowerPsiMode} \&
\ref{figPowerOmegaProb}) strongly implicate Lindley's paradox as a primary
cause.
In all of our model-averaging analyses, all prior models have the same number
of samples.
Thus, it is difficult to explain the method's propensity towards the model with
the smallest parameter space with insufficient sampling.
While analyses that sample each model proportional to their relative parameter
space could be explored, it seems very likely that the broad uniform priors
are simply inhibiting the marginal likelihoods of spacious models.

As discussed by \citet{Oaks2012}, a straightforward prediction of Lindley's
paradox is that it should disappear as the model generating the data converges
to the prior.
\citet{Oaks2012} test this prediction by performing 100,000 simulations to
assess the model-choice behavior of \msb when the prior model is correct.
They find that the bias actually switches direction (at least for the
regression-adjusted estimates) and the method tends to underestimate the
probability of the model with one divergence event (see Figure 4 of
\citet{Oaks2012}).
The same prediction is not as straightforward for the insufficient-sampling
hypothesis.
Even when the prior is correct, due to the discrete uniform prior on
\numt{} implemented in \msb, models with larger numbers of divergence
events (and thus greater parameter space) will still be less densely
sampled than those with fewer divergence events \citep{Oaks2012}.





\section*{Differing utilities of \numt{} and \vmratio{} in \msb}
We preface this section with a clarification of an issue that has
been confused throughout the \msb literature.
The mean (\meant{}{}), variance (\vart{}{}), and dispersion index
($\vart{}{}/\meant{}{} = \vmratio{}$) of divergence times are \emph{not} parameters
of the model implemented in \msb.
Rather they are statistics that summarize parameters of the model.
This is in contrast to \numt{}, which is a parameter of the model.

\citet{Hickerson2013} strongly argue that \vmratio{} is a better estimator than
\numt{}, but the logic behind the argument is dubious.
They present a plot of \numt{} against \vmratio{} (Fig.~S1) which is simply a
plot of sample size versus variance.
After showing that \vmratio{} has essentially no information about the
number of divergences, they conclude it is more informative and biogeographically
relevant.
We struggle to follow this logic.
Certainly the maximum information is contained within the divergence time
vector that \vmratio{} is summarizing.
The temporal information contained in this vector coupled with its cardinality
(\numt{}) is certainly more informative than its summary.
The dispersion index is not a sufficient statistic for this vector.

They also argue that ``\msb can estimate \vmratio{} much better than \numt{}.''
However, the primary objective of \msb is to estimate the posterior probability
of divergence models.
\citet{Oaks2012} demonstrate that even when all assumptions of the model are
met, \vmratio is a terrible model-choice estimator (see plots B, D \& F of
Figure 4), whereas \numt{} performs better.
Furthermore, \vmratio{} is limited to estimating the probability of only a
single divergence model (the one divergence model), and thus its utility for
model choice is extremely limited.
The model-choice utility of \vmratio{} is limited to the probability that
this continuous statistic, which can range from zero to infinity, is at its lower
limit of zero. In theory, this point density will always be zero, thus an
arbitrary threshold (0.01 is used throughout the \msb literature) must 
be chosen to make the probability estimable.
However, it is still not surprising to see that it is numerically difficult to
obtain reliable estimate of the probability that the continuous \vmratio{}
statistic is ``near'' its limit of zero.
It is much easier, less subjective, and more interpretable to estimate
the probability of the discrete parameter of the model, \numt{}, is
at its lower limit of one.
Thus, it is not surprising to see this estimator of model probability
perform better.

\section*{General thoughts}
It is not too surprising that the model-averaging approach proposed by
\citet{Hickerson2013} behaves so poorly.
While this approach is trivial to implement, it is not a trivial change
to the the basic \msb model.
To better understand why this is, it might help to step back and get a
sense of the scale of the \msb model.
To do this, we will use the dataset of 22 vertebrate taxon pairs of
\citet{Oaks2012} as an example.

Following the model description and notation of \citet{Oaks2012}, we
will tally up all of the free parameters in the \msb model.

Under the simplest model in \msb (i.e., assuming no migration and
no intra-locus recombination), the number of parameters for each
taxon pair include:
The population sizes of the ancestral and descendant populations
(\ancestralTheta{}, \descendantTheta{1}{}, \descendantTheta{2}{}),
The magnitude of population contraction in each of the descendant
populations (\bottleScalar{1}{} and \bottleScalar{2}{}) and the
timing of these contractions (\bottleTime{}), and the $N-1$ node heights
(coalescent times) of the gene tree that gives rise to the $N$ gene
copies sampled from both populations of the pair.

Because we only have a single locus for each taxon, there are no locus-specific
\myTheta{}-scaling parameters (\locusMutationRateScalar{}) or
\locusRateHetShapeParameter shape parameter for the gamma prior distribution on
\locusMutationRateScalar{}.
Lastly, there are between one and 22 divergence time parameters \divt{} in
the vector \divtvector.
Thus for our Philippines data and under the simplest model in \msb, there are
581--602 free parameters.
Under this rich model, the method is sampling over 1002 divergence models
\citep[i.e., the number of integer partitions of $Y=22$][]{Oaks2012}.

This is a very tall order, and the method only uses four summary statistics
calculated from the sequence alignment of each taxon pair:
$\pi$ \citep{Tajima1983}, $\theta_W$
\citep{Watterson1975}, $\pi_{net}$ \citep{Takahata1985}, and
$SD(\pi-\theta_W)$ \citep{Tajima1989}.
That gives us a total of 88 summarys statistics (four for each of the 22 taxon
pairs), which contain minimal information about many of the $\approx 600$
parameters in the model.
More summary statistics can be used in \msb, but most are highly correlated
with these four statistics (which are even highly correlated among themselves),
and thus contribute little additional information about the data.

This very large number of parameters and divergence models relative to 
summary statistics is undoubtedly the reason the method is so sensitive
to the prior distributions.
It also likely contributes to the methods lack of robustness.
We use robustness here as a measure of a methods insensitivity to model
violations.
Robustness is an extremely important characteristic of a method to gauge its
applicability to empirical data, because we know the model and priors will be
wrong to some degree.

The approach of \citet{Hickerson2013} adds an additional dimension of model
choice to the model. They expand the model to sample over eight prior models.
This extends the original model to having 582-603 free parameters and, more
importantly, sampling over 8016 unique model states across both divergence
models and prior models.
This certainly is a non-trivial extension of the model, and given the method's
very large number of parameters and models relative to the information used
from the data, likely plays a major role in the poor behavior of this approach.
Certainly, such a large modification of the model demands simulation-based
assessments of the behavior of the new model, as we provide here.


In theory, the approach of \citet{Hickerson2013} is very appealing.  It sums
over multiple candidate prior models to produce a posterior estimate
marginalized over the uncertainty in prior choice.
In general, Bayesian model-averaging is a powerful approach that leverages a
great strengths of Bayesian statistical procedures, namely the ability to
obtain marginalized estimates that incorporate uncertainty in nuisance
parameters.
However, given the basic \msb model is already struggling to estimate
a huge number of parameters and model probabilities with scant information
from the data, it is not surprising that adding an additional layer of
model choice to the method causes problems.

The recommendations of \citet{Oaks2012} for mitigating the lack of robustness
and biases of \msb are actually similar to those of \citet{Hickerson2013},
but avoid the need for imposing additional model choice.
\citet{Oaks2012} suggest that uniform priors are inappropriate for parameters,
and recommend the use of more cogent probability distributions from the
exponential family.
If we look at the prior distribution on the divergence time parameter \divt{}
imposed by the model-averaging prior approach of \citet{Hickerson2013} we see
it is a mixture of overlapping uniforms with lower limits of zero
(Figure~\ref{figMCTauPrior}).
This looks very much like an exponential distribution.
Thus would be much more natural and easier to simply place a gamma prior (the
exponential being a special case of the gamma) on divergence times.
This would allow the investigator to much better capture their prior
uncertainty in model parameters while avoiding awkward broad uniform
distributions, and without the need of costly model-averaging.
This seems like a much more natural solution to the problems of Lindley's
paradox and sampling error, which are both certainly exacerbated by the fact
that uniform distributions are inappropriately the only choice of prior for
many of the parameters of the \msb model \citep[\divt{}, \ancestralTheta{},
\descendantTheta{1}{}, \descendantTheta{2}{}, \bottleTime{},
\bottleScalar{1}{}, \bottleScalar{2}{}, \locusMutationRateScalar,
\migrationRate{}, \recombinationRate;][]{Oaks2012}.
There is no cost whatsoever to this strategy.
Given that ABC methods merely have to draw random values from prior
distributions, hence there are no difficult proposal probability ratios to
calculate, etc., there is no excuse for not using the most appropriate
distributions on parameters with conjugacy in mind.
Also, this approach reduces the ``need'' for empirically guided priors.
The investigator can place the majority of the prior density in regions
of parameters space the believe, a priori, are most plausible, but still
capture uncertainty in the tails of distributions with low density.
As discussed in \citet{Oaks2012} the use of inflexible and unforgiving uniform
priors necessitated the need to use broad priors to avoid excluding the truth a
priori.
Thus, to avoid the   behavior of the method under these priors, cheating
and using the data was a very tempting alternative.



\section*{Other clarifications}

\subsection*{The validity of \msb estimates}
\citet{Hickerson2013} claim that \citet{Oaks2012} ``assume that all previous
\msb results are invalid.''
With the exception of \citet{Hickerson2013}, we do not claim any previous
results of \msb are invalid.
Rather, we conservatively recommend that the common inference of temporally
clustered divergences
\citep{Barber2010, Bell2012, Carnaval2009, Chan2011, Daza2010, Hickerson2006,
    Huang2011, Lawson2010, Leache2007, Plouviez2009, Stone2012, Voje2009},
when not accompanied with the necessary simulation-based analyses to guide the
interpretation of such results, should be treated with caution, because the
method has been shown to spuriously infer clustered divergences over a range of
prior conditions.

\subsection*{Saturation of summary statistics}
\citet{Hickerson2013} claim that our priors ``cause much of the explored
parameter space to be beyond the threshold of saturation in most mtDNA genes.''
To explore this possibility, we simulated datasets under prior settings that
match two of the three priors used by \citet{Oaks2012}: $\meanDescendantTheta
\sim U(0.0005, 0.04)$ and $\ancestralTheta \sim U(0.0005, 0.02)$.
We draw divergence time parameters from a uniform distribution of $U(0, 20)$,
simulated datasets, and plotted the \divt{} values against the summary
statistics calculated from the resulting datasets
(Figure~\ref{figSaturationPlot}).
Clearly, the priors used by \citet{Oaks2012} with upper limits on \divt{} of 5
and 10 did not suffered from little to no effect from saturation.  Even at
divergence times of 20 coalescent units, there is still signal in the summary
statistics used by \msb (Figure~\ref{figSaturationPlot}).



\section*{Acknowledgments}
We thank the National Science Foundation for supporting this work (DEB
1011423).
J.\ Oaks was also supported by the University of Kansas (KU) Office of Graduate
Studies, Society of Systematic Biologists, Sigma Xi Scientific Research
Society, KU Department of Ecology and Evolutionary Biology, and the KU
Biodiversity Institute.

\bibliography{../bib/references}
% \bibliography{references}

%% LIST OF FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\singlespacing

\renewcommand\listfigurename{Figure Captions}
\cftsetindents{fig}{0cm}{2.2cm}
\renewcommand\cftdotsep{\cftnodots}
\setlength\cftbeforefigskip{10pt}
\cftpagenumbersoff{fig}
\listoffigures


\end{linenumbers}

%% TABLES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\singlespacing

%:TABLE-re-analysis
% \begin{table}[htbp]
%     \sffamily
%     \footnotesize
%     \addtolength{\tabcolsep}{-0.08cm}
%     \rowcolors{2}{}{myGray}
%     %\captionsetup{font=footnotesize}
%     \caption{Results of the model-averaging approach of \citet{Hickerson2013}
%         applied to the Philippines dataset of \citet{Oaks2012} using eight
%         prior models (see \citet{Hickerson2013} Table 1).  The results of (1)
%         \citet{Hickerson2013}, (2) our full re-analysis, and of (3) repeating
%         the final rejection step of \citet{Hickerson2013} using the prior means and
%         standard deviations to normalize the summary statistics, are all compared.}
%     \centering
%     \begin{tabular}{ l l l l }
%         \toprule
%          & Hickerson et al. (2013) & Re-analysis & blah \\
%         $\hat{\numt{}}$ & 2 & 1 & 1 \\
%         $p(\numt{} = 1 | \ssSpace)$ & 0.062 & 0.206 & 0.146 \\
%         $BF_{\numt{} = 1, \numt{} \neq 1}$ 1.39 & 5.45 & 3.59 \\
%         $p(\vmratio{} < 0.01 | \ssSpace)$ & 0.298 & 0.416 & 0.361 \\
%         $BF_{\vmratio{} < 0.01, \vmratio{} \geq 0.01}$ 6.65 & 11.16 & 8.85 \\
%         $\hat{M}$ & 1 & 2 & 2 \\
%         $p(M_1 | \ssSpace)$ & 0.513 & 0.358 & 0.376 \\
%         $p(M_2 | \ssSpace)$ & 0.286 & 0.369 & 0.389 \\
%         $p(M_3 | \ssSpace)$ & 0.036 & 0.086 & 0.089 \\
%         $p(M_4 | \ssSpace)$ & 0.002 & 0.012 & 0.011 \\
%         $p(M_5 | \ssSpace)$ & 0.142 & 0.129 & 0.107 \\
%         $p(M_6 | \ssSpace)$ & 0.020 & 0.043 & 0.024 \\
%         $p(M_7 | \ssSpace)$ & 0.001 & 0.003 & 0.004 \\
%         $p(M_8 | \ssSpace)$ & 0.000 & 0.000 & 0.001 \\
%         \bottomrule
%     \end{tabular}
%     \label{tabModelChoiceEmpirical}
% \end{table}

%:TABLE-model-choice-empirical
\begin{table}[htbp]
    \sffamily
    % \footnotesize
    \addtolength{\tabcolsep}{-0.08cm}
    \rowcolors{2}{}{myGray}
    %\captionsetup{font=footnotesize}
    \caption{Results of the model-averaging approach of \citet{Hickerson2013}
        applied to the Philippines dataset of \citet{Oaks2012} using three sets
        of prior models. All models used priors on population size of
        $\meanDescendantTheta{} \sim U(0.0001,0.1)$ and $\ancestralTheta{} \sim
        U(0.0001, 0.05)$, and differ only in their prior on divergence time
        (\divt{}) parameters.  Each set of five models differ only in the
        divergence time prior used for the model with the narrowest prior:
        $M_1$ ($\divt{} \sim U(0, 0.1)$), $M_{1A}$ ($\divt{} \sim U(0, 0.01)$),
        or $M_{1B}$ ($\divt{} \sim U(0, 0.001)$). The approximate posterior
        probability of each model ($p(M_i \given \ssSpace)$) is given for each
        of the three analyses.  The posterior estimates are based on 10,000
        samples retained from $1\e6$ prior samples
    from each model.}
    \centering
    \begin{tabular}{ l l l l l }
        \toprule
        & & \multicolumn{3}{c}{$p(M_i \given \ssSpace)$} \\
        \cmidrule(){3-5}
        Model & \divt{} prior & $M_{*}=M_1$ & $M_{*}=M_{1A}$ & $M_{*}=M_{1B}$ \\
        \midrule
        $M_*$ & --        & 0.899 & 0.821 & 0.673 \\
        $M_2$ & $U(0,1)$  & 0.079 & 0.136 & 0.251 \\
        $M_3$ & $U(0,5)$  & 0.013 & 0.026 & 0.044 \\
        $M_4$ & $U(0,10)$ & 0.006 & 0.012 & 0.022 \\
        $M_5$ & $U(0,20)$ & 0.003 & 0.005 & 0.010 \\
        \bottomrule
    \end{tabular}
    \label{tabModelChoiceEmpirical}
\end{table}

\clearpage

%% FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

%:FIGURE-saturation plot
\mFigure{../../saturation/saturation-plot.pdf}{
    The summary statistics $\pi$ \citep{Tajima1983} and $\pi_{net}$
    \citep{Takahata1985} as a function of divergence time between populations.
    Each plot represents 1100 pairs of parameter draws and summary statistics
    calculated from the simulated data.
    Prior settings for the simulations were $\divt{} \sim U(0, 20)$,
    $\meanDescendantTheta{} \sim U(0.0005, 0.04)$, and $\ancestralTheta{} \sim
    U(0.0005, 0.02)$.
}{figSaturationPlot}

% \mFigure{../../response-redux/results/hickerson/pymsbayes-results/pymsbayes-output/d1/m12345678-combined/mean_by_dispersion.pdf}{
%     Results of reanalysis.
% }{figJointPosterior}

\mFigure{../../response-redux/hickerson-et-al-posterior/hickerson-posterior-1k/mean_by_dispersion.pdf}{
    The joint posterior of the mean (\meant{}{}) and dispersion index ($\vmratio{} = 
    \vart{}{}/\meant{}{}$) of divergence times for 22 vertebrate taxon pairs as
    estimated by \citet{Hickerson2013} (see Figure 2B of \citet{Hickerson2013}).
    The posterior samples are color-coded to indicate the erroneous mixture of
    timescales in the analysis of \citet{Hickerson2013};
    grey = $0.05/\mutationRate$ generations and
    black = $0.02/\mutationRate$ generations.
}{figJointPosteriorHickerson}

\mFigure{../../response-redux/results/sampling-error/pymsbayes-results/omega_over_sampling.pdf}{
    Traces of the estimated lower and upper limits of the 95\% highest posterior density (HPD)
    interval of \vmratio{} (the dispersion index of divergence times) as 100 million prior
    samples are accumulated. Each pair of points is based on 1000 posterior samples retained
    from the prior. Both (A) unadjusted and (b) GLM-regression-adjusted estimates are shown.
    Prior settings were $\divt{} \sim U(0,10)$, $\meanDescendantTheta{} \sim U(0.0005, 0.04)$,
    and $\ancestralTheta{} \sim U(0.0005, 0.02)$.
}{figSamplingError}

%% Exclusion simulation results
\mFigure{../../model-choice/results/m1-1-sim/pymsbayes-results/num_tau_excluded.pdf}{
    The propensity of the model-averaging approach of \citet{Hickerson2013} to
    exclude the truth.
    The plots illustrate the number of true \divt{} parameters excluded from analyses of
    simulated datasets where \divt{} for 22 pairs of populations is drawn from an
    exponential distribution, $\divt{} \sim Exp(2)$.
    The plots represent (A) unadjusted and (B) GLM-adjusted estimates from 1000
    simulation replicates analyzed using $5\e6$ samples from the prior.
    The proportion of simulation replicates in which at least one true
    parameter value is excluded from the model preferred by a Bayes factor
    ($p(\divt{} \notin \hat{M})$) is also given.
}{figExclusionSimTau}

\mFigure{../../model-choice/results/m1-1-sim/pymsbayes-results/prob_of_exclusion.pdf}{
    The propensity of the model-averaging approach of \citet{Hickerson2013} to
    exclude the truth.
    The plots illustrate the estimated probability of excluding at least one
    true \divt{} value from analyses of simulated datasets where \divt{} for 22
    pairs of populations is drawn from an exponential distribution, $\divt{}
    \sim Exp(2)$.
    The plots represent (A) unadjusted and (B) GLM-adjusted estimates from 1000
    simulation replicates analyzed using $5\e6$ samples from the prior.
    The proportion of simulation replicates in which there is strong support
    for at least one true parameter value being excluded from the model
    ($p(BF_{\divt{} \notin M, \divt{} \in M} > 10)$) is also given.
}{figExclusionSimProb}

%% Power results
\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_accuracy_omega_median.pdf}{
    The accuracy of the model-averaging approach of \citet{Hickerson2013} to
    estimate the dispersion index of divergenct times (\vmratio{}) from
    analyses of simulated datasets where \divt{} for 22 pairs of populations is
    drawn from a series of uniform distributions, $\divt{} \sim U(0,
    \divt{max})$.
    The proportion of estimates less than the true value of,
    $p(\hat{\vmratio{}} < \vmratio{})$, is given for each \divt{max}.
    Each plot represents unadjusted median estimates from 500 simulation
    replicates analyzed using $5\e6$ samples from the prior.
}{figPowerAccOmegaMedian}

\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_accuracy_omega_mode_glm.pdf}{
    The accuracy of the model-averaging approach of \citet{Hickerson2013} to
    estimate the dispersion index of divergenct times (\vmratio{}) from
    analyses of simulated datasets where \divt{} for 22 pairs of populations is
    drawn from a series of uniform distributions, $\divt{} \sim U(0,
    \divt{max})$.
    The proportion of estimates less than the true value of,
    $p(\hat{\vmratio{}} < \vmratio{})$, is given for each \divt{max}.
    Each plot represents GLM-regression-adjusted mode estimates from 500
    simulation replicates analyzed using $5\e6$ samples from the prior.
}{figPowerAccOmegaModeGLM}

\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_psi_mode.pdf}{
    The bias of the model-averaging approach of \citet{Hickerson2013} to infer
    clustered divergences in our simulation-based power analyses.
    The plots illustrate the estimated number of divergence events ($\hat{\numt{}}$)
    from analyses of simulated datasets where \divt{} for 22 pairs of populations is drawn from a series of
    uniform distributions, $\divt{} \sim U(0, \divt{max})$.
    The estimated probability of the method inferring on divergecne event, $p(\numt{} = 1)$,
    is given for each \divt{max}.
    Each plot represents 500 simulation replicates analyzed using $5\e6$
    samples from the prior.
}{figPowerPsiMode}

\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_omega_prob.pdf}{
    The bias of the model-averaging approach of \citet{Hickerson2013} to
    support one divergence event in our simulation-based power analyses.
    The plots illustrate histograms of the estimated posterior probability that
    the dispersion index of divergence times is less than 0.01 ($p(\vmratio{} <
    0.01 | \ssSpace)$) from analyses of simulated datasets where \divt{} for 22
    pairs of populations is drawn from a series of uniform distributions,
    $\divt{} \sim U(0, \divt{max})$.
    The proportion of simulation replicates that strongly support one
    divergence event, $p(BF_{\vmratio{} < 0.01, \vmratio{} \geq 0.01} > 10)$,
    is given for each \divt{max}.
    Each plot represents 500 simulation replicates analyzed using $5\e6$
    samples from the prior.
}{figPowerOmegaProb}

\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_num_excluded.pdf}{
    The propensity of the model-averaging approach of \citet{Hickerson2013} to exclude
    the truth in our simulation-based power analyses.
    The plots illustrate the number of true \divt{} parameters excluded from analyses of
    simulated datasets where \divt{} for 22 pairs of populations is drawn from a series of
    uniform distributions, $\divt{} \sim U(0, \divt{max})$. The proportion
    of simulation replicates in which at least one true parameter value is excluded from
    the model preferred by a Bayes factor ($p(\divt{} \notin \hat{M})$) is
    given for each \divt{max}.
    Each plot represents 500 simulation replicates analyzed using $5\e6$
    samples from the prior.
}{figPowerNumExcluded}

\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_prob_exclusion.pdf}{
    The propensity of the model-averaging approach of \citet{Hickerson2013} to
    exclude the truth in our simulatoin-based power analyses.  The plots
    illustrate the estimated posterior probability of excluding at least one
    true \divt{} value from analyses of simulated datasets where \divt{} for 22
    pairs of populations is drawn from a series of uniform distributions,
    $\divt{} \sim U(0, \divt{max})$. The proportion of simulation replicates in
    which there is strong support for at least one true parameter value being
    excluded from the model ($p(BF_{\divt{} \notin M, \divt{} \in M} > 10)$) is
    given for each \divt{max}.
    Each plot represents 500 simulation replicates analyzed using $5\e6$
    samples from the prior.
}{figPowerProbExclusion}

%% Validation results
\mFigure{../../model-choice/results/validation/results/pymsbayes-results/plots/mc_behavior.pdf}{
    An assessment of the approximate Bayesian model-averageing approach of
    \citet{Hickerson2013} under the ideal conditions when the prior model
    is correct (i.e., the pseudo-replicate datasets are simulated from
    parameters drawn from the same prior distributions used in the
    analysis).
    The plots show the relationship between the estimated posterior and true
    probability of (A \& C) $\numt{}=1$ and (B \& D) $\vmratio{} < 0.01$, based
    on 50,000 simulations.
    The results summarize the (A \& B) unadjusted and (C \& D) GLM-adjusted
    posterior estimate from each simulation replicate.
    The prior settings for all replicates included five prior models with
    $\meanDescendantTheta{} \sim U(0.0001, 0.1)$ and $\ancestralTheta{} \sim
    U(0.0001, 0.05)$ for all five models, and
    $M_1: \divt{} \sim U(0, 0.1)$,
    $M_2: \divt{} \sim U(0, 1)$,
    $M_3: \divt{} \sim U(0, 5)$,
    $M_4: \divt{} \sim U(0, 10)$, and
    $M_5: \divt{} \sim U(0, 20)$.
    The number of samples from the prior was $2.5\e6$.
    The simulated data structure was 8 population pairs, with a single 1000
    bp locus sampled from 10 individuals from each population.  The 50,000
    estimates of the posterior probability of one divergence event were
    assigned to 20 bins of width 0.05.
    The estimated posterior probability of each bin is plotted against the
    proportion of replicates in that bin with a true value consistent with
    one divergence event (i.e., $\numt{}=1$ or $\vmratio{} < 0.01)$.
}{figValidationMCBehvior}

%% Model-choice tau prior
\mFigure{../../model-choice/tau-prior/tau_prior.pdf}{
    The prior distribution on divergence times imposed by the model-averaging prior
    comprised of five models with different uniform priors on \divt{}:
    $M_1$ ($\divt{} \sim U(0, 0.1)$), $M_2$ ($\divt{} \sim U(0, 1)$), $M_3$
    ($\divt{} \sim U(0, 5)$), $M_4$ ($\divt{} \sim U(0, 10)$), $M_5$ ($\divt{}
    \sim U(0, 20)$).
}{figMCTauPrior}

\mFigure{../images/coin_flip.pdf}{
    A plot of three beta probability density functions that represent a prior
    (black; $beta(0.5, 0.5)$), true posterior (blue; $beta(4.5, 1.5)$), and
    empirical Bayes density (red; $beta(8.5, 2.5)$) for a dataset of five
    Bernoulli trials, four of which are successes.
}{figCoinFlip}

\mFigure{../../model-choice/priors-for-pc-plot/pc-plots.pdf}{
    The graphical checks recommended by \citet{Hickerson2013} for three prior
    models: (A) $M_1$ ($\divt{} \sim U(0, 0.1)$), (B) $M_{1A}$ ($\divt{} \sim
    U(0, 0.01)$), and (C) $M_{1B}$ ($\divt{} \sim U(0, 0.001)$).
    The plots project the summary statistics from 1000 random samples from each
    model onto the first two orthogonal axes of a principle component analysis,
    with the blue dot representing the observed summary statistics from the 22
    population pairs of Philippine vertebrates.
}{figPCA}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SUPPORTING INFO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{page}{1}

\singlespacing

\renewcommand{\refname}{\noindent\MakeUppercase{\LARGE\sffamily\upshape supporting information}}

% PUT MAIN TEXT CITATION HERE
% \begin{thebibliography}{1}
% \providecommand{\natexlab}[1]{#1}
% \providecommand{\url}[1]{\texttt{#1}}
% \providecommand{\urlprefix}{URL }

% \bibitem

% \end{thebibliography}


%% SUPPL TABLES %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setcounter{table}{0}


\end{document}

