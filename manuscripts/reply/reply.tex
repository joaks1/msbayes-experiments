%&<latex>
\documentclass[letterpaper,12pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% preamble %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{../utils/preamble.tex}
\input{../utils/macros.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\doublespacing
\raggedright
\setlength{\parindent}{0.5in}
\begin{linenumbers}

\begin{titlepage}
    \begin{flushleft}
        \sffamily

        \MakeUppercase{\large\bfseries Why you should not fix a biased
        model-choice method by adding an additional dimension of model choice:
        A reply to Hickerson et al.}

        \vspace{12pt}
        \textbf{Running head:} \MakeUppercase{Approximate Bayesian model
        choice}

        \vspace{12pt}
        Jamie R. Oaks$^{1,2,5}$, Jeet Sukumaran$^{1,2}$, Jacob A.
        Esselstyn$^{3}$, Charles W. Linkem$^{1,2}$, Cameron D.
        Siler$^{4}$, Mark T. Holder$^{2}$ and Rafe M. Brown$^{1,2}$

        \bigskip
        $^1$\emph{Biodiversity Institute\\
            $^2$Department of Ecology and Evolutionary Biology\\ 
            University of Kansas\\
            %1345 Jayhawk Blvd\\
            Lawrence, KS 66045\\
            USA}\\[.1in]
        $^3$\emph{Biology Department\\
            McMaster University\\
            % Life Sciences Building Rm 328\\
            Hamilton, Ontario L8S4K1\\
            Canada}\\[.1in]
        $^4$\emph{Department of Biology\\
            University of South Dakota\\
            Vermillion, SD 57069\\
            USA}\\[.1in]
        $^5$\emph{Corresponding author} (\href{mailto:joaks1@ku.edu}{\tt
        joaks1@ku.edu})\\

    \end{flushleft}
\end{titlepage}

{\sffamily
    \noindent\textbf{ABSTRACT} \\
    \noindent Abstract here \ldots

    \vspace{12pt}
    \noindent\textbf{KEY WORDS: } 
}

\newpage
\noindent Recently, this journal has served as a venue for discussion of the
potential pitfalls of approximate Bayesian methods of comparative
phylogeographical model choice.
\citet{Oaks2012} published their findings that the method \msb can often be
biased towards inferring models of temporally clustered divergence times
among taxon pairs.
\citet{Hickerson2013} has published a response to this paper where they
present a model-averaging approach that they conclude circumvents the poor
behavior of the method revealved by \citet{Oaks2012}.
Both of these papers are largely in agreement.
In fact, \citet{Hickerson2013} reiterate a lot of the discussion of
\citet{Oaks2012} regarding the impact of broad uniform priors on Bayesian model
choice.
The main differences in their perspectives are centered around
(1) the means by which broad uniform priors cause the poor behavior of \msb,
and
(2) how to potentially ameliorate this issue.

\citet{Oaks2012} conclude the primary mechanism by which broad priors cause the
poor behavior of the method is likely the low marginal likelihoods of
parameter-rich models integrated over vast parameter space with low probability
of producing the data, yet relatively high prior density \citep[this is often
referred to as Lindley's paradox;][]{Lindley1957}.
Note, this suggests the bias is extrinsic to \msb, and the numerical
approximation machinery of the method could be sound.
\citet{Hickerson2013} take a more pessimistic view of the bias, and suggest it
is intrinsic to \msb, i.e., the method's rejection algorithm is
inefficient and will be increasingly biased as the overall space of the model
increases, either as a function of the number of taxon pairs or the width of
the uniform priors on nuisance parameters.
They support their position by giving a probabilistic argument that focuses on
only one of the three prior models used by \citet{Oaks2012}.
We show that this argument is based on dubious assumptions, and does not
explain the bias of the method found in several analyses of \citet{Oaks2012}.
We reiterate several of the findings of \citet{Oaks2012} as well as present
results of additional analyses, which strongly suggest that Lindley's paradox
is playing a larger role in the poor behavior of the method.

\citet{Oaks2012} suggest that more cogent prior probability distributions on
divergence models and nuisance parameters could mitigate the effect of
Lindley's paradox.
They also layout a set of simulation-based procedures for determining power,
accuracy, and robustness of the method given a dataset, and recommend any
application of \msb should be accompanied by such procedures, especially
if any biological conclusions are going to drawn from the results.
\citet{Hickerson2013} present an approximate Bayesian model-averaging approach
for accommodating uncertainty in selecting among empirically guided priors, and
champion the method as a means of avoiding the pitfalls raised by
\citet{Oaks2012}.
Unfortunately, we show that fundamental errors in this approach render some of
their results invalid and leave the remaining results difficult to interpret.
Furthermore, we follow the advice of \citet{Oaks2012} and present
simulation-based assessments of Hickerson et al.'s \citeyear{Hickerson2013}
approach.
Our results demonstrate that the method is biased and dangerous.
The approach merely provides another means by which the model can ``escape''
large parameter space, and the bias towards models with smaller space still
remains.
This bias can be dangerous and often excludes the true parameter space due to
the use of uniform priors.
Furthermore we discuss the potential theoretical and practical problems of
empirical Bayesian model choice.

\section*{EB}
\citet{Hickerson2013} repeatedly refer the prior distributions used by
\citet{Oaks2012} as ``poorly selected''.
However, this is bit misleading, as \citet{Oaks2012} discuss in detail how they
selected their priors (see their section ``Specifying and simulating the joint
prior'').
They detail how the being limited to uniform prior distributions necessitates
awkward broad priors to avoid excluding the truth, a priori.
What \citet{Hickerson2013} really mean by ``poorly selected'' is that
\citet{Oaks2012} did not use their data to inform their initial prior
distributions (i.e., they did not use an empirical Bayesian approach).
\citet{Oaks2012} did also use empirically informed priors when assessing
both the prior sensitivity of the method and determining the power of \msb
under the ``best'' possible real-world conditions.
\citet{Oaks2012} discuss the potential dangers of taking an empirical Bayesian
model choice (see last paragraph of ``Assessing prior sensitivity of \msb''),
and we expand on this here.

\subsection*{Theoretical implications of empirical priors for Bayesian model
choice}
\begin{linenomath}
Bayesian inference is a rational method of inductive learning in which Bayes'
rule is used to update our beliefs about a model as new information becomes
available.
If we let \allParameterValues represent the set of all possible parameter
values, we can define a prior distribution for all $\theta \in
\allParameterValues$ such that $p(\theta)$ describes our belief that any given
\myTheta{} is the true value of the parameter.
If we let \allDatasets represent all possible datasets then we can 
define a sampling model for all $\theta \in
\allParameterValues$ and $\alignment{}{} \in \allDatasets$ such that
$p(\alignment{}{} | \theta)$ measures our belief that any dataset \alignment{}{}
will be generated by any model state \myTheta{}.
After collecting a new dataset \alignment{i}{}, we can use Bayes' rule to
calculate the posterior distribution
\begin{equation}
    p(\myTheta{} \given \alignment{i}{}) = \frac{p(\alignment{i}{} \given 
    \myTheta{})p(\myTheta{})}{p(\alignment{i}{})},
    \label{eq:bayesrule}
\end{equation}
where
\begin{equation}
    p(\alignment{i}{}) = \int_{\myTheta{}} p(\alignment{i}{} \given
    \myTheta{})p(\myTheta{}) d\myTheta{}.
\end{equation}
The posterior distribution is a measure of our beliefs after seeing the new
information.
\end{linenomath}

This is an elegant method of updating our beliefs as data are accumulated.
However, this all hinges on the fact that the prior ($p(\myTheta{})$) is
defined for all possible parameters independently of the new data being
analyzed.
Any other datasets or external information can safely be used to inform our
beliefs about $p(\myTheta{})$.
However, if the data are used to both inform the prior and calculate the
posterior, the prior becomes conditional on the data, and Bayes' rule breaks
down.

Now, this is not to say that empirical Bayesian approaches not useful.
Quite the contrary.
Empirical Bayes is a well studied branch of Bayesian statistics that
has given rise to a many powerful inference methods.
It is true that empirical Bayes methods have an uncertain theoretical basis and
do not yield a valid posterior distribution from Bayes' rule \citep[e.g.,
empirical Bayesian estimates of the posterior are often too narrow, off-center,
and misshapen;][]{Morris1983,Laird1987,Carlin1990,Efron2013}.
Nevertheless, empirical Bayes methods are often a powerful means of parameter
estimation that display favorable frequentist properties.
Furthermore, many post-hoc correction methods have been developed for
estimating confidence-intervals from empirical Bayes estimates of posterior
distributions that often display good frequentist coverage behavior
\citep{Morris1983,Laird1987,Laird1989, Carlin1990,Hwang2009}.

Whereas empirical Bayes approaches can provide powerful methods for parameter
estimation, a theoretical justification for empirical Bayes approaches to model
choice is much more dubious.
In Bayesian model choice, the goal is usually not to estimate parameters, but
to estimate the relative probabilities of candidate models.
Unlike parameter estimates, posterior probabilities represent a summary of the
entire posterior distribution.
Thus, given that empirical Bayesian posterior distributions are not accurate,
there is no guarantee regarding the accuracy of probabilities summarized from
them.

This can be demonstrated with a simple, albeit contrived, example.
Let us say that principal investigator Mary and her new postdoc Will are
interested in the hypothesis that the mass of George Washington's periwig
renders the United States' quarter dollar unfair.
That is to say that our null hypothesis is that the probability of a US quarter
landing heads when tossed is less than 0.5 ($\theta < 0.5$).
They can certainly evaluate this hypothesis, they can have undergraduate
worker, Joe, flip the coin for them while the tabulate the results.
But being Bayesians, before they call Joe into the lab, they agree on a
prior probability to place on the set of all possible probabilities that
the quarter will land heads when it is flipped.
Given their prior knowledge that Joe moonlights as a magician, and is notorious
for performing coin and card tricks in the lab, they suspect there is a good
chance that Joe will use his slight-of-hand skills to replace their quarter
with one of his two-headed or two-tailed coins he has up his sleeve at all
times.
So, knowing that the beta distribution is the conjugate prior for a binomial
likelihood, they decide to use a $beta(a=0.5, b=0.5)$ prior distribution
(Figure~\ref{figCoinFlip}).

Mary calls Joe into the lab, and hands him a regular-issue US quarter minted in
1992.
After five tosses, four of which was heads, Joe decides that academics are
crazy and leaves the lab to pursue a major in theatre.  Mary and Will, both
being computational biologists, are satisfied with their
empirical dataset of $y = 4$ heads out of $N = 5$ trials.
They know from the conjugacy of the beta prior, that the posterior distribution
has the nice analytical form $\theta|y,N \sim beta(a + y, b + N - y)$, which
in this case is simply $beta(4.5, 1.5)$; this is the true posterior distribution
of $\theta$ given their prior belief and data (Figure~\ref{figCoinFlip}).
This allows them to plug these values into the beta cumulative distribution
function to determine that the posterior probability of their hypothesis is
$p(\theta < 0.5 | y=1, N=5) = 0.088$.
Given their prior belief and dataset, this indeed is the correct posterior
probability of the hypothesis, and Mary and Will should now update their
posterior belief accordingly.

However, upon reflecting on the results of their experiment
(Figure~\ref{figCoinFlip}), Mary and Will regret not trusting Joe to refrain
from turning their experiment into a magic trick.
Their prior looks very ``poorly selected,'' and if they had only
had more faith in Joe's integrity, they would have selected a much better
prior. Clearly form their results, they should have used a prior centered
around 0.2; their data suggest a prior of $beta(4.5, 1.5)$ would have been
much more appropriate.
Rather than resort to flipping the coin five more times, Mary and Will decide
to redo their analysis using the much better ``prior'' of $p(\theta) \sim
beta(4.5, 1.5)$.
This gives a ``posterior'' of $\theta|y,N \sim beta(8.5, 2.5)$, and a
probability of their hypothesis of $p(\theta < 0.5 | y=1, N=5) = 0.026$.
Now convinced that the mass of President Washington's head renders the US
Quarter unfair, Will begins composing an e-mail of complaint for the U.S.\
Mint.

If Joe's flips were a representative sample, Mary and Will's empirical Bayes
estimate might very well be a better estimator for the parameter \myTheta{}.
However, their empirical Bayes estimate of the probability of their hypothesis
is incorrect and biased.
This simple example shows how parameter estimation is fundamentally different
from estimating the probability of a model.
While empirical informed priors can be used to obtain well-behaved parameter
estimators, using them for model choice is much less certain.


\subsection*{Practical dangers of narrow uniform priors for Bayesian model
choice}
Given that \msb can be biased toward models with smaller parameter space, and
Hickerson et al.'s \citeyear{Hickerson2013} use of narrow, empirically informed
uniform priors on divergence times, we sought to determine whether such an
approach will often exclude the truth (i.e., the model-averaged posterior will
be dominated by models that excude the true values of parameters of the model).
We explored this possiblity in two ways.
First, we reanalyzed the Philippines dataset using the model-averaging approach
of \citet{Hickerson2013}, but set one of prior models with a uniform prior on
divergence times that is unrealistically narrow, and almost certainly excludes
the truth.
If Lindley's paradox causes the method to prefer models with less parameter
space, we expect \msb will preferentially sample from this incorrect model
yielding a marginal posterior that is invalid (i.e., the model-averaged
posterior will be dominated by an incorrect model that excludes the truth).
Second, we generated simulated datasets for which the divergence times are
drawn form an exponential distribution and applied the approach of
\citet{Hickerson2013} to each of them to see how often the method excludes the
truth.

For our re-analysis of the Philippines dataset we used the model-averaging
approach of \citet{Hickerson2013}, but with different prior models to avoid
mixing time units. We used five priors, all of which had priors on population
sizes of $\meanDescendantTheta{} \sim U(0.0001, 0.1)$ and
$\ancestralTheta{} \sim U(0.0001, 0.05)$.
Following \citet{Hickerson2013}, each of these priors had the following
priors on divergence time parameters:
$M_1$, $\divt{} \sim U(0, 0.1)$;
$M_2$, $\divt{} \sim U(0, 1)$;
$M_3$, $\divt{} \sim U(0, 5)$;
$M_4$, $\divt{} \sim U(0, 10)$; and
$M_5$, $\divt{} \sim U(0, 20)$.
For our prior, we simulated $1\e6$ random samples from each of the models
for a total sample of $5\e6$.
For each model, we retained the 10,000 samples with the smallest Euclidean
distance from the observed summary statistics, standardizing the statistics
using the prior means and standard deviations of the given model.
We then retained the 10,000 samples with the smallest Euclidean distance from
the observed summary statistics, this time standardizing the statistics using
the prior means and standard deviations across all five models.
We then repeated this analysis twice, replacing the $M_1$ with
$M_1A$ and $M_1B$, which differ only by having priors on divergence
times of $\divt{} \sim U(0, 0.01)$ and $\divt{} \sim U(0, 0.001)$,
respectively.
While we suspect the prior of $\divt{} \sim U(0, 0.1)$ used by
\citet{Hickerson2013} likely excludes the true divergence times of
at least some of the 22 taxa, we are nearly certain that these
narrower priors are incorrect and exclude the truth.

Our results show that the model-averaging approach of \citet{Hickerson2013}
does not reduce the methods bias towards models with smaller parameter space,
but rather allow it to manifest in a very dangerous way.
The method strongly prefers the prior model with the narrowest distribution on
divergence times across all three of our analyses, even when this model is
almost certainly incorrect and excludes the true divergence times of the
Philippine taxa.
Unfortunately, ``checking'' the priors by plotting the summary statistics from
1000 random samples of each prior model along the first two orthogonal axes of
a principle component analysis, as recommended by \citet{Hickerson2013},
provides no warning of a problem (Figure~\ref{figPCA}.
Given that the results of \citet{Hickerson2013} strongly prefer the models with
the narrowest prior on divergence times, it seems quite likely that their
model-averaged results are dominated by models that exclude at least some of
the 22 true divergence times, making their results difficult to interpret.

To better quantify the propensity of Hickerson et al.'s
\citeyear{Hickerson2013} approach to exclude the truth, we simulated
1000 pseudo-observed datasets in which the divergence times for the
22 population pairs are drawn randomly from an exponential distribution
with a mean of 0.5 ($\divt{} \sim Exp(2)$). All other parameters were
identically distributed as in the $M_1$--$M_5$ models.
We then repeated the analysis described above, using $5\e6$ prior samples from
$M_1$, $M_2$, $M_3$, $M_4$, and $M_5$, and retaining 1,000 posterior samples
for each of the 1,000 pseudo-observed datasets.

\highLight{Add how we determined exclusion.}
Our results show that the model-averaging approach of \citet{Hickerson2013}
excludes the true values of parameters in 97\% of the replicates (90\% with
GLM-regression adjustment), excluding up to 21 of the 22 true divergence times
(Figure~\ref{figExclusionSimTau}.
Furthermore, the posterior probability of excluding at least one true parameter
value is very high in nearly all of the replicates
(Figure~\ref{figExclusionSimProb}).
We used a Bayes factor (comparing models that exclude the truth to those that
do not) calculated for each replicate of greater than 10 as strong support for
excluding the truth.
With that criterion, 66\% of the replicats (87\% with GLM-regression adjustment)
strongly support the exclusion of true values (Figure~\ref{figExclusionSimProb}).

The results of our empirical and simulation-based analyses clearly demonstrate
the danger of using narrow, empirically guided priors in a Bayesian
model-averaging framework \ldots

\subsection*{Final thought on EB}
Given all of this, it is quite clear why one should use caution before
overly criticizing ones choice of priors after having seen the posterior.
This really is a strawman argument.
They were not there before the data were seen.


\section*{Their power analysis}
\citet{Hickerson2013} present a power analysis in which they demonstrate that
\msb has the power to infer multiple divergence events when divergence times
are random over hundreds of thousands of generations (rather than millions
as demonstrated by \citet{Oaks2012}.
It is not surprising to find that within certain parameter space the method
has increased power to detect temporal variation in divergences over narrower
time windows than millions of generations.
However, what is important to consider is whether those conditions are relevant
to real world applications of the method.
\citet{Oaks2012} explore the behavior of the method under three divergence time
priors as narrow as 0--5 coalescent units, which is quite narrow considering
that this expresses the prior belief that all 22 taxon pairs diverged within
this window.
\citet{Hickerson2013} limit their power analysis to a single prior of 0--1
coalescent unit.
It seems that being 100\% certain a priori that all taxa under comparison
diverged within the last coalescent unit is not generally applicable to most
empirical systems.
Furthermore, even when such extensive prior information is available to only be
able to detect multiple divergences on the scale of hundreds of thousands of
generations does not seem very ``powerful.''

Also, \citet{Hickerson2013} present information only on the extreme case of a
single divergence event.
As discussed in \citet{Oaks2012}, it seems dubious to consider the estimation
of two divergence events as a success when divergence is random over hundreds
of thousands of generations.
Unlike the results of \citet{Oaks2012} it is impossible to assess the
performance of \msb based on the results presented by \cite{Hickerson2013}
other than to know whether or not it inferred the worst possible scenario of a
single event.
In an empirical system such as the Philippines, where island fragmentation
has occurred at least X times over the past several millions of years, an
estimate where 22 taxa share even a handful of divergence events would be
of biogeographic interest.
\citet{Hickerson2013} seem to suggest in their response that \msb should only
be used as a binary estimator; i.e., was there one event or more?
This is not consistent with how \msb has been presented, used, and interpreted
in the past.

\section*{Our power analysis}


\section*{Differing utilities of \numt{} and \vmratio{} in \msb}
We preface this section with a clarification of an issue that has
been confused throughout the \msb literature.
The mean (\meant{}{}), variance (\vart{}{}), and dispersion index
($\vart{}{}/\meant{}{} = \vmratio{}$) of divergence times are \emph{not} parameters
of the model implemented in \msb.
Rather they are statistics that summarize parameters of the model.
This is in contrast to \numt{}, which is a parameter of the model.

\citet{Hickerson2013} strongly argue that \vmratio{} is a better estimator than
\numt{}, but the logic behind the argument is dubious.
They present a plot of \numt{} against \vmratio{} (Fig.~S1) which is simply a
plot of sample size versus variance.
After showing that \vmratio{} has essentially no information about the
number of divergences, they conclude it is more informative and biogeographically
relevant.
We struggle to follow this logic.
Certainly the maximum information is contained within the divergence time
vector that \vmratio{} is summarizing.
The temporal information contained in this vector coupled with its cardinality
(\numt{}) is certainly more informative than its summary.
The dispersion index is not a sufficient statistic for this vector.

They also argue that ``\msb can estimate \vmratio{} much better than \numt{}.''
However, the primary objective of \msb is to estimate the posterior probability
of divergence models.
\citet{Oaks2012} demonstrate that even when all assumptions of the model are
met, \vmratio is a terrible model-choice estimator (see plots B, D \& F of
Figure 4), whereas \numt{} performs better.
Furthermore, \vmratio{} is limited to estimating the probability of only a
single divergence model (the one divergence model), and thus its utility for
model choice is extremely limited.
The model-choice utility of \vmratio{} is limited to the probability that
this continuous statistic, which can range from zero to infinity, is at its lower
limit of zero. In theory, this point density will always be zero, thus an
arbitrary threshold (0.01 is used throughout the \msb literature) must 
be chosen to make the probability estimable.
However, it is still not surprising to see that it is numerically difficult to
obtain reliable estimate of the probability that the continuous \vmratio{}
statistic is ``near'' its limit of zero.
It is much easier, less subjective, and more interpretable to estimate
the probability of the discrete parameter of the model, \numt{}, is
at its lower limit of one.
Thus, it is not surprising to see this estimator of model probability
perform better.

\section*{General thoughts}
It is not too surprising that the model-averaging approach proposed by
\citet{Hickerson2013} behaves so poorly.
While this approach is trivial to implement, it is not a trivial change
to the the basic \msb model.
To better understand why this is, it might help to step back and get a
sense of the scale of the \msb model.
To do this, we will use the dataset of 22 vertebrate taxon pairs of
\citet{Oaks2012} as an example.

Following the model description and notation of \citet{Oaks2012}, we
will tally up all of the free parameters in the \msb model.

Under the simplest model in \msb (i.e., assuming no migration and
no intra-locus recombination), the number of parameters for each
taxon pair include:
The population sizes of the ancestral and descendant populations
(\ancestralTheta{}, \descendantTheta{1}{}, \descendantTheta{2}{}),
The magnitude of population contraction in each of the descendant
populations (\bottleScalar{1}{} and \bottleScalar{2}{}) and the
timing of these contractions (\bottleTime{}), and the $N-1$ node heights
(coalescent times) of the gene tree that gives rise to the $N$ gene
copies sampled from both populations of the pair.

Because we only have a single locus for each taxon, there are no locus-specific
\myTheta{}-scaling parameters (\locusMutationRateScalar{}) or
\locusRateHetShapeParameter shape parameter for the gamma prior distribution on
\locusMutationRateScalar{}.
Lastly, there are between one and 22 divergence time parameters \divt{} in
the vector \divtvector.
Thus for our Philippines data and under the simplest model in \msb, there are
581--602 free parameters.
Under this rich model, the method is sampling over 1002 divergence models
\citep[i.e., the number of integer partitions of $Y=22$][]{Oaks2012}.

This is a very tall order, and the method only uses four summary statistics
calculated from the sequence alignment of each taxon pair:
$\pi$ \citep{Tajima1983}, $\theta_W$
\citep{Watterson1975}, $\pi_{net}$ \citep{Takahata1985}, and
$SD(\pi-\theta_W)$ \citep{Tajima1989}.
That gives us a total of 88 summarys statistics (four for each of the 22 taxon
pairs), which contain minimal information about many of the $\approx 600$
parameters in the model.
More summary statistics can be used in \msb, but most are highly correlated
with these four statistics (which are even highly correlated among themselves),
and thus contribute little additional information about the data.

This very large number of parameters and divergence models relative to 
summary statistics is undoubtedly the reason the method is so sensitive
to the prior distributions.
It also likely contributes to the methods lack of robustness.
We use robustness here as a measure of a methods insensitivity to model
violations.
Robustness is an extremely important characteristic of a method to gauge its
applicability to empirical data, because we know the model and priors will be
wrong to some degree.

The approach of \citet{Hickerson2013} adds an additional dimension of model
choice to the model. They expand the model to sample over eight prior models.
This extends the original model to having 582-603 free parameters and, more
importantly, sampling over 8016 unique model states across both divergence
models and prior models.
This certainly is a non-trivial extension of the model, and given the methods
very large number of parameters and models relative to the information
used from the data, likely plays a major role in the poor behavior of this
approach.

In theory, the approach of \citet{Hickerson2013} is very appealing.  It sums
over multiple candidate prior models to produce a posterior estimate
marginalized over the uncertainty in prior choice.
In general, Bayesian model-averaging is a powerful approach that leverages a
great strengths of Bayesian statistical procedures, namely the ability to
obtain marginalized estimates that incorporate uncertainty in nuisance
parameters.
However, given the basic \msb model is already struggling to estimate
a huge number of parameters and model probabilities with scant information
from the data, it is not surprising that adding an additional layer of
model choice to the method causes problems.

The recommendations of \citet{Oaks2012} for mitigating the lack of robustness
and biases of \msb are actually similar to those of \citet{Hickerson2013},
but avoid the need for imposing additional model choice.
\citet{Oaks2012} suggest that uniform priors are inappropriate for parameters,
and recommend the use of more cogent probability distributions from the
exponential family.
If we look at the prior distribution on the divergence time parameter \divt{}
imposed by the model-averaging prior approach of \citet{Hickerson2013} we see
it is a mixture of overlapping uniforms with lower limits of zero
(Figure~\ref{figMCTauPrior}).
This looks very much like an exponential distribution.
Thus would be much more natural and easier to simply place a gamma prior (the
exponential being a special case of the gamma) on divergence times.
This would allow the investigator to much better capture their prior
uncertainty in model parameters while avoiding awkward broad uniform
distributions, and without the need of costly model-averaging.
This seems like a much more natural solution to the problems of Lindley's
paradox and sampling error, which are both certainly exacerbated by the fact
that uniform distributions are inappropriately the only choice of prior for
many of the parameters of the \msb model \citep[\divt{}, \ancestralTheta{},
\descendantTheta{1}{}, \descendantTheta{2}{}, \bottleTime{},
\bottleScalar{1}{}, \bottleScalar{2}{}, \locusMutationRateScalar,
\migrationRate{}, \recombinationRate;][]{Oaks2012}.
There is no cost whatsoever to this strategy.
Given that ABC methods merely have to draw random values from prior
distributions, hence there are no difficult proposal probability ratios to
calculate, etc., there is no excuse for not using the most appropriate
distributions on parameters with conjugacy in mind.
Also, this approach reduces the ``need'' for empirically guided priors.
The investigator can place the majority of the prior density in regions
of parameters space the believe, a priori, are most plausible, but still
capture uncertainty in the tails of distributions with low density.
As discussed in \citet{Oaks2012} the use of inflexible and unforgiving uniform
priors necessitated the need to use broad priors to avoid excluding the truth a
priori.
Thus, to avoid the   behavior of the method under these priors, cheating
and using the data was a very tempting alternative.


\section*{Acknowledgments}

% \bibliography{../bib/references}
\bibliography{references}

%% LIST OF FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\singlespacing

\renewcommand\listfigurename{Figure Captions}
\cftsetindents{fig}{0cm}{2.2cm}
\renewcommand\cftdotsep{\cftnodots}
\setlength\cftbeforefigskip{10pt}
\cftpagenumbersoff{fig}
\listoffigures


\end{linenumbers}

%% TABLES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\singlespacing

%:TABLE-model-choice-empirical
\begin{table}[htbp]
    \sffamily
    \footnotesize
    \addtolength{\tabcolsep}{-0.08cm}
    \rowcolors{2}{}{myGray}
    %\captionsetup{font=footnotesize}
    \caption{Results of the model-averaging approach of \citet{Hickerson2013}
        applied to the Philippines dataset of \citet{Oaks2012} using three sets
        of prior models. All models used priors on population size of
        $\meanDescendantTheta{} \sim U(0.0001,0.1)$ and $\ancestralTheta{} \sim
        U(0.0001, 0.05)$, and differ only in their prior on divergence time
        (\divt{}) parameters.  Each set of five models differ only in the
        divergence time prior used for the model with the narrowest prior:
        $M_1$ ($\divt{} \sim U(0, 0.1)$), $M_{1A}$ ($\divt{} \sim U(0, 0.01)$),
        or $M_{1B}$ ($\divt{} \sim U(0, 0.001)$). The approximate posterior
        probability of each model ($p(M_i \given \ssSpace)$) is given for each
        of the three analyses.  The posterior estimates are based on 10,000
        samples retained from $1\e6$ prior samples
    from each model.}
    \centering
    \begin{tabular}{ l l l l l }
        \toprule
        & & \multicolumn{3}{c}{$p(M_i \given \ssSpace)$} \\
        \cmidrule(){3-5}
        Model & \divt{} prior & $M_{*}=M_1$ & $M_{*}=M_{1A}$ & $M_{*}=M_{1B}$ \\
        \midrule
        $M_*$ & --        & 0.899 & 0.821 & 0.673 \\
        $M_2$ & $U(0,1)$  & 0.079 & 0.136 & 0.251 \\
        $M_3$ & $U(0,5)$  & 0.013 & 0.026 & 0.044 \\
        $M_4$ & $U(0,10)$ & 0.006 & 0.012 & 0.022 \\
        $M_5$ & $U(0,20)$ & 0.003 & 0.005 & 0.010 \\
        \bottomrule
    \end{tabular}
    \label{tabModelChoiceEmpirical}
\end{table}


\clearpage

%% FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

%:FIGURE-saturation plot
\mFigure{../../saturation/saturation-plot.pdf}{
    Saturation plot \ldots
}{figSaturationPlot}

\mFigure{../../response-redux/results/hickerson/pymsbayes-results/mean_by_dispersion.pdf}{
    Results of reanalysis.
}{figJointPosterior}

\mFigure{../../response-redux/results/sampling-error/pymsbayes-results/omega_over_sampling.pdf}{
    Sampling error \ldots
}{figSamplingError}

%% Exclusion simulation results
\mFigure{../../model-choice/results/m1-1-sim/pymsbayes-results/num_tau_excluded.pdf}{
    Histogram of the number of \divt{} parameters excluded.
}{figExclusionSimTau}

\mFigure{../../model-choice/results/m1-1-sim/pymsbayes-results/prob_of_exclusion.pdf}{
    Histogram of the posterior probability of excluding the truth.
}{figExclusionSimProb}

%% Power results
\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_accuracy_omega_median.pdf}{
    Histograms of the posterior probability that $\vmratio{} < 0.01$.
}{figPowerAccOmegaMedian}

\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_accuracy_omega_mode_glm.pdf}{
    Histograms of the posterior probability that $\vmratio{} < 0.01$.
}{figPowerAccOmegaModeGLM}

\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_psi_mode.pdf}{
    Histograms of \numt{} estimates.
}{figPowerPsiMode}

\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_omega_prob.pdf}{
    Histograms of the posterior probability that $\vmratio{} < 0.01$.
}{figPowerOmegaProb}

\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_prob_exclusion.pdf}{
    Histograms of the posterior probability of excluding the truth.
}{figPowerProbExclusion}

\mFigure{../../model-choice/results/power-1/pymsbayes-results/plots/power_num_excluded.pdf}{
    Histograms of the number of true \divt{} parameters excluded.
}{figPowerNumExcluded}

%% Validation results
\mFigure{../../model-choice/results/validation/results/pymsbayes-results/plots/mc_behavior.pdf}{
    True versus estimated probability of a single divergence event.
}{figValidationMCBehvior}

%% Model-choice tau prior
\mFigure{../../model-choice/tau-prior/tau_prior.pdf}{
    The prior distribution on divergence times imposed by the model-averaging prior.
}{figMCTauPrior}

\mFigure{../images/coin_flip.pdf}{
    A plot of three beta density functions meant to represent a prior
    (black), true posterior (blue), and empirical Bayes density (red).
}{figCoinFlip}

\mFigure{../../model-choice/priors-for-pc-plot/pc-plots.pdf}{
    The graphical checks recommended by \citet{Hickerson2013} for three prior
    models: (A) $M_1$ ($\divt{} \sim U(0, 0.1)$), (B) $M_{1A}$ ($\divt{} \sim
    U(0, 0.01)$), and (C) $M_{1B}$ ($\divt{} \sim U(0, 0.001)$).
    The plots project the summary statistics from 1000 random samples from each
    model onto the first two orthogonal axes of a principle component analysis,
    with the blue dot representing the observed summary statistics from the 22
    population pairs of Philippine vertebrates.
}{figPCA}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SUPPORTING INFO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{page}{1}

\singlespacing

\renewcommand{\refname}{\noindent\MakeUppercase{\LARGE\sffamily\upshape supporting information}}

% PUT MAIN TEXT CITATION HERE
% \begin{thebibliography}{1}
% \providecommand{\natexlab}[1]{#1}
% \providecommand{\url}[1]{\texttt{#1}}
% \providecommand{\urlprefix}{URL }

% \bibitem

% \end{thebibliography}


%% SUPPL TABLES %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setcounter{table}{0}


\end{document}

