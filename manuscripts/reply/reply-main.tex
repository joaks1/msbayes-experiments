\section{Introduction}
Biogeographers frequently seek to explain population and species
differentiation on geographical phenomena.
Establishing that a set of population splitting events occurred
at the same time can be a potentially persuasive argument that a set of taxa
were affected by the same geographic events.
The approximate-Bayesian method, \msb, allows biogeographers to estimate the
probabilities of models in which multiple sets of taxa diverge at the same
time \citep{Hickerson2006,Huang2011}.

Recently, \citet{Oaks2012} used this model-choice framework to study 22 pairs
of vertebrate lineages distributed across the Philippines; they also studied
the behavior of the \msb approach using computer simulations.
They found the method is very sensitive to prior assumptions and often
supports shared divergences across taxa that diverged randomly over broad time
periods, to which \citet{Hickerson2013} responded.
\citet{Oaks2012} and \citet{Hickerson2013} agree on the fundamental
methodological point about the model selection performed in \msb:
\begin{itemize}
   \item The use of vague priors on the divergence-time parameters can lead to
       spurious support for models with few divergence events shared across
       taxa. Thus, the primary inference enabled by the approach is very
       sensitive to the priors on divergence times.
\end{itemize}
However, the two papers suggest alternative mechanisms by which the priors on
divergence times cause this behavior:
\begin{description}
    \item[Hypothesis 1: Numerical approximation error \citep{Hickerson2013}]
        Under broad uniform priors, the rejection algorithm implemented in \msb
        is unable to adequately sample the space of the models within
        reasonable computational time, which leads to bias toward models with
        fewer divergence-time parameters because they are better sampled.
    % \item \citet{Hickerson2013} suggest that under broad uniform priors, the
    %     rejection algorithm implemented in \msb is unable to adequately sample
    %     the space of the models within reasonable computational time, which
    %     leads to bias toward models with fewer divergence-time parameters
    %     because they are better sampled.
    \item[Hypothesis 2: Strongly weighted marginal likelihoods
        \citep{Oaks2012}]
        The uniform priors on divergence times simply lead to very small
        marginal likelihoods of models with many divergence-time parameters.
        These models are ``averaged'' over much greater parameter space with
        small likelihood, but large prior weight.
        \citep{Jeffreys1939,Lindley1957}.
    % \item \citet{Oaks2012} suggest the cause is more fundamental; the uniform
    %     priors on divergence times simply lead to very small marginal
    %     likelihoods of models with many divergence-time parameters. These
    %     models are ``averaged'' over much greater parameter space with large
    %     prior weight and small likelihood \citep{Jeffreys1939,Lindley1957}.
\end{description}
In Hypothesis 1, the problem is numerical approximation error due to
insufficient computation;
% In the former case, the problem is numerical approximation error due to
% insufficient computation;
the exact posterior supports models with many divergence-time parameters
given data generated under temporally random divergences, but we are unable to
accurately approximate the posterior.
In Hypothesis 2, the problem is more fundamental, because the exact posterior
supports simultaneous divergences given data from randomly diverged taxa.
% In the latter case, numerical error is not the issue, because the exact
% posterior supports simultaneous divergences given data from randomly diverged
% taxa.
I.e., given the assumption that divergence times are broadly and uniformly
distributed, the exact posterior from Bayes' rule leads us to the wrong
conclusions about evolutionary history.
% I.e., this is the correct (albeit biogeographically misleading) answer given
% the prior assumption that divergence times are broadly and uniformly
% distributed.
Such posterior support for simultaneous divergence, even if ``correct'' from
the perspective of Bayesian model choice, does not provide the biogeographical
insights that a researcher who employs \msb seeks to gain.

While these phenomena are not mutually exclusive, it is important to
distinguish between them in order to determine how to improve our ability to
estimate shared divergence histories.
If Hypothesis 1 is correct, then the model is sound and we need to increase our
computational effort or improve our Monte Carlo integration procedures.
For example, Markov chain or sequential Monte Carlo algorithms might sample the
posterior more efficiently than the simple Monte Carlo rejection sampler
implemented in \msb.
Rather than alter the sampling algorithm, \citet{Hickerson2013} tried using
narrow, empirically informed uniform priors in the hope that with less
parameter space to sample, the rejection algorithm will produce better
estimates of the posterior.
Here, we discuss theoretical considerations for using empirically informed
priors for Bayesian model choice and evaluate the approach of
\citet{Hickerson2013} as a potential solution to the biases of \msb revealed by
\citet{Oaks2012}.
In their analyses, \citet{Hickerson2013} made an error by mixing different
units of time, which invalidates the results presented in their response (see
Supporting Information for details).
We correct this error, but still find their approach will often support
(1) clustered divergence models when divergences are random, and
(2) models that exclude from consideration the true values of the
parameters.
% The behavior of their approach also suggests that insufficient computation
% (Hypothesis 1) is not the primary cause of the issue.

If Hypothesis 2 is correct, we need to correct the model, because no amount of
computation will help; even if we could calculate the exact posterior, we would
still reach the wrong interpretation about evolutionary history.
Accordingly, \citet{Oaks2014dpp} has introduced a method that uses more
flexible probability distributions to accommodate prior uncertainty in
divergence times without overly inhibiting the marginal likelihoods of models
with more divergence-time parameters.
This greatly increases the method's robustness and power to detect temporal
variation in divergences \citep{Oaks2014dpp}.
This is not surprising given the rich statistical literature showing that
marginal likelihoods are very sensitive to the priors used in Bayesian model
selection
\citep[e.g.,][]{Jeffreys1939,Lindley1957}.

We also explore the distinct predictions made by Hypotheses 1 and 2.
We show the behavior of \msb matches the predictions of Hypothesis 2,
but not Hypothesis 1.
This strongly suggests the method tends to support models of shared divergences
not due to insufficient computation, but rather due to the larger marginal
likelihoods of these models under the prior assumption of uniformly distributed
divergence times.
% We also show that numerical approximation error is not a major contributer to
% the problem.



\section{The potential implications of empirical Bayesian model choice}
\citet{Oaks2012} found \msb spuriously supports models of temporally
clustered divergences even when using a prior on divergence times that was
informed by the data being analyzed.
The main argument of \citet{Hickerson2013} is that the priors used by
\citet{Oaks2012} were not informative enough.
They suggest a very narrow, highly informed uniform prior on divergence times
is necessary to avoid the method's preference for models with few
divergence-time parameters.
Such an empirical Bayesian approach to model selection raises some theoretical
and practical concerns, some of which were discussed by \citet{Oaks2012} (see
the last paragraph of ``Assessing prior sensitivity of \msb'' in
\citet{Oaks2012}); we expand on this here.

\subsection{Theoretical implications of empirical priors for Bayesian model
choice}
\begin{linenomath}
Bayesian inference is a method of inductive learning in which Bayes' rule is
used to update our beliefs about a model $M$ as new information becomes
available.
If we let \allParameterValues represent the set of all possible parameter
values for model $M$, we can define a prior distribution for all $\theta \in
\allParameterValues$ such that $p(\theta \given M)$ describes our belief that
any given \myTheta{} is the true value of the parameter.
If we let \allDatasets represent all possible datasets then we can 
define a sampling model for all $\theta \in
\allParameterValues$ and $\alignment{}{} \in \allDatasets$ such that
$p(\alignment{}{} | \theta, M)$ measures our belief that any dataset \alignment{}{}
will be generated by any state \myTheta{} of model $M$.
After collecting a new dataset \alignment{i}{}, we can use Bayes' rule to
calculate the posterior distribution
\begin{equation}
    p(\myTheta{} \given \alignment{i}{}, M) = \frac{p(\alignment{i}{} \given
    \myTheta{}, M)p(\myTheta{} \given M)}{p(\alignment{i}{} \given M)},
    \label{eq:bayesrule}
\end{equation}
as a measure of our beliefs after seeing the new information, where
\begin{equation}
    p(\alignment{i}{} \given M) = \int_{\myTheta{}} p(\alignment{i}{} \given
    \myTheta{}, M)p(\myTheta{} \given M) d\myTheta{}
    \label{eq:marginallikelhiood}
\end{equation}
is the marginal likelihood of the model.
\end{linenomath}

This is an elegant method of updating our beliefs as data are accumulated.
However, this all hinges on the fact that the prior ($p(\myTheta{} \given M)$)
is defined for all possible parameter values independently of the new data
being analyzed.
Any other datasets or external information can safely be used to inform our
beliefs about $p(\myTheta{} \given M)$.
However, if the same data are used to both inform the prior and calculate the
posterior, the prior becomes conditional on the data, and Bayes' rule breaks
down.

Thus, empirical Bayes methods have an uncertain theoretical basis and
do not yield a valid posterior distribution from Bayes' rule \citep[e.g.,
empirical Bayesian estimates of the posterior are often too narrow, off-center,
and incorrectly shaped;][]{Morris1983,Laird1987,Carlin1990,Efron2013}.
This is not to say that empirical Bayesian approaches are not useful.
Empirical Bayes is a well-studied branch of Bayesian statistics that has given
rise to many methods for obtaining parameter estimates that often
exhibit favorable frequentist properties.
\citep{Morris1983,Laird1987,Laird1989, Carlin1990,Hwang2009}.

\begin{linenomath}
Whereas empirical Bayesian approaches can provide powerful methods for
parameter estimation, a theoretical justification for empirical Bayesian
approaches to model choice is questionable.
In Bayesian model choice, the primary goal is not to estimate parameters, but
to estimate the probabilities of candidate models.
In a simple example where two candidate models, $M_1$ and $M_2$, are being
compared, the goal is to estimate the posterior probabilities of these two
models.
Again, we can use Bayes' rule to calculate this as
\begin{equation}
    p(M_1 \given \alignment{i}{}) = \frac{p(\alignment{i}{} \given
    M_1)p(M_1)}{p(\alignment{i}{} \given M_1)p(M_1) + p(\alignment{i}{} \given
    M_2)p(M_2)}.
    \label{eq:bayesmodelchoice}
\end{equation}
By comparing Equations \ref{eq:bayesrule} and \ref{eq:bayesmodelchoice}, we
see fundamental differences between Bayesian parameter estimation and
model choice.
\end{linenomath}

In Equation \ref{eq:bayesrule}, we see that the posterior density of any state
$\myTheta{}$ of the model, is the prior density updated by the probability of
the data given $\myTheta{}$ (the likelihood of $\myTheta{}$).
The marginal likelihood of the model only appears as a normalizing constant in
the denominator.
Thus, as long as the prior distribution contains the values of $\myTheta{}$
under which the data are probable and the data are strongly informative
relative to the prior, the values of the parameters that maximize the posterior
distribution will be relatively robust to prior choice, even if the posterior
is technically incorrect due to using the data to inform the priors.
However, if we look at Equation~\ref{eq:bayesmodelchoice}, we see that in
Bayesian model choice it is now the \emph{marginal} likelihood of a model that
updates the prior to yield the model's posterior probability.
The integral over the entire parameter space of the likelihood weighted by the
prior density is no longer a normalizing constant, rather it is how the data
inform the posterior probability of the model.
Because the prior probability distributions placed on the model's parameters
have a strong affect on the integrated, or ``average'', likelihood of a model,
Bayesian model choice tends to be much more sensitive to priors than parameter
estimation \citep{Jeffreys1939,Lindley1957}.
Another important difference of Bayesian model choice illustrated by
Equation~\ref{eq:bayesmodelchoice} is that the value of interest, the posterior
probability of a model, is not a function of \myTheta{} because the parameters
are integrated out of the marginal likelihoods of the candidate models.
Thus, unlike parameter estimates, the estimated posterior probability of a
model is a single value (rather than a distribution) lacking a measure of
posterior uncertainty.

The justification for an empirical Bayesian approach to parameter estimation is
that giving the data more weight relative to the prior (i.e., using the data
twice) will often shift the peak of the estimated distribution nearer to the
true value(s) of the model's parameter(s).
However, there is no such justification for model selection, because unlike
model parameters, the posterior probabilities of candidate models often have no
clear true values.
Model posterior probabilities are inherently measures of our belief in the
models after our prior beliefs are updated by the data being analyzed.
This complicates the meaning of model posterior probabilities when Bayes' rule
is violated by informing priors with the same data to be analyzed.
By using the data twice, we fail to account for prior uncertainty and mislead
our posterior beliefs in the models being compared; we will be over confident
in some models and under confident in others.

Nonetheless, empirical Bayesian model choice does perform well for some
problems.
Particularly, in cases where large aggregate data sets are used for many
parallel model-choice problems simultaneously, pooling information to inform
priors can lead to favorable group-wise frequentist coverage across tests
\citep{Efron2008}.
However, this is far removed from the single model-choice problem of \msb.
In the Supporting Information we use a simple example to help highlight the
distinctions between Bayesian parameter estimation and model choice.

\subsection{Practical concerns about empirically informed uniform priors for
    Bayesian model choice}
In addition to the theoretical concerns discussed above, there are practical
problems with using narrow, empirically informed, uniform priors.
The results of Hickerson et al.'s (\citeyear{Hickerson2013}) reanalysis of the
Philippine dataset strongly favored models with the narrowest, empirically
informed prior on divergence times, and thus their model-averaged posterior
estimates are dominated by models $M_1$ and $M_2$ (see Table 1 of
\citet{Hickerson2013}).
This is concerning, because the narrowest \divt{} prior used by
\citet{Hickerson2013} ($\divt{} \sim U(0,0.1)$) likely excludes the true
divergence times for at least some of the Philippine taxa.
\citet{Hickerson2013} set this prior to match the 95\% highest posterior
density (HPD) interval for the mean divergence time estimated under one of the
priors used by \citet{Oaks2012} (see Tables 2 and 3 of \citet{Oaks2012}).
Given this interval estimate is for the \emph{mean} divergence time across all
22 taxa, it may be inappropriate to set this as the limit on the prior, because
some of the taxon pairs are expected to have diverged at times older than the
upper limit.
Furthermore, this prior is \emph{excluded} from the 95\% HPD interval estimates
of the mean divergence time under the other two priors explored by
\citet{Oaks2012} (under these priors the 95\% HPD is approximately 0.3--0.6;
see Table~6 of \citet{Oaks2012}).

The strong preference for the narrowest prior on divergence times suggests the
approach of \citet{Hickerson2013} is biased toward models with less parameter
space and, as a consequence, will estimate model-averaged posteriors dominated
by models that exclude true values of the parameters.
We explored this possibility in two ways.
First, we re-analyzed the Philippines dataset using the model-averaging
approach of \citet{Hickerson2013}, but set one of the prior models with a
uniform prior on divergence times that is unrealistically narrow and almost
certainly excludes most, if not all, of the true divergence times of the 22
taxon pairs.
If small likelihoods of large models cause the method to prefer models with
less parameter space, we expect \msb will preferentially sample from this
erroneous model yielding a posterior that is misleading (i.e., the
model-averaged posterior will be dominated by a model that excludes
the truth).
Second, we generated simulated datasets for which the divergence times are
drawn from an exponential distribution and applied the approach of
\citet{Hickerson2013} to each of them to see how often the method excludes the
truth.

\subsubsection{Re-analyses of the Philippines dataset using empirical Bayesian
model averaging}

For our re-analyses of the Philippines dataset we followed the model-averaging
approach of \citet{Hickerson2013}, but with a reduced set of prior models to
avoid their error of mixing units of time (see SI for details).
We used five prior models, all of which had priors on population sizes of
$\meanDescendantTheta{} \sim U(0.0001, 0.1)$ and $\ancestralTheta{} \sim
U(0.0001, 0.05)$.
Following \citet{Hickerson2013}, each of these models had the following
priors on divergence times:
$M_1$, $\divt{} \sim U(0, 0.1)$;
$M_2$, $\divt{} \sim U(0, 1)$;
$M_3$, $\divt{} \sim U(0, 5)$;
$M_4$, $\divt{} \sim U(0, 10)$; and
$M_5$, $\divt{} \sim U(0, 20)$.
We simulated $1\e6$ random samples from each of the models for a total of
$5\e6$ prior samples.
For each model, we retained the 10,000 samples with the smallest Euclidean
distance from the observed summary statistics after standardizing the
statistics using the prior means and standard deviations of the given model.
From the remaining 50,000 samples, we then retained the 10,000 samples with the
smallest Euclidean distance from the observed summary statistics, this time
standardizing the statistics using the prior means and standard deviations
across all five models.
We then repeated this analysis twice, replacing the $M_1$ model with
$M_{1A}$ and $M_{1B}$, which differ only by having priors on divergence
times of $\divt{} \sim U(0, 0.01)$ and $\divt{} \sim U(0, 0.001)$,
respectively.
While we suspect the prior of $\divt{} \sim U(0, 0.1)$ used by
\citet{Hickerson2013} likely excludes the true divergence times of at least
some of the 22 taxa, we are nearly certain that these narrower priors exclude
most, if not all, of the divergence times of the Philippine taxa.

Our results show that the model-averaging approach of \citet{Hickerson2013}
strongly prefers the prior model with the narrowest distribution on divergence
times across all three of our analyses, even when this model excludes the true
divergence times of the Philippine taxa
(Table~\ref{tabModelChoiceEmpirical}).
However, \citet{Hickerson2013} vetted the priors used in their model-averaging
approach via ``graphical checks,'' in which the summary statistics from 1000
random samples of each prior model are plotted along the first two orthogonal
axes of a principle component analysis (see Figure 1 of \citet{Hickerson2013}).
To determine if such prior-predictive analyses would indicate the $M_{1A}$ and
$M_{1B}$ models are problematic, we performed these graphical checks on our
prior models.
Unfortunately, these prior-predictive checks provide no warning that these
priors are too narrow (Figure~S\ref{figPCA}).
Rather, the graphs suggest these invalid priors are ``better fit''
(Figure~S\ref{figPCA}A--C) than the valid priors similar to those used by
\citet{Oaks2012} (Figure~S\ref{figPCA}D--F).


\subsubsection{Simulation-based assessment of Hickerson et al.'s
    (\citeyear{Hickerson2013}) model averaging over empirical priors}

To better quantify the propensity of Hickerson et al.'s
(\citeyear{Hickerson2013}) approach to exclude the truth, we simulated 1000
datasets in which the divergence times for the 22-population pairs are drawn
randomly from an exponential distribution with a mean of 0.5 ($\divt{} \sim
Exp(2)$).
All other parameters were identically distributed as the $M_1$--$M_5$ models
(Table~\ref{tabModelChoiceEmpirical}).
We then repeated the model-averaging  analysis described above, retaining 1000
posterior samples for each of the 1000 simulated datasets.
For each simulation replicate, we estimated the Bayes factor in favor
of excluding the truth as the ratio of the posterior to prior odds of
excluding the true value of at least one parameter.
Whenever the Bayes factor preferred a model excluding the truth we counted the
number of the 22 true divergence times that were excluded by the preferred
model.

Our results show that the model-averaging approach of \citet{Hickerson2013}
favors a model that excludes the true values of parameters in 97\% of the
replicates (90\% with GLM-regression adjustment), excluding up to 21 of the 22
true divergence times (Figure~\ref{figExclusionSimTau}).
Importantly, the posterior probability of excluding at least one true parameter
value is very high in most replicates
(Figure~\ref{figExclusionSimProb}).
Using a Bayes factor of greater than 10 as a criterion for strong support, 66\%
of the replicates (87\% with GLM-regression adjustment) strongly support the
exclusion of true values (Figure~\ref{figExclusionSimProb}).

The results of the above empirical and simulation analyses clearly demonstrate
the risk of using narrow, empirically guided uniform priors in a Bayesian
model-averaging framework.
The consequence of this approach is obtaining a model-averaged posterior
estimate that is heavily weighted toward models that exclude true
values of the parameters.
This is not a general critique of Bayesian model averaging.
Rather, model averaging can provide an elegant way of incorporating
model uncertainty in Bayesian inference.
However, when averaging over models with narrow and broad uniform priors on a
parameter with a likelihood density that is not expected to be uniformly
distributed, the posterior can be dominated by models that exclude from
consideration the true values of parameters due to the larger marginal
likelihoods of the models that integrate over less space with high prior
weight yet low likelihood.

When using uniformly distributed priors, the alternative to capturing prior
uncertainty is to risk excluding the true values one seeks to estimate.
Fortunately, more flexible continuous distributions that are better suited as
priors for the positive real-valued parameters of the \msb model have been
shown to greatly reduce spurious support for clustered divergence models while
allowing prior uncertainty to be accommodated
\citep{Oaks2014dpp}.


\section{Assessing the power of the model-averaging approach of
    \citet{Hickerson2013}}
While our results above clearly demonstrate the risks inherent to the empirical
Bayesian model-choice approach used by \citet{Hickerson2013}, one could justify
such risk if the approach does indeed increase power to detect temporal
variation in divergences.
We assess this possibility using simulations.
Following \citet{Oaks2012}, we simulated 1000 datasets with \divt{} for each of
the 22 population pairs randomly drawn from a uniform distribution, $U(0,
\divt{max})$, where \divt{max} was set to: 0.2, 0.4, 0.6, 0.8, 1.0, and 2.0, in
\globalcoalunit generations.
All other parameters were identically distributed as the prior models.
As above, we generated $5\e6$ samples from the prior models in
Table~\ref{tabModelChoiceEmpirical}.
For each of the 6000 simulated datasets, we approximated the posterior
by retaining 1000 samples from the prior.

Our results demonstrate that the approach of \citet{Hickerson2013} consistently
infers highly clustered divergences across all the \divt{max} we simulated
(Figure~\ref{figPower4}A--D \& S\ref{figPower6}A--F).
If we follow \citet{Hickerson2013} and use a Bayes factor of greater than 10 as
the criterion for incorrect inference of a single divergence event, we find
that the approach will often strongly support the extreme case of one
divergence event across all our simulation conditions (Figure~\ref{figPower4}E--H \&
S\ref{figPower6}G--L).
The method also struggles to estimate the variance of divergence times
(\vmratio{}), whether evaluating the unadjusted
(Figure~S\ref{figPowerAccuracy}A--F) or GLM-adjusted
(Figure~S\ref{figPowerAccuracy}G--L) posterior estimates.
Overall, the empirical Bayesian model-averaging approach leads to erroneous
support for highly clustered divergences when populations diverged randomly
over the last $8\globalpopsize$ generations.
For loci with per-site rates of mutation on the order of $1\e{-8}$ and
$1\e{-9}$ per generation, this translates to 10 million and 100 million
generations, respectively.

Also, the results of our power analyses further demonstrate the propensity of
Hickerson et al.'s (\citeyear{Hickerson2013}) approach to exclude true
parameter values.
Across all but one of the \divt{max} we simulated, the method favors a model
that excludes the truth in a large proportion of replicates, and across many of
the \divt{max} the preferred model will exclude a large proportion of the true
divergence times (Figure~\ref{figPowerExclusion4}A--D \&
S\ref{figPowerExclusion6}A--F).
Importantly, the posterior probability of excluding at least one true
divergence value is also quite high across many of the \divt{max}
(Figure~\ref{figPowerExclusion4}E--H \& S\ref{figPowerExclusion6}G--L).


\section{The importance of power analyses to guide applications of \texttt{msBayes}}
\citet{Hickerson2013} presented a power analysis of \msb under a narrow uniform
divergence-time prior of 0--1 coalescent units ago.
They found that under these prior conditions \msb can, assuming a
per-site rate of $1.92\e{-8}$ mutations per generation, detect multiple
divergence events among 18 taxa when the true divergences were random over
hundreds of thousands of generations or more.
It is important that investigators perform such simulations to determine the
method's power for their dataset, and decide if \msb has sufficient temporal
resolution to address their hypotheses; in the case of the Philippines dataset,
it did not.
When doing so, it is important to consider what prior conditions are relevant
to the empirical system.
It is rare for there to be enough \emph{a priori} information to be certain
that all taxa diverged within the last 4$\globalpopsize$ generations (i.e.,
0--1 coalescent units).
Also, it seems unlikely that when such prior information is available that
being able to detect more than one divergence event in the face of 18 random
divergences over hundreds of thousands of generations will provide much insight
into the evolutionary history of the taxa.

Inferring more than one divergence time shared across all taxa does not confirm
the method is working well when analyzing data generated under random temporal
variation in divergences (e.g., an inference of two divergence events could be
biogeographically interesting yet spurious).
Thus, it is important that investigators not limit their assessment of the
method's power to only differentiating inferences of one event or more (i.e.
$\numt{} = 1$ versus $\numt{} > 1$).
Rather, looking at the distribution of estimates, as in Figure~\ref{figPower4}
and \citet{Oaks2012}, provides much more information about the behavior of the
method.


\section{The causes of support for models of co-divergence}
To determine how best to improve the behavior of \msb, it is important to
determine the mechanism by which broad uniform priors cause support for
clustered models of divergence.
It is well established that vague priors can be problematic in Bayesian model
selection.
Models that integrate over more parameter space characterized by low
probability of producing the data and relatively high prior density will have
smaller marginal likelihoods \citep{Jeffreys1939,Lindley1957}.
Given the uniformly distributed priors on divergence times employed in \msb,
the likelihood of models with more divergence parameters will be ``averaged''
over much greater parameter space, all with equal prior weight, and much of it
with small likelihood.
In light of this fundamental statistical issue, it is not surprising that the
method tends to support simple models.

However, \citet{Hickerson2013} conclude that the bias is caused by
numerical approximation error due to insufficient computation.
They argue the widest of the three priors used by \citet{Oaks2012} would
infrequently produce random samples with many independent population divergence
times as recent as the estimated gene divergence times presented in
\citet{Oaks2012}.
However, this argument assumes the gene divergence times presented in
\citet{Oaks2012} were estimated without error.
These estimates were intended to provide only a rough comparison of the
gene divergence times across the 22 taxa.
These analyses assumed an arbitrary strict per-site rate of $2\e{-8}$ mutations
per generation for all taxa, and are, of course, subject to estimation error.
Furthermore, the branch-length units of the gene trees are in millions of
years, whereas the divergence-time prior of \msb is in generations, thus
\citet{Hickerson2013} make the implicit assumption that all 22 Philippine taxa
have a generation time of one year.
The argument of \citet{Hickerson2013} that divergence-time estimates of
\citet{Oaks2012} should set an upper bound on the prior for divergence times
seems questionable, especially given our findings presented above regarding the
behavior of \msb when empirically informed uniform priors are employed.

Even if we assume the arbitrary strict clock is correct, gene divergence times
were estimated without error, and all 22 taxa have one-year generation times,
Hickerson et al.'s
(\citeyear{Hickerson2013}) argument only applies to one of the \emph{three}
priors used by \citet{Oaks2012}.
Under these assumptions, the narrowest prior on divergence times used by
\citet{Oaks2012} ($U(0, 5)$) closely mirrors the range of estimates of
gene-divergence times (0--5 million years ago).
Applying Hickerson et al.'s (\citeyear{Hickerson2013}) sampling-probability
argument demonstrates this prior is densely populated with samples with large
numbers of divergence parameters with values younger than the estimated gene
divergence estimates.
Thus, if insufficient prior sampling is to blame for the bias, it should be
much reduced under the narrow prior on \divt{}.
However, the magnitude of the bias is very similar across all three priors
explored by \citet{Oaks2012}.
\citet{Hickerson2013} point out a case where the narrowest prior performs
slightly better (panel L of Figures S32, S37, and S38 of \citet{Oaks2012}).
However, it is important to note that these results suffered from a bug
in \msb, and there are many cases after \citet{Oaks2012} corrected the 
bug where the narrowest prior performs slightly worse (see panels D--J of
Figures 3 and S12).

To disentangle whether model likelihoods or numerical approximation error is to
blame for the method's erroneous support for simple models, we must look at the
different predictions made by these two phenomena.
For example, numerical error due to insufficient prior sampling should create
large variance among posterior estimates and cause analyses to be highly
sensitive to the number of samples drawn from the prior.
Furthermore, if insufficient prior sampling is \emph{biasing} estimates toward
models with less parameter space, as suggested by \citet{Hickerson2013}, we
expect to see support for these models decrease as sampling from the prior
increases.
\citet{Oaks2012} did not see such sensitivity when they compared prior sample
sizes of $2\e6$, $5\e6$, and $10^7$.

To explore this prediction further, we repeat the analysis of the Philippines
dataset under the intermediate prior used by \citet{Oaks2012} ($\divt{} \sim
U(0, 10)$, $\meanDescendantTheta{} \sim (0.0005, 0.04)$, $\ancestralTheta{}
\sim (0.0005, 0.02)$), using a very large prior sample size of $10^8$.
When we look at the trace of the estimates of the dispersion index of
divergence times (\vmratio{}) as the prior samples accumulate
(Figure~S\ref{figSamplingError}) we do not see the trend predicted by
\citet{Hickerson2013}.
While approximation error is always present for any numerical analysis, it does
not appear to be playing a large role in the biases revealed by the results of
\citet{Oaks2012} or presented above.

A straightforward prediction if marginal likelihoods are causing the preference
for simple models is that the bias should disappear as the model generating the
data converges to the prior.
\citet{Oaks2012} tested this prediction by performing 100,000 simulations to
assess the model-choice behavior of \msb when the prior model is correct.
The results confirm the prediction:
\msb estimates the probability of the one-divergence model quite well when the
prior is correct (see Figure 4 of \citet{Oaks2012}).
We confirmed this same behavior for the model-averaging approach used by
\citet{Hickerson2013} (see SI text and Figure~S\ref{figValidationMCBehavior}).
These results are not clearly predicted if insufficient computation was causing
numerical error.
Even when the prior is correct, due to the discrete uniform prior on the number
of divergence events (\numt{}) implemented in \msb, models with larger numbers
of divergence-time parameters (and thus greater parameter space) will still be
far less densely sampled than those with fewer divergence events
\citep{Oaks2012}.
Thus, the results of the simulations of \citet{Oaks2012} are more consistent
with the fundamental sensitivity of marginal likelihoods to priors.

This is further demonstrated by the results presented herein that show the
model-averaging approach of \citet{Hickerson2013} prefers models with narrower
\divt{} priors (Table~\ref{tabModelChoiceEmpirical} and
Figs.~\labelcref{figExclusionSimTau,figExclusionSimProb,figPowerExclusion4})
and fewer \divt{} parameters (Figure~\ref{figPower4}).
In all of these analyses, the same number of random samples were drawn from
each of the prior models.
Thus, while approximation error will always be present in any numerical
approximation method, insufficient prior sampling is an unlikely explanation
for the erroneous support for models with less parameter space.
While analyses that sample each model proportional to their parameter space
could be explored, it is clear that the marginal likelihoods under broad
uniform priors on divergence times will be greater for models with fewer
divergence-time dimensions.




\section{Improving inference of shared divergences}
In theory, the model-averaging approach of \citet{Hickerson2013} is appealing.
It leverages a great strength of Bayesian statistical procedures, namely the
ability to obtain marginalized estimates that incorporate uncertainty in
nuisance parameters.
However, when sampling over models with narrow-empirical and diffuse uniform
priors for a parameter that is expected to have a very non-uniform likelihood
density, models that exclude the true values of the parameters we aim to
estimate will often have the largest marginal likelihood.

The recommendations of \citet{Oaks2012} for mitigating the lack of robustness
of \msb are similar to those of \citet{Hickerson2013}, but avoid the
need for imposing an additional dimension of model choice.
\citet{Oaks2012} suggest that uniform priors may not be ideal for many
parameters of the \msb model, and recommend the use of probability
distributions from the exponential family.
If we look at the prior distribution on divergence times imposed by
the model-averaging approach of \citet{Hickerson2013} we see it is a mixture of
overlapping uniforms with lower limits of zero (Figure~S\ref{figMCTauPrior}).
This looks very much like an exponential distribution, except that in any state
of the model, all the divergence times are restricted to the hard
bounds of one of the uniform distributions.
Thus, it seems more appropriate to simply place a gamma prior (the exponential
being a special case) on divergence times.
This would capture the prior uncertainty that \citet{Hickerson2013} are
suggesting for divergence times (Figure~S\ref{figMCTauPrior}) while avoiding
costly model-averaging and the constraint that all divergence times must fall
within the hard bounds of the current model state.
It also would allow an investigator to place the majority of the prior density
in regions of parameter space they believe, \emph{a priori}, are most
plausible, but still capture uncertainty in the tails of distributions with low
density.
Indeed, \citet{Oaks2014dpp} has shown that the use of gamma distributions in
place of uniform priors improves the power of the method to detect temporal
variation in divergences and reduces erroneous support for clustered
divergences.


\section{Conclusions}
We demonstrate how the approximate Bayesian model-choice method implemented in
\msb can spuriously support models with less parameter space.
This is caused by the use of uniform priors on divergence times.
Uniform distributions necessitate the use of priors that place high density in
unlikely regions of parameter space, less the risk of excluding the true
divergence times \emph{a priori}.
These broad uniform priors reduce the marginal likelihoods of models with more
divergence-time parameters.
We show that the empirical Bayesian model-averaging approach of
\citet{Hickerson2013} does not mitigate this bias, but rather causes it to
manifest by sampling predominantly from models that may exclude the true values
of the parameters.
Our results show that it is difficult to choose an uniformly distributed
prior on divergence times that is broad enough to confidently contain the true
values of parameters while being narrow enough to avoid erroneous support for
models with less parameter space.
More generally, it is important to carefully choose prior assumptions about
parameters in Bayesian model selection; these prior assumptions about
parameters strongly influence the posterior probabilities we seek to estimate.

The common inference of temporally clustered divergences \citep{Barber2010,
    Bell2012, Carnaval2009, Chan2011, Daza2010, Hickerson2006, Huang2011,
    Lawson2010, Leache2007, Plouviez2009, Stone2012, Voje2009}, when not
accompanied with the necessary analyses to assess the robustness and temporal
resolution of such results, should be treated with caution, because \msb has
been shown to erroneously infer clustered divergences over a range of prior
conditions.
Fortunately, \citet{Oaks2014dpp} has shown that alternative probability
distributions allow prior uncertainty to be accommodated while avoiding
excessive prior density in regions of low likelihood, which greatly improves
the ability to infer shared divergence histories.


The work presented herein follows the principles of Open Notebook Science.
All aspects of the work were recorded in real-time via version-control software
and are publicly available at
\href{https://github.com/joaks1/msbayes-experiments}{\url{https://github.com/joaks1/msbayes-experiments}}.
All information necessary to reproduce our results is provided there.

