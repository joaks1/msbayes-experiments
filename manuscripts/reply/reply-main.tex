Recently, the journal \emph{Evolution} has served as a medium for a discussion
of potential promises and pitfalls of approximate-Bayesian methods of
comparative phylogeographical model choice and prior selection.
The discussion has centered around the model-choice method, \msb
\citep{Huang2011}, which uses approximate Bayesian computation (ABC) to
estimate the temporal distribution of divergences among co-distributed pairs of
taxa by sampling over models of divergence that range from a single
divergence-time parameter (i.e., all pairs of taxa diverged at the same time)
to the fully generalized model in which each pair is assigned a divergence-time
parameter.
A full description of the model is given in \citet{Oaks2012}.

\citet{Oaks2012} published results of simulation-based power analyses showing
that \msb is biased toward inferring models of temporally clustered divergence
times across taxon pairs.
\citet{Hickerson2013} published a response to \citet{Oaks2012} where they
present an empirical Bayesian model-averaging approach that they conclude
circumvents the problems revealed by \citet{Oaks2012}.
These papers are largely in agreement, with \citet{Hickerson2013} reiterating
much of the discussion of \citet{Oaks2012} regarding the impact of broad
uniform priors on Bayesian model choice.
Both papers agree that when broad uniform priors are used to reflect \emph{a
prior} uncertainty about divergence times of all pairs of taxa under study,
the method can be biased toward inferring biogeographically interesting
scenarios of temporally clustered divergences among taxa (i.e. models with few
divergence-time parameters).
Both papers also agree that this bias toward models with few divergence-time
parameters is due to the much greater parameter space of models with more
divergence-time parameters.
However, the papers disgree about the primary mechanism by which the greater
parameter space of parameter-rich models causes the bias toward clustered
divergence models;
\citet{Oaks2012} suggest a statistical issue is the cause, whereas
\citep{Hickerson2013} prefer a numerical explanation.
% The main areas of disagreement between the papers are centered around
% (1) the mechanism by which broad uniform priors cause the bias of \msb toward
% models with few divergence-time parameters, and
% (2) how to potentially mitigate this issue.

\citet{Oaks2012} suggest the primary cause of the bias of \msb is the low
marginal likelihoods of parameter-rich models integrated over vast parameter
space with low probability of producing the data, yet relatively high prior
density \citep{Lindley1957} imposed by the uniformly distributed prior on
divergence times.
Note, this suggests the bias is extrinsic to \msb, and the numerical
approximation machinery of \msb could be sound.
\citet{Hickerson2013} suggest the bias is intrinsic to \msb, concluding the
method's rejection algorithm is inefficient and will be biased
toward models with fewer divergence-time parameters, because the parameter
space of these models are more densely sampled relative to models with more
divergence-time parameters.
We agree that sampling error is present in all numerical Bayesian estimation
methods, however, we show here that it is not a major contributer to the
biases found by \citet{Oaks2012}.
We present results of additional analyses that, when combined with
the results of \citet{Oaks2012}, strongly suggest small marginal likelihoods of
models with more parameter space is a primary mechanism by which uniform priors
are causing the bias of \msb.

% We show that this argument is based on empirical Bayesian reasoning and
% questionable assumptions, and does not explain the bias of the method found in
% many of the analyses of \citet{Oaks2012}. 

There are also differences between the papers regarding potential solutions to
the bias.
\citet{Oaks2012} suggest alternative prior probability distributions on
divergence-times and other nuisance parameters could increase the marginal
likelihoods of models with more divergence-time parameters and thus reduce the
bias.
Alternatively, \citet{Hickerson2013} present an approach that uses empirically
informed priors and Bayesian model-averaging in an attempt to accommodate
uncertainty in selecting priors.
% \citet{Hickerson2013} champion this strategy as a means of avoiding the
% pitfalls raised by \citet{Oaks2012}.
In general, we agree with the use of Bayesian model averaging to obtain a
posterior estimate marginalized over uncertainty in prior choice.
However, we question whether adding an additional dimension of model choice
that samples over empirically informed uniform priors is a fruitful solution
for an already biased model-choice method.

In this paper, we discuss the potential theoretical and practical problems of
using empirically informed priors for Bayesian model choice.
Furthermore, we evaluate the empirical Bayesian model averaging approach of
\citet{Hickerson2013} as a potential solution to the biases revealed by
\citet{Oaks2012}.
In their re-analysis of the dataset of \citet{Oaks2012}, \citet{Hickerson2013}
made an error by mixing different time units, which makes the results presented
in their response difficult to interpret (see Supporting Information for
details).
However, here we show that even when that error is avoided, the empirical
Bayesian model-averaging approach of \citet{Hickerson2013} remains biased
toward clustered divergence models, and is predisposed to excluding true
parameter space.
The approach provides another means by which the method can ``escape'' models
with large parameter space, and the method's bias toward smaller models
remains.
Importantly, this bias can manifest by preferring narrow, empirically informed
priors that exclude true parameter space.

% We discuss errors in this approach that render some of their results invalid
% and leave the remaining results difficult to interpret.
% Furthermore, we follow the recommendations of \citet{Oaks2012} and present
% simulation-based assessments of Hickerson et al.'s (\citeyear{Hickerson2013})
% approach.



% \cwlnote{}{This part seems important, but not related to the error in units
% discussed above}
% Another important characteristic of the approach used by \citet{Hickerson2013}
% that makes their results difficult to interpret is their use of narrow,
% empirically guided uniform priors in an approximate-Bayesian model-choice
% framework.
% We discuss the risks associated with this approach in the next section, and
% show how the results of \citet{Hickerson2013} likely exclude true values of the
% model's parameters.

% In addition to this error, we were not able to reproduce the results of
% \citet{Hickerson2013}.
% We followed their methods, generating $5\e6$ prior samples for each of the
% eight models in their Table 1.
% We retained the 10,000 samples from each model with the smallest Euclidean
% distance to the observed summary statistics, and subsequently retained 10,000
% samples from the remaining 80,000.


\section*{The potential implications of empirical Bayesian model choice}
\citet{Hickerson2013} repeatedly refer to the prior distributions used by
\citet{Oaks2012} as ``poorly selected.''
However, this is misleading, as \citet{Oaks2012} discuss in detail how they
selected their priors to reflect the large amount of uncertainty about the
model's parameters for their study system (see section ``Specifying and
simulating the joint prior'' of \citet{Oaks2012}).
\citet{Oaks2012} further discuss how the use of uniform prior distributions in
\msb for many of the model's parameters requires investigators to select broad
priors to avoid excluding the truth \textit{a priori}, and how these broad
uniform priors place too much density in improbable regions of parameter space.
What \citet{Hickerson2013} mean by ``poorly selected'' is that
\citet{Oaks2012} did not use their data to inform their initial prior
distributions (i.e., they did not use an empirical Bayesian approach).
\citet{Oaks2012} did use empirically informed priors when assessing both the
prior sensitivity of the method and the power of \msb under real-world
situations when a large amount of prior information is available. 
In doing so, \citet{Oaks2012} discuss the potential dangers of taking an
empirical Bayesian approach to model choice (see the last paragraph of
``Assessing prior sensitivity of \msb'' in \citet{Oaks2012}), and we expand on
this here by exploring both the theoretical and practical implications of using
empirically informed priors for Bayesian model choice.

\subsection*{Theoretical implications of empirical priors for Bayesian model
choice}
\begin{linenomath}
Bayesian inference is a method of inductive learning in which Bayes' rule is
used to update our beliefs about a model $M$ as new information becomes
available.
If we let \allParameterValues represent the set of all possible parameter
values for model $M$, we can define a prior distribution for all $\theta \in
\allParameterValues$ such that $p(\theta \given M)$ describes our belief that
any given \myTheta{} is the true value of the parameter.
If we let \allDatasets represent all possible datasets then we can 
define a sampling model for all $\theta \in
\allParameterValues$ and $\alignment{}{} \in \allDatasets$ such that
$p(\alignment{}{} | \theta, M)$ measures our belief that any dataset \alignment{}{}
will be generated by any state \myTheta{} of model $M$.
After collecting a new dataset \alignment{i}{}, we can use Bayes' rule to
calculate the posterior distribution
\begin{equation}
    p(\myTheta{} \given \alignment{i}{}, M) = \frac{p(\alignment{i}{} \given
    \myTheta{}, M)p(\myTheta{}, M)}{p(\alignment{i}{} \given M)},
    \label{eq:bayesrule}
\end{equation}
where
\begin{equation}
    p(\alignment{i}{} \given M) = \int_{\myTheta{}} p(\alignment{i}{} \given
    \myTheta{}, M)p(\myTheta{} \given M) d\myTheta{}
    \label{eq:marginallikelhiood}
\end{equation}
is the marginal likelihood of the model.
The posterior distribution is a measure of our beliefs after seeing the new
information.
\end{linenomath}

This is an elegant method of updating our beliefs as data are accumulated.
However, this all hinges on the fact that the prior ($p(\myTheta{} \given M)$)
is defined for all possible parameter values independently of the new data
being analyzed.
Any other datasets or external information can safely be used to inform our
beliefs about $p(\myTheta{} \given M)$.
However, if the same data are used to both inform the prior and calculate the
posterior, the prior becomes conditional on the data, and Bayes' rule breaks
down.

Thus, empirical Bayes methods have an uncertain theoretical basis and
do not yield a valid posterior distribution from Bayes' rule \citep[e.g.,
empirical Bayesian estimates of the posterior are often too narrow, off-center,
and incorrectly shaped;][]{Morris1983,Laird1987,Carlin1990,Efron2013}.
This is not to say that empirical Bayesian approaches are not useful.
Empirical Bayes is a well-studied branch of Bayesian statistics that has given
rise to many inference methods that provide means of
parameter estimation that often exhibit favorable frequentist properties.
Furthermore, many post-hoc correction methods have been developed for
estimating confidence-intervals from empirical Bayes estimates of posterior
distributions that often exhibit well-behaved frequentist coverage
probabilities
\citep{Morris1983,Laird1987,Laird1989, Carlin1990,Hwang2009}.

Whereas empirical Bayes approaches can provide powerful methods for parameter
estimation, a theoretical justification for empirical Bayes approaches to model
choice is questionable.
In Bayesian model choice, the primary goal is not to estimate parameters, but
to estimate the relative probabilities of candidate models.
In a simple example where two candidate models, $M_1$ and $M_2$, are being
compared, the goal is to estimate the posterior probabilities of these two
models.
Again, we can use Bayes' rule to calculate this as
\begin{equation}
    p(M_1 \given \alignment{i}{}) = \frac{p(\alignment{i}{} \given
    M_1)p(M_1)}{p(\alignment{i}{} \given M_1)p(M_1) + p(\alignment{i}{} \given
    M_2)p(M_2)}.
    \label{eq:bayesmodelchoice}
\end{equation}
By comparing Equations \ref{eq:bayesrule} and \ref{eq:bayesmodelchoice}, we
see fundamental differences between Bayesian parameter estimation and
model choice.
In Equation \ref{eq:bayesrule}, we see that the posterior density of any
state $\myTheta{}$ of the model, is the prior density updated by the
probability of the data given the state $\myTheta{}$ (the likelihood of
$\myTheta{}$).
The integral over the entire parameter space of the model, which is defined by
the priors, only appears as a normalizing constant in the denominator.
Thus, as long as the joint prior distribution contains the values of
$\myTheta{}$ that maximize the probability of the data (i.e., the maximum
likelihood) and the data are strongly informative relative to the prior, the
modes of the estimated posterior distributions of parameters will be relatively
robust to prior choice, even if the shape of the posterior is incorrect due to
using the data to inform the priors.
This is why empirical Bayes can work well for estimating the values of model
parameters.

% Unlike parameter estimates, posterior probabilities represent a summary of the
% entire posterior distribution.
% Thus, given that empirical Bayesian posterior distributions are not accurate,
% there is no guarantee regarding the accuracy of probabilities summarized from
% them.

However, if we look at Equation~\ref{eq:bayesmodelchoice}, we see that in
Bayesian model choice it is now the \emph{marginal} likelihood of a model the
updates the prior to yield the model's posterior probability.
Thus the integral over the entire parameter space of the likelihood weighted by
the prior probability density is no longer a normalizing constant, but rather
is what informs the posterior probability of that model from the data.
As a result, Bayesian model choice tends to be much more sensitive to priors
than Bayesian parameter estimation.

Another important difference of Bayesian model choice illustrated by
Equation~\ref{eq:bayesmodelchoice} is that the value of interest, the
posterior probability of a model, is not a function of \myTheta{}, because
it is integrated out of the marginal likelihoods of the candidate models.
Thus, unlike parameter estimates, the estimated posterior probability of a
model is a single value (rather than a distribution) lacking a measure of
posterior uncertainty, and in most cases has no true value.
In most empirical applications, we know all candidate models are incorrect, but
our goal is to summarize the relative probabilities that they could produce the
observed data.
Model posterior probabilities are inherently measures of our belief in the
models after our prior beliefs are updated by the data being analyzed.

This complicates the meaning of model posterior probabilities when Bayes' rule
is violated by empirically informing priors.
For empirical Bayesian parameter estimation, the objective is that by giving the
data more weight relative to the prior, the posterior distribution will be
peaked near the true value(s) of the model's parameter(s).
There is no such justification for empirical Bayesian model choice, because
there are no true values for the model probabilities being estimated.
Using the data twice will simply mislead our posterior beliefs in the models
being compared.

These distinctions between Bayesian parameter estimation and model choice can
be illustrated with a simple example.
Let us say we are interested in the probability of a particular coin being
weighted toward landing heads (i.e., $p(p(heads) > 0.5)$).
We already have data from flipping a different coin 20 times that landed both
heads and tails 10 times, and so we decide to use these data in specifying a
beta prior on fairness of the new coin of $beta(a=10, b=10)$
(Figure~\ref{figCoinFlip}).
We collect data by flipping the coin $N=10$ times, $y=3$ of which land heads.
Given the beta distribution is a conjugate prior for a binomial likelihood, the
posterior distribution has the nice analytical form $\theta \given y,N \sim
beta(a + y, b + N - y)$, which for the new dataset is simply $beta(13, 17)$;
this is the true posterior distribution of $\theta$ given our prior information
and data (Figure~\ref{figCoinFlip}).
Thus, the posterior mode estimate of the probability of heads is 0.429, and the
posterior probability of our hypothesis is then $p(p(heads) > 0.5 \given
heads=3, N=10) = 0.23$.
To give the data more weight relative to the prior, we could use it twice, and
calculate an empirical Bayes estimate using a prior of $beta(13,17)$.  This
results in a distribution of $beta(16, 24)$ (Figure~\ref{figCoinFlip}), with a
mode of 0.395, and the $p(p(heads) > 0.5 \given heads=3, N=10) = 0.10$.
Whereas the estimate of the parameter $p(heads)$ is a distribution, the mode of
which changed only 8\% under the empirically informed prior, the posterior
probability of our model of interest ($p(p(heads) > 0.5)$) is a single value,
which changed by 56\% under the empirical prior.
From this example, we can see how Bayesian model choice is more sensitive
to priors, and how using the data twice can mislead the posterior
probability of a model in light of new data.

In addition to these theoretical concerns there are practical problems with
using narrow, empirical, uniform priors for a method that has already been
shown to be biased toward models with less parameter space.

% Let us say that principal investigator Mary and her new postdoc Will are
% interested in the hypothesis that the mass of George Washington's periwig
% renders the quarter dollar of the United States unfair.
% That is to say their null hypothesis is that the probability of a US
% quarter landing heads when tossed is less than 0.5 ($\theta < 0.5$).
% They can certainly evaluate this hypothesis, they can have undergraduate
% worker, Joe, flip the coin for them while they tabulate the results.
% But being Bayesians, before they call Joe into the lab, they agree on a
% prior probability to place on the set of all possible probabilities that
% the quarter will land heads when it is flipped.
% Given that neither of them have a quarter, and their prior knowledge that Joe
% moonlights as a magician, and is notorious for performing coin and card tricks
% in the lab, they suspect there is a good chance that the quarter Joe uses will
% be either two-headed or two-tailed.
% So, knowing that the beta distribution is the conjugate prior for a binomial
% likelihood, they decide to use a $beta(a=0.5, b=0.5)$ prior distribution
% (Figure~\ref{figCoinFlip}).

% Mary calls Joe into the lab, confirms that he has a quarter, and tells him to
% begin flipping it.
% After five tosses, four of which was heads, Joe decides that academics are
% crazy and leaves the lab to pursue a major in theatre.  Mary and Will, both
% being computational biologists, are satisfied with their
% empirical dataset of $y = 4$ heads out of $N = 5$ trials.
% They know from the conjugacy of the beta prior, that the posterior distribution
% has the nice analytical form $\theta|y,N \sim beta(a + y, b + N - y)$, which
% in this case is simply $beta(4.5, 1.5)$; this is the true posterior distribution
% of $\theta$ given their prior belief and data (Figure~\ref{figCoinFlip}).
% This allows them to plug these values into the beta cumulative distribution
% function to determine that the posterior probability of their hypothesis is
% $p(\theta < 0.5 | y=1, N=5) = 0.088$.
% Given their prior belief and dataset, this indeed is the correct posterior
% probability of the hypothesis, and Mary and Will should now update their
% posterior belief accordingly.

% However, reflecting upon the results of their experiment
% (Figure~\ref{figCoinFlip}), Mary and Will regret their choice of prior.
% Their prior looks very ``poorly selected,'' and if they had only known that the
% coin was fair before the data were collected, they would have selected a much
% better prior.
% Clearly, from their results, they should have used a prior centered
% around 0.2; their data suggest a prior of $beta(4.5, 1.5)$ would have been
% much ``better.''
% Rather than resort to flipping the coin five more times, Mary and Will decide
% to redo their analysis using the much better ``prior'' of $p(\theta) \sim
% beta(4.5, 1.5)$.
% This gives a ``posterior'' of $\theta|y,N \sim beta(8.5, 2.5)$, and a
% probability of their hypothesis of $p(\theta < 0.5 | y=1, N=5) = 0.026$.
% Now convinced that the US quarter is unfair, albeit not due to the mass of
% President Washington's head as they hypothesized, Will begins composing an
% e-mail of complaint addressed to the U.S.\ Mint.

% If Joe's flips were a representative sample, Mary and Will's empirical Bayes
% estimate might very well be a better point estimate for the parameter
% \myTheta{}.
% However, their empirical Bayes estimate of the probability of their hypothesis
% is incorrect and biased.
% This simple example shows how parameter estimation is fundamentally different
% from estimating the probability of a model.
% While empirically informed priors can be used to obtain well-behaved parameter
% estimators, using them for model choice is much less certain.




% %CWL:
% %I like the example, but it may be too cute. It is also too long. I think a
% %contrived example is good, but if there could be an example that is more
% %relevant to model choice and comparison of inaccurate posterior distributions,
% %I think that would be better. This is an example of empirical Bayes, but not
% %the potential problem with the use in msbayes. Unless I am interpreting wrong.
% %I would be careful about making it to cute as well. You may intend to make it
% %easy to understand for a general audience, but it can also seem like talking
% %down to Hickerson et al, who we are responding to. It just depends on how you
% %read it.
% This can be demonstrated with a simple, albeit contrived, example.
% Let us say that principal investigator Mary and her new postdoc Will are
% interested in the hypothesis that the mass of George Washington's periwig
% renders the quarter dollar of the United States unfair.
% That is to say their null hypothesis is that the probability of a US
% quarter landing heads when tossed is less than 0.5 ($\theta < 0.5$).
% They can certainly evaluate this hypothesis, they can have undergraduate
% worker, Joe, flip the coin for them while they tabulate the results.
% But being Bayesians, before they call Joe into the lab, they agree on a
% prior probability to place on the set of all possible probabilities that
% the quarter will land heads when it is flipped.
% Given that neither of them have a quarter, and their prior knowledge that Joe
% moonlights as a magician, and is notorious for performing coin and card tricks
% in the lab, they suspect there is a good chance that the quarter Joe uses will
% be either two-headed or two-tailed.
% So, knowing that the beta distribution is the conjugate prior for a binomial
% likelihood, they decide to use a $beta(a=0.5, b=0.5)$ prior distribution
% (Figure~\ref{figCoinFlip}).

% Mary calls Joe into the lab, confirms that he has a quarter, and tells him to
% begin flipping it.
% After five tosses, four of which was heads, Joe decides that academics are
% crazy and leaves the lab to pursue a major in theatre.  Mary and Will, both
% being computational biologists, are satisfied with their
% empirical dataset of $y = 4$ heads out of $N = 5$ trials.
% They know from the conjugacy of the beta prior, that the posterior distribution
% has the nice analytical form $\theta|y,N \sim beta(a + y, b + N - y)$, which
% in this case is simply $beta(4.5, 1.5)$; this is the true posterior distribution
% of $\theta$ given their prior belief and data (Figure~\ref{figCoinFlip}).
% This allows them to plug these values into the beta cumulative distribution
% function to determine that the posterior probability of their hypothesis is
% $p(\theta < 0.5 | y=1, N=5) = 0.088$.
% Given their prior belief and dataset, this indeed is the correct posterior
% probability of the hypothesis, and Mary and Will should now update their
% posterior belief accordingly.

% However, reflecting upon the results of their experiment
% (Figure~\ref{figCoinFlip}), Mary and Will regret their choice of prior.
% Their prior looks very ``poorly selected,'' and if they had only known that the
% coin was fair before the data were collected, they would have selected a much
% better prior.
% Clearly, from their results, they should have used a prior centered
% around 0.2; their data suggest a prior of $beta(4.5, 1.5)$ would have been
% much ``better.''
% Rather than resort to flipping the coin five more times, Mary and Will decide
% to redo their analysis using the much better ``prior'' of $p(\theta) \sim
% beta(4.5, 1.5)$.
% This gives a ``posterior'' of $\theta|y,N \sim beta(8.5, 2.5)$, and a
% probability of their hypothesis of $p(\theta < 0.5 | y=1, N=5) = 0.026$.
% Now convinced that the US quarter is unfair, albeit not due to the mass of
% President Washington's head as they hypothesized, Will begins composing an
% e-mail of complaint addressed to the U.S.\ Mint.

% If Joe's flips were a representative sample, Mary and Will's empirical Bayes
% estimate might very well be a better point estimate for the parameter
% \myTheta{}.
% However, their empirical Bayes estimate of the probability of their hypothesis
% is incorrect and biased.
% This simple example shows how parameter estimation is fundamentally different
% from estimating the probability of a model.
% While empirically informed priors can be used to obtain well-behaved parameter
% estimators, using them for model choice is much less certain.

\subsection*{Practical concerns about empirically informed uniform priors for
    Bayesian model choice}
% \cwlnote{}{Should this be part of the empirical Bayes section, or something
%     else?}

% \cwlnote{}{It seems like this section can be boiled down to ``empirical Bayes
%     approaches for model choice when using uniform priors can exclude the
%     $truth$ from the prior, which is a violation of Bayes theorem" or something
%     like that. Picking apart what they calculated and we calculated seems like
%     too much detail. I would try to simplify how this is being presented. This
%     can lead into how the use of more appropriate prior distributions should be
%     used.}
The results of Hickerson et al.'s \citeyear{Hickerson2013} reanalysis of the
Philippine dataset strongly favored models with the narrowest, empirically
informed prior on divergence time, and thus their model-averaged posterior
estimates are dominated by models $M_1$ and $M_2$ (see Table 1 of
\citet{Hickerson2013}).
This is concerning, because the narrowest \divt{} prior used by
\citet{Hickerson2013} ($\divt{} \sim U(0,0.1)$) likely excludes the true
divergence times for at least some of the Philippine taxa, a major problem when
using uniform priors.
\citet{Hickerson2013} set this prior to match the 95\% highest posterior
density (HPD) interval for the mean divergence time estimated under one of the
priors used by \citet{Oaks2012} (see Tables 2 and 3 of \citet{Oaks2012}).
Given this interval estimate is for the \emph{mean} divergence time across all
22 taxa, it may be inappropriate to set this as the limit on the prior, because
many of the taxon pairs are expected to have diverged at times older than the
upper limit.
Furthermore, this prior is \emph{excluded} from the 95\% HPD interval estimates
of the mean divergence time under the other two priors explored by
\citet{Oaks2012} (under these priors the 95\% HPD is approximately 0.3--0.6;
see Table~6 of \citet{Oaks2012}).
Thus, the results of \citet{Hickerson2013} indicate that integration over
parameter space with low likelihood is biasing the method toward models with
smaller parameter space, and as a consequence is biasing the method toward
models that exclude the truth (i.e., toward estimating model-averaged
posteriors dominated by models that exclude true values of parameters of the
model).


%CWL:This should be in the section on model-averaging, not empirical Bayes
%limitations
We explored the propensity of the empirically informed model-averaging approach
of \citet{Hickerson2013} to exclude the truth in two ways.
First, we re-analyzed the Philippines dataset using the model-averaging approach
of \citet{Hickerson2013}, but set one of prior models with a uniform prior on
divergence times that is unrealistically narrow, and almost certainly excludes
most, if not all, of the true divergence times of the 22 taxon pairs.
If small likelihoods of large models cause the method to prefer models with
less parameter space, we expect \msb will preferentially sample from this
incorrect model yielding a marginal posterior that is incorrect (i.e., the
model-averaged posterior will be dominated by an incorrect model that excludes
the truth).
Second, we generated simulated datasets for which the divergence times are
drawn from an exponential distribution and applied the approach of
\citet{Hickerson2013} to each of them to see how often the method excludes the
truth.

For our re-analysis of the Philippines dataset we used the model-averaging
approach of \citet{Hickerson2013}, but with a reduced set of prior models to
avoid mixing time units.
We used five prior models, all of which had priors on population sizes of
$\meanDescendantTheta{} \sim U(0.0001, 0.1)$ and $\ancestralTheta{} \sim
U(0.0001, 0.05)$.
Following \citet{Hickerson2013}, each of these models had the following
priors on divergence time parameters:
$M_1$, $\divt{} \sim U(0, 0.1)$;
$M_2$, $\divt{} \sim U(0, 1)$;
$M_3$, $\divt{} \sim U(0, 5)$;
$M_4$, $\divt{} \sim U(0, 10)$; and
$M_5$, $\divt{} \sim U(0, 20)$.
We simulated $1\e6$ random samples from each of the models for a total of
$5\e6$ prior samples.
For each model, we retained the 10,000 samples with the smallest Euclidean
distance from the observed summary statistics, standardizing the statistics
using the prior means and standard deviations of the given model.
From the remaining 50,000 samples, we then retained the 10,000 samples with the
smallest Euclidean distance from the observed summary statistics, this time
standardizing the statistics using the prior means and standard deviations
across all five models.
We then repeated this analysis twice, replacing the $M_1$ with
$M_{1A}$ and $M_{1B}$, which differ only by having priors on divergence
times of $\divt{} \sim U(0, 0.01)$ and $\divt{} \sim U(0, 0.001)$,
respectively.
While we suspect the prior of $\divt{} \sim U(0, 0.1)$ used by
\citet{Hickerson2013} likely excludes the true divergence times of at least
some of the 22 taxa, we are nearly certain that these narrower priors are
incorrect and exclude most, if not all, of the divergence times of the
Philippine taxa.

Our results show that the model-averaging approach of \citet{Hickerson2013}
does not reduce the method's bias toward models with less parameter space,
but rather allows it to manifest in a rather dangerous way.
The method strongly prefers the prior model with the narrowest distribution on
divergence times across all three of our analyses, even when this model is
almost certainly incorrect and excludes the true divergence times of the
Philippine taxa (Table~\ref{tabModelChoiceEmpirical}).
Unfortunately, ``checking'' the priors by plotting the summary statistics from
1000 random samples of each prior model along the first two orthogonal axes of
a principle component analysis, as recommended by \citet{Hickerson2013},
provides no warning of a problem (Figure~\ref{figPCA}).
Given that the results of \citet{Hickerson2013} strongly prefer the models with
the narrowest prior on divergence times, it seems quite likely that their
model-averaged results are dominated by models that exclude at least some of
the 22 true divergence times, making their results difficult to interpret
(in addition to the error of time units).

To better quantify the propensity of Hickerson et al.'s
(\citeyear{Hickerson2013}) approach to exclude the truth, we simulated 1000
datasets in which the divergence times for the 22 population pairs are drawn
randomly from an exponential distribution with a mean of 0.5 ($\divt{} \sim
Exp(2)$).
All other parameters were identically distributed as the $M_1$--$M_5$ models
(Table~\ref{tabModelChoiceEmpirical}).
We then repeated the analysis described above using $1\e6$ random samples from
prior models $M_1$, $M_2$, $M_3$, $M_4$, and $M_5$, retaining 1000 posterior
samples for each of the 1000 simulated datasets.

% \highLight{Add how we determined exclusion.}
For each simulation replicate, we estimated the Bayes factor in favor
of excluding the truth as the ratio of the posterior to prior odds of
excluding the true value of at least one parameter.
Whenever the Bayes factor preferred a model excluding the truth we counted
the number of 22 true divergence times that were excluded by the preferred
model.
Our results show that the model-averaging approach of \citet{Hickerson2013}
excludes the true values of parameters in 97\% of the replicates (90\% with
GLM-regression adjustment), excluding up to 21 of the 22 true divergence times
(Figure~\ref{figExclusionSimTau}).
We also used the mode estimate of the preferred model for each replicate to
estimate the number of true values excluded, which produced very similar
results.
Furthermore, the posterior probability of excluding at least one true parameter
value is very high in nearly all of the replicates
(Figure~\ref{figExclusionSimProb}).
Using a Bayes factor of greater than 10 as a criterion for strong support, 66\%
of the replicates (87\% with GLM-regression adjustment) strongly support the
exclusion of true values (Figure~\ref{figExclusionSimProb}).

The results of our empirical and simulation-based analyses clearly demonstrate
the danger of using narrow, empirically guided uniform priors in a Bayesian
model-averaging framework.
% This is especially concerning in \msb, which has already been demonstrated to
% be biased away from models with more parameter space \citep{Oaks2012}.
The consequence of this approach is a high risk of obtaining a model-averaged
posterior estimate that is heavily weighted toward incorrect models that
exclude true values of model parameters.


\subsection*{Additional thoughts on empirical priors in Bayesian model choice}
Given all of the theoretical and practical issues with empirical Bayes
approaches to Bayesian model choice discussed in the proceeding sections, it is
quite clear why one should use caution before overly criticizing an
investigator's choice of priors after having seen the resulting posterior.
%In a strictly Bayesian world, this is cheating.
As discussed by \citet{Oaks2012}, prior to analyzing the data, there was a large
amount of uncertainty regarding the divergence times of the 22 population pairs
under study.
Two of these pairs represent distinct species, and the taxonomy of many groups
in the Philippines has repeatedly been shown to mask deeply divergent lineages
\citep{RafeDiesmosAlcala2008,Linkem2010,Siler2010,Welton2010,Siler2011HerpMonographs,
    Siler2011,Siler2012,RafeStuart2012,LinkemBrown2013}.
\citet{Oaks2012} also discuss how the sole use of uniform priors in \msb makes
it very difficult for investigators to express their prior uncertainty without
putting high prior density in regions of improbable parameter space.
The alternative, as shown above, is to risk excluding the truth before
performing the analysis.

While our results above clearly demonstrate the risks inherent to the empirical
Bayesian model-choice approach used by \citet{Hickerson2013}, one could justify
such risk if the method does indeed increase the power of the method and thus
decrease bias toward clustered divergence models, as claimed by
\citet{Hickerson2013}.
In the next section, we use simulation-based analyses to evaluate the power of
Hickerson et al.'s (\citeyear{Hickerson2013}) approach, which show the method is
still biased toward highly clustered models of divergence.





\section*{Assessing the power of the model-averaging approach of
    \citet{Hickerson2013}}
% \cwlnote{}{I think the empirical use of the model-averaging should go here,
%     instead of above in the empirical Bayes section}

As recommended by \citet{Oaks2012}, we perform simulation-based analyses
to explore the power of the approximate Bayesian model-averaging approach
proposed by \citet{Hickerson2013}.
Following \citet{Oaks2012}, we simulated 1000 datasets with \divt{} for each of
the 22 population pairs randomly drawn from a uniform distribution, $U(0,
\divt{max})$, where \divt{max} was set to: 0.2, 0.4, 0.6, 0.8, 1.0, and 2.0, in
\globalcoalunit generations.
All other parameters were identically distributed as the prior.
We use the same $5\e6$ samples from the same set of five prior models as above
($M_1$, $M_2$, $M_3$, $M_4$, and $M_5$).
As described above, for each simulated dataset, we approximated the posterior
by retaining 1000 samples from the prior with the smallest Euclidean distance
from the true summary statistics.
In total, we analyzed 6000 replicate datasets, retaining 1000 model-averaged
posterior samples for each of them.

We find that the approach of \citet{Hickerson2013} struggles to estimate the
dispersion index of divergence times (\vmratio{}) across most of the \divt{max}
we simulated, whether evaluating the unadjusted
(Figure~\ref{figPowerAccOmegaMedian}) or GLM-adjusted
(Figure~\ref{figPowerAccOmegaModeGLM}) posterior estimates.
The method only estimates \vmratio{} relatively well when the simulated
distribution of divergence times is identical to one of the prior models
(Figures~\ref{figPowerAccOmegaMedian}E \& \ref{figPowerAccOmegaModeGLM}E).
This is consistent with the conclusion of \citet{Oaks2012} that \msb lacks
robustness and is highly sensitive to the prior distribution deviating from the
true, underlying distribution of the data.

Furthermore, our results demonstrate that the approach of \citet{Hickerson2013}
consistently infers highly clustered divergences across all the \divt{max} we
simulated (Figure~\ref{figPowerNumExcluded}).
The method is most likely to infer the extreme case of a single divergence event
when populations diverged randomly over the past \globalcoalunit generations.
Even when divergences are random over the past $8\globalpopsize$ generations,
the most likely inference is only two divergence events, and a single 
divergence is still estimated in more than 10\% of the replicates.
It is very interesting to note that as \divt{max} increases, but before the
estimates are finally pulled away from $\numt{} = 1$, the distribution of
\numt{} estimates closely mirror the odd U-shaped prior on divergence models
used by \msb (see Figure~\ref{figPowerNumExcluded}E and Oaks et al.'s
(\citeyear{Oaks2012}) Figure 5B).
This supports the conclusions of \citet{Oaks2012} that this U-shaped
prior coupled with the poor marginal likelihoods of models with many
\divt{} parameters, is a major cause of the method's bias toward
clustered divergence models.

Looking at our simulation results in terms of the posterior probability of the
dispersion index of divergence times supporting the extreme case of one
divergence event (i.e., $p(\vmratio{} < 0.01 \given \ssSpace)$), we find the
method strongly supports one divergence in greater than 27\% of the replicates
across all the \divt{max} we simulated (Figure~\ref{figPowerOmegaProb}).
Following \citet{Hickerson2013}, we use a Bayes factor of greater than 10 as
the criterion for incorrect inference of a single divergence event.
There is strong support for a single divergence event in more than 90\% of the
replicates when divergences are random over the past $2.4\globalpopsize$
generations, and more than 60\% when over the past $3.2\globalpopsize$
generations or less (Figure~\ref{figPowerOmegaProb}).

Contrary to Hickerson et al.'s (\citeyear{Hickerson2013}) claim that their
model-averaging approach ``can discriminate complex multispecies histories and
correctly reject synchronous divergence, even when discrete divergence times
differ by much less than \ldots millions of generations,'' our results show
that method is biased toward supporting the extreme case of a single
divergence event when populations diverged randomly over the last
$8\globalpopsize$ generations.
To put this on the scale roughly consistent with a vertebrate mitochondrial
locus, assuming a mutation rate of $2\e{-8}$ per site per generation, this
translates to 5 million generations.
Assuming a mutation rate consistent with nuclear loci of $1\e{-9}$, this is 100
million generations.

The results of our power analyses further demonstrate the propensity of
Hickerson et al.'s (\citeyear{Hickerson2013}) approach to exclude true
parameter values.
Across all but one of the \divt{max} we simulated, the method excludes the
truth in a large proportion of replicates, and across many of the \divt{max} it
will exclude a large proportion of the true divergence time values
(Figure~\ref{figPowerNumExcluded}).
The posterior probability of excluding at least on true divergence value is
also quite high across many of the \divt{max}
(Figure~\ref{figPowerProbExclusion}).
Only when the data are identically distributed as one of the prior models does
the method avoid excluding the truth more than 5\% of the time
(Figure~\ref{figPowerNumExcluded}E).
Again, this demonstrates the method's lack of robustness.

Given our results, we conclude that the approach of \citet{Hickerson2013}
excludes regions of parameter space containing the truth, and lacks power to
detect random variation in divergence times over a scale of $8\globalpopsize$
generations or more.
This roughly translates to millions of generations for mitochondrial loci, and
hundreds of millions of generations for nuclear loci.


%CWL: I would suggest removing this section
\section*{The power analysis of \citet{Hickerson2013}}
\citet{Hickerson2013} present a power analysis in which they find that
\msb has the power to infer multiple divergence events when divergence times
are random over hundreds of thousands of generations (rather than millions as
demonstrated by \citet{Oaks2012}).
It is not surprising that under limited parameter space the method has
increased power to detect temporal variation in divergences over narrower time
windows than millions of generations.
However, what is important to consider is whether those conditions are relevant
to real-world applications of the method.
\citet{Oaks2012} explore the behavior of the method under three divergence time
priors, as narrow as 0--5 coalescent units. This is quite narrow considering
that this expresses the prior belief that all 22 taxon pairs diverged within
this window.
\citet{Hickerson2013} limit their power analysis to a single prior of 0--1
coalescent unit.
This prior setting assumes that there is enough a priori information to
be 100\% certain that all taxa (18 for their simulations) diverged within
the last 4$\globalpopsize$ generations.
This does not seem applicable to most empirical systems.
Furthermore, even when such extensive prior information is available, to only
be able to detect multiple divergences on the scale of hundreds of thousands of
generations does not seem very powerful.

\citet{Hickerson2013} present information only on the extreme case of a
single divergence event.
As discussed in \citet{Oaks2012}, it seems odd to consider the estimation
of two divergence events as a success when divergence is random over hundreds
of thousands (or millions) of generations.
In an empirical system such as the Philippines, where island fragmentation has
occurred at least 10 times over the past several millions of years
\citep{Haq1987,Rohling1998,Siddall2003,Miller2005}, an estimate where 22 taxa
share even a handful of divergence events would be of biogeographic interest.

Lastly, \citet{Hickerson2013} translate their results from units of
\globalcoalunit generations to generations assuming a mutation rate of
$1.92\e{-8}$ mutations per site per generation.
If we translate their results to a scale more consistent with rates of mutation
of nuclear loci ($1\e{-9}$), even under the very optimistic prior settings used
by \citet{Hickerson2013} the method can only reliably reject the extreme case
of one divergence event when divergences are random over a period of more than
3 million generations.
Again, considering \citet{Hickerson2013} are assuming prior knowledge that
all population pairs diverged within the last coalescent unit, the
method has low power to detect random variation in divergence times.




\section*{Model likelihoods or insufficient prior sampling as the cause of bias
in \msb}
One of the points of disagreement between \citet{Hickerson2013} and
\citet{Oaks2012} is the underlying mechanism causing the bias toward models
with clustered divergences (i.e., models with few divergence events).
\citet{Oaks2012} present simulation-based analyses that suggest the broad
uniform prior on divergence time parameters decrease the marginal likelihoods
of models with more divergence time parameters, whereas \citet{Hickerson2013}
argue that the bias is inherent to the inefficient rejection algorithm
implemented in \msb.

\citet{Hickerson2013} present an interesting probabilistic argument
 to show that insufficient prior sampling is to blame for the bias.
They argue the widest of the three priors used by \citet{Oaks2012} was very
unlikely to produce any prior samples with large numbers of divergence times
that were consistent with the Philippine data.
There are a few issues with their argument.
First, the probabilities they present assume the gene divergence times
estimated by \citet{Oaks2012} are correct.
The sole purpose of the estimates of ultrametric gene trees presented by
\citet{Oaks2012} was to provide a very rough comparison of the gene divergence
times across the 22 taxa.  These analyses assumed an arbitrary strict clock of
$2\e{-8}$ for all taxa, and are, of course, subject to estimation error.
Furthermore, the branch-length units of the gene trees are in millions of
years, whereas the divergence time prior of \msb is in generations, thus the
logic of \citet{Hickerson2013} requires the additional assumption that all 22
Philippine taxa have a generation time of one year.
Thus, the argument of \citet{Hickerson2013} that divergence time estimates of
\citet{Oaks2012} ``should set an upper bound on their prior for \divt{}'' seems
questionable, especially given our findings on the behavior of empirically
informed priors presented above.

Finally, even \emph{if} we make all of these assumptions and assume the gene
divergence times are estimated without error, the probabilistic argument only
applies to one of the three different priors used by \citet{Oaks2012}.
The narrowest prior on divergence times used by \citet{Oaks2012} closely mirrors
the range of estimates of gene-divergence times, and applying Hickerson et al.'s
(\citeyear{Hickerson2013}) probability equations demonstrates that the prior
is densely populated with samples with large numbers of divergence parameters
that are consistent with the gene divergence estimates.
Thus, according to the argument of \citet{Hickerson2013}, if insufficient prior
sampling is to blame for the bias, it should be much reduced under the narrow
prior on \divt{}.
However, the magnitude of the bias is very similar across all three priors
explored by \citet{Oaks2012}.
\citet{Hickerson2013} point out a case where the narrow prior performs
slightly better (panel L of Figures S32, S37, and S38 of \citet{Oaks2012}).
However, it is important to note that these results suffered from a bug
in \msb, and there are many cases after \citet{Oaks2012} corrected the 
bug where the narrow prior performs slightly worse (see panels D--J of
Figures 3 and S12).

To disentangle whether model likelihoods or insufficient prior sampling is to
blame for the biases revealed by \citet{Oaks2012}, we must look at the
different predictions made by these two phenomena.
One example, as discussed by \citet{Oaks2012}, is that insufficient prior
sampling should create higher variance in posterior estimates, and thus it
should cause analyses to be sensitive to the number of samples drawn from the
prior.
\citet{Oaks2012} do not see such sensitivity when they compare prior sample
sizes of $2\e6$, $5\e6$, and $10^7$.

To explore this prediction further, we repeat the analysis of the Philippines
dataset under the intermediate prior used by \citet{Oaks2012} ($\divt{} \sim U(0,
10)$, $\meanDescendantTheta{} \sim (0.0005, 0.04)$, $\ancestralTheta{} \sim
(0.0005, 0.02)$), using a very large prior sample size of $10^8$.
When we look at the trace of the estimates of the dispersion index of
divergence times (\vmratio{}) as the prior samples accumulate
(Figure~\ref{figSamplingError}) we see no trend in either the unadjusted or
GLM-regression-adjusted estimates.
This suggests that insufficient prior sampling did not play a large
role in the bias found by \citet{Oaks2012}.

Our results presented above that demonstrate the bias of the model-averaging
approach of \citet{Hickerson2013} both toward models with narrower \divt{}
priors (Table~\ref{tabModelChoiceEmpirical} and
Figs.~\labelcref{figExclusionSimTau,figExclusionSimProb,figPowerNumExcluded,figPowerProbExclusion})
and models with fewer \divt{} parameters (Figs.~\ref{figPowerPsiMode} \&
\ref{figPowerOmegaProb}) strongly suggest the primary cause is the uniform
priors reducing the marginal likelihoods of large models.
In all of our model-averaging analyses, all prior models have the same number
of samples.
% Thus, it is difficult to explain the method's propensity toward the model with
% the smallest parameter space via insufficient prior sampling.
While analyses that sample each model proportional to their relative parameter
space could be explored, it seems much more likely that the broad uniform
priors are simply inhibiting the marginal likelihoods of spacious models.

As discussed by \citet{Oaks2012}, a straightforward prediction of reduced
likelihoods of complex models is that they should disappear as the model
generating the data converges to the prior.
\citet{Oaks2012} test this prediction by performing 100,000 simulations to
assess the model-choice behavior of \msb when the prior model is correct.
They find that the bias actually switches direction (at least for the
regression-adjusted estimates) and the method tends to underestimate the
probability of the model with one divergence event (see Figure 4 of
\citet{Oaks2012}).
The same prediction is not as straightforward for the insufficient-sampling
hypothesis.
Even when the prior is correct, due to the discrete uniform prior on
\numt{} implemented in \msb, models with larger numbers of divergence
events (and thus greater parameter space) will still be less densely
sampled than those with fewer divergence events \citep{Oaks2012}.
Thus, the results of the simulations of \citet{Oaks2012} are more consistent
with marginal model likelihoods causing the bias toward models with fewer
divergence time parameters.

Taken together, the results presented here and in \citet{Oaks2012} support
reduced marginal likelihoods of large models as a primary mechanism by which
broad uniform priors cause biases in \msb.
Nonetheless, posterior sampling error will always be present in any numerical
Bayesian approximation method.
Thus, insufficient sampling of the prior will contribute to the error of all
approximate Bayesian estimates.
However, there is no strong evidence that it is playing a large role in
the biases revealed herein and by \citet{Oaks2012}.




\section*{Some general thoughts on the model of \msb}
Our results demonstrating the poor behavior of the model-averaging approach
proposed by \citet{Hickerson2013} are not too surprising.
While this approach is trivial to implement, it is not a trivial change
to the \msb model.
To better understand the reason for this, it might help to step back and get a
sense of the scale of the \msb model.
To do this, we will use the dataset of 22 vertebrate taxon pairs of
\citet{Oaks2012} as an example.

Following the model description and notation of \citet{Oaks2012}, let us tally
up all of the free parameters in the \msb model.  Under the simplest model in
\msb (i.e., assuming no migration and
no intra-locus recombination), the number of parameters for each
taxon pair include:
The population sizes of the ancestral and descendant populations
(\ancestralTheta{}, \descendantTheta{1}{}, \descendantTheta{2}{}),
the magnitude of population contraction in each of the descendant
populations (\bottleScalar{1}{} and \bottleScalar{2}{}) and the
timing of these contractions (\bottleTime{}), and the $N-1$ node heights
(coalescent times) of the gene tree that gave rise to the $N$ gene
copies sampled from both populations of the pair.
Because we only have a single locus for each taxon, there are no locus-specific
\myTheta{}-scaling parameters (\locusMutationRateScalar{}) or
\locusRateHetShapeParameter shape parameter for the gamma prior distribution on
\locusMutationRateScalar{}.
Lastly, there are between one and 22 divergence time parameters \divt{} in
the vector \divtvector.
Overall, for our Philippines data under the simplest model in \msb, there are
581--602 free parameters (depending on the cardinality of \divtvector).
Furthermore, under this rich model, the method is sampling over 1002 divergence
models \citep[i.e., the number of integer partitions of $Y=22$;][]{Oaks2012}.

This is a very difficult inference problem, and the method only uses four
summary statistics calculated from the sequence alignment of each taxon pair:
$\pi$ \citep{Tajima1983}, $\theta_W$ \citep{Watterson1975}, $\pi_{net}$
\citep{Takahata1985}, and $SD(\pi-\theta_W)$ \citep{Tajima1989}.
That gives us a total of 88 summary statistics (four from each of the 22 taxon
pairs), which contain minimal information about many of the $\approx 600$
parameters in the model.
More summary statistics can be used in \msb, but most are highly correlated
with these four statistics (which are even highly correlated among themselves),
and thus contribute little additional information about the data.

The large number of parameters and divergence models relative to 
summary statistics is undoubtedly a key reason the method is so sensitive
to the prior distributions.
It also likely contributes to the method's lack of robustness.
% We use robustness here as a measure of a method's insensitivity to model
% violations.
Robustness is an extremely important characteristic of a method to gauge its
applicability to real-world data, because we know the model and priors will be
wrong to some degree.

The approach of \citet{Hickerson2013} adds an additional dimension of model
choice to the model. They expand the model to sample over eight prior models.
This extends the original model to having 582-603 free parameters and, more
importantly, sampling over 8016 unique model states across both divergence
models and prior models.
This is a non-trivial extension of the model and, given the method's very large
number of parameters and divergence models relative to the information used
from the data, likely plays a major role in the poor behavior of this approach.
% Certainly, such a large modification of the model demands simulation-based
% assessments of the behavior of the new model, as we provide here.

In theory, the approach of \citet{Hickerson2013} is very appealing.  It sums
over multiple candidate prior models to produce a posterior estimate
marginalized over the uncertainty in prior choice.
In general, Bayesian model-averaging is a powerful approach that leverages a
great strength of Bayesian statistical procedures, namely the ability to
obtain marginalized estimates that incorporate uncertainty in nuisance
parameters.
However, given the basic \msb model is already struggling to estimate
a huge number of parameters and model probabilities with scant information
from the data, it is not surprising that adding another dimension of
model choice to the method causes problems.

The recommendations of \citet{Oaks2012} for mitigating the bias and lack of
robustness of \msb are actually similar to those of \citet{Hickerson2013}, but
avoid the need for imposing additional model choice.
\citet{Oaks2012} suggest that uniform priors are inappropriate for many
parameters of the \msb model, and recommend the use of probability
distributions from the exponential family.
If we look at the prior distribution on the divergence time parameter \divt{}
imposed by the model-averaging approach of \citet{Hickerson2013} we see
it is a mixture of overlapping uniforms with lower limits of zero
(Figure~\ref{figMCTauPrior}).
This looks very much like an exponential distribution, except that each sample
of divergence times are restricted to the hard bounds of one of the eight prior
models.
Thus, it seems more appropriate to place a gamma prior (the exponential being a
special case of the gamma) on divergence times.
This would provide investigators flexibility to represent their prior
uncertainty in model parameters while avoiding broad uniform
distributions and the need of costly model-averaging.
This seems like a natural solution to the problem of skewed marginal
likelihoods and/or sampling error, which are both likely exacerbated by the
fact that uniform distributions are the only choice of prior for many of the
parameters of the \msb model \citep[\divt{}, \ancestralTheta{},
\descendantTheta{1}{}, \descendantTheta{2}{}, \bottleTime{},
\bottleScalar{1}{}, \bottleScalar{2}{}, \locusRateHetShapeParameter,
\migrationRate{}, \recombinationRate;][]{Oaks2012}.

% Importantly, this strategy is easy to implement.
Given that ABC methods only need to draw random values from prior distributions
(hence there are no difficult prior and proposal ratios to calculate, etc.) it
is easy to use the most appropriate distributions on parameters.
Also, this approach reduces the temptation of empirically guided priors.
The investigator can place the majority of the prior density in regions of
parameter space they believe, \emph{a priori}, are most plausible, but still
capture uncertainty in the tails of distributions with low density.
As discussed in \citet{Oaks2012} the use of uniform priors necessitate
broad priors to avoid excluding the truth \emph{a priori}.
Thus, to avoid the behavior of the method under these uniform priors, using the
data is a very tempting, albeit risky, option \citep{Hickerson2013}.
More appropriate prior distributions would alleviate this issue.



%CWL: This seems to nit-picky. Probably not worth including it
\section*{Other clarifications}

\subsection*{Graphical prior comparisons}
\citet{Hickerson2013} advocate the use of what they call graphical checks of
prior models.
This entails generating a small number (1000) of random samples from the prior
and plotting the resulting summary statistics in comparison to the observed
statistics to see if they coincide (see Figure 1 of \citet{Hickerson2013}).
As we show above, this strategy can be misleading, because the resulting plots
of this approach have little correlation with the appropriateness of priors.
Given the richness of the \msb model ($\approx 600$ parameters for the Philippine
dataset analyzed by \citet{Hickerson2013}), we do not expect that 1000
\emph{random} draws from the vast prior parameter space will yield data and
summary statistics consistent with the observed data.
In fact, when such random draws are tightly clustered around the observed
statistics, this can be an indication that the prior is over-fit, as we show
above (Table~\ref{tabModelChoiceEmpirical} and Figure~\ref{figPCA}).
Thus, using such plots to select priors should be avoided, and the use of
posterior predictive analyses would be much more informative about the overall
fit of models.

\subsection*{Saturation of summary statistics}
\citet{Hickerson2013} claim the priors used by \citet{Oaks2012} ``cause much of
the explored parameter space to be beyond the threshold of saturation in most
mtDNA genes.'' To explore this possibility, we simulated datasets under prior
settings that match two of the three priors used by \citet{Oaks2012}:
$\meanDescendantTheta{} \sim U(0.0005, 0.04)$ and $\ancestralTheta{} \sim
U(0.0005, 0.02)$.
Under this prior, we draw divergence time parameters from a uniform
distribution of $U(0, 20)$, simulate datasets, and plot the \divt{} values
against the summary statistics calculated from the resulting datasets
(Figure~\ref{figSaturationPlot}).
Clearly, the priors used by \citet{Oaks2012} with upper limits on \divt{} of five
and 10 suffered from little to no effect from saturation.
Even at divergence times of 20 coalescent units, there is still signal in the
summary statistics used by \msb (Figure~\ref{figSaturationPlot}).
Thus, the assertion of \citet{Hickerson2013} does not apply to at least
two of the priors used by \citet{Oaks2012} and, as a result, does not
explain the bias they found.



\section*{Conclusions}
We demonstrate how the approximate Bayesian model choice method implemented in
\msb can be strongly biased away from models with greater parameter space.
As suggested by \citet{Oaks2012}, this is likely caused by the use of uniform
priors on most of the model's parameters.
Uniform distributions necessitate the use of broad priors that place high prior
density in unlikely regions of parameter space, less the risk of excluding the
truth \emph{a priori}.
This likely reduces the marginal likelihoods of models with greater parameter
space, either due to more divergence time parameters or broader prior
distributions on those parameters.
We show that the empirical Bayesian model-averaging approach of
\citet{Hickerson2013} does not mitigate this bias, but rather causes it to
manifest by sampling predominantly from models that may exclude the true values
of the parameters.
Exploring alternative prior probability distributions on most of the model's
parameters, in addition to different priors over divergence models, should
help mitigate the method's biases.

Based on our results, we discourage the use of empirically informed uniform
priors in Bayesian model choice, especially when combined with model-averaging
\ldots

We do not ``assumed that all previous \msb results are invalid,'' as suggested
by \citet{Hickerson2013}.
Rather, we conservatively recommend that the common inference of temporally
clustered divergences \citep{Barber2010, Bell2012, Carnaval2009, Chan2011,
Daza2010, Hickerson2006, Huang2011, Lawson2010, Leache2007, Plouviez2009,
Stone2012, Voje2009}, when not accompanied with the necessary simulation-based
analyses to guide the interpretation of such results, should be treated with
caution, because the method has been shown to spuriously infer clustered
divergences over a range of prior conditions.

The work presented herein follows the principles of Open Notebook Science.
All aspects of the work were recorded in real-time via version-control
software and are publicly available at
\href{https://github.com/joaks1/msbayes-experiments}{https://github.com/joaks1/msbayes-experiments}.
All information necessary to reproduce our results is provided there.

