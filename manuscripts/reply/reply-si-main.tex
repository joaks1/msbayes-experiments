\section{An error in Hickerson et al.'s re-analysis of the Philippines data}
\citet{Hickerson2013} re-analyzed the dataset of \citet{Oaks2012} using a
model-averaging approach, where they placed a discrete uniform prior over eight
different prior models (see Table 1 of \citet{Hickerson2013}).
However, there was an error in their methodology; their model mixes different
units of time.

Each of the eight prior models used in the re-analysis by \citet{Hickerson2013}
has one of two priors on the mean size of the descendant populations of each
taxon pair:
$\meanDescendantTheta{} \sim U(0.0001, 0.1)$ or
$\meanDescendantTheta{} \sim U(0.0005, 0.04)$.
As described in \citet{Oaks2012}, the divergence-time parameters in the model
implemented in \msb are in generations scaled relative to a constant
reference-population size, \myTheta{C}.
This reference-population size is defined in terms of the upper limit of the
uniform prior on the mean size of the descendant populations,
\meanDescendantTheta{}, such that for the prior $\meanDescendantTheta{} \sim
U(\uniformMin{\meanDescendantTheta{}},\uniformMax{\meanDescendantTheta{}})$,
the size of the constant reference population is $\myTheta{C} =
\uniformMax{\meanDescendantTheta{}}/2$.
Thus, the model used by \citet{Hickerson2013} mixes two different units of
time.
In other words, some of their prior and posterior samples are in units of
$0.05/\mutationRate$ generations, whereas others are in units of
$0.02/\mutationRate$ generations.

The fact that their posterior samples are in different units makes the results
of \citet{Hickerson2013} difficult to interpret, and renders their
regression-adjusted results invalid.
A fundamental assumption of regression is that all of the values of the
response variable are in the same units.
Thus, the results in sections ``Using ABC Model Comparison to Weight
Alternative Priors for the Philippine Vertebrate Data'' and ``Improved Sampling
Efficiency by Prior Weighting Supports Asynchronous and Recent Divergence for
the Philippines Vertebrate Data'' and presented in Figure 2 of
\citet{Hickerson2013} should be disregarded.
The error is easily illustrated by re-plotting their results with the different
time units indicated (Figure~S\ref{figJointPosteriorHickerson}).

\section{Theoretical implications of empirical priors for Bayesian model
choice---A simple example}
\begin{linenomath}
The distinctions between Bayesian parameter estimation and model choice
discussed in the main text can be illustrated with a simple example.
Let us say we are interested in the fairness of a particular coin, and we
denote the unknown probability of it landing heads as \myTheta{}.
More specifically, we are interested in the probability of two models, $M_1$
and $M_2$.
In both models the outcomes of flipping the coin are assumed to be binomially
distributed, but under $M_1$ the coin is weighted toward landing heads (i.e.,
$\myTheta{} > 0.5)$), whereas under $M_2$, the coin is weighted toward landing
tails (i.e., $\myTheta{} < 0.5$).
We already have data from flipping a different coin 20 times that landed both
heads and tails 10 times each, and so we decide to use these data in specifying
a beta prior on fairness of the new coin of $beta(a=10, b=10)$
(Figure~S\ref{figCoinFlip}).
We collect data by flipping the coin of interest $N=10$ times, $y=3$ of which
land heads.
Given the beta distribution is a conjugate prior for a binomial likelihood, the
posterior distribution has the nice analytical form $\theta \given y,N \sim
beta(a + y, b + N - y)$, which for the new dataset is simply $beta(13, 17)$
(Figure~S\ref{figCoinFlip}).
The maximum a posteriori (MAP) estimate of the probability of heads is 0.429,
and following Equation~\ref{eq:marginallikelhiood} in the main text the
marginal likelihoods of our models of interest are
\begin{equation}
    p(y=3, N=10 \given M_1) = \int_{0.5}^{1} p(y=3, N=10 \given
    \myTheta{}, M_1)p(\myTheta{} \given M_1) d\myTheta{} \approx 0.029,
\end{equation}
and
\begin{equation}
    p(y=3, N=10 \given M_2) = \int_{0}^{0.5} p(y=3, N=10 \given
    \myTheta{}, M_2)p(\myTheta{} \given M_2) d\myTheta{} \approx 0.097.
\end{equation}
Given the models have equal probability under our prior, we can calculate the
posterior probability of Model 1 as
\begin{equation}
    p(M_1 \given y=3, N=10) = \frac{p(y=3, N=10 \given M_1)}{p(y=3, N=10 \given
    M_1) + p(y=3, N=10 \given M_2)} \approx 0.23.
\end{equation}
This is the correct posterior probability of Model 1 given our prior
and data.
\end{linenomath}

To give the data more weight relative to the prior, we could use it twice, and
calculate an empirical Bayes estimate using a prior of $beta(13,17)$.  This
results in a ``posterior'' distribution of $beta(16, 24)$
(Figure~S\ref{figCoinFlip}), with a MAP estimate of 0.395, and $p(M_1 \given
y=3, N=10) = 0.10$.
The estimated posterior distribution of the parameter, and resulting MAP
estimate, is similar whether or not an empirically informed prior is used.
However, the posterior probability of Model 1 is very sensitive to the
empirical prior, decreasing by 56\%.
By using the empirically informed prior, we ignored prior uncertainty, leading
to an underestimate of our posterior uncertainty (Figure~S\ref{figCoinFlip}).
While this did not greatly affect our estimate of \myTheta{}, it misled us
to be overconfident in Model 2.
% Our mode estimate of \myTheta{} under the empirical prior might be closer
% to the true value than our posterior mode.
% Let us assume the truth of the matter is that the coin under study has a broad
% flat edge, and as a result lands on its edge at a certain frequency.
% Hence, both of our models are incorrect (i.e., the outcomes of flipping the
% coin are not binomially distributed).



% This can be demonstrated with a simple, albeit contrived, example.
% Let us say that principal investigator Mary and her new postdoc Will are
% interested in the hypothesis that the mass of George Washington's periwig
% renders the quarter dollar of the United States unfair.
% That is to say their null hypothesis is that the probability of a US
% quarter landing heads when tossed is less than 0.5 ($\theta < 0.5$).
% They can certainly evaluate this hypothesis, they can have undergraduate
% worker, Joe, flip the coin for them while they tabulate the results.
% But being Bayesians, before they call Joe into the lab, they agree on a
% prior probability to place on the set of all possible probabilities that
% the quarter will land heads when it is flipped.
% Given that neither of them have a quarter, and their prior knowledge that Joe
% moonlights as a magician, and is notorious for performing coin and card tricks
% in the lab, they suspect there is a good chance that the quarter Joe uses will
% be either two-headed or two-tailed.
% So, knowing that the beta distribution is the conjugate prior for a binomial
% likelihood, they decide to use a $beta(a=0.5, b=0.5)$ prior distribution
% (Figure~\ref{figCoinFlip}).

% Mary calls Joe into the lab, confirms that he has a quarter, and tells him to
% begin flipping it.
% After five tosses, four of which was heads, Joe decides that academics are
% crazy and leaves the lab to pursue a major in theatre.  Mary and Will, both
% being computational biologists, are satisfied with their
% empirical dataset of $y = 4$ heads out of $N = 5$ trials.
% They know from the conjugacy of the beta prior, that the posterior distribution
% has the nice analytical form $\theta|y,N \sim beta(a + y, b + N - y)$, which
% in this case is simply $beta(4.5, 1.5)$; this is the true posterior distribution
% of $\theta$ given their prior belief and data (Figure~\ref{figCoinFlip}).
% This allows them to plug these values into the beta cumulative distribution
% function to determine that the posterior probability of their hypothesis is
% $p(\theta < 0.5 | y=1, N=5) = 0.088$.
% Given their prior belief and dataset, this indeed is the correct posterior
% probability of the hypothesis, and Mary and Will should now update their
% posterior belief accordingly.

% However, reflecting upon the results of their experiment
% (Figure~\ref{figCoinFlip}), Mary and Will regret their choice of prior.
% Their prior looks very ``poorly selected,'' and if they had only known that the
% coin was fair before the data were collected, they would have selected a much
% better prior.
% Clearly, from their results, they should have used a prior centered
% around 0.2; their data suggest a prior of $beta(4.5, 1.5)$ would have been
% much ``better.''
% Rather than resort to flipping the coin five more times, Mary and Will decide
% to redo their analysis using the much better ``prior'' of $p(\theta) \sim
% beta(4.5, 1.5)$.
% This gives a ``posterior'' of $\theta|y,N \sim beta(8.5, 2.5)$, and a
% probability of their hypothesis of $p(\theta < 0.5 | y=1, N=5) = 0.026$.
% Now convinced that the US quarter is unfair, albeit not due to the mass of
% President Washington's head as they hypothesized, Will begins composing an
% e-mail of complaint addressed to the U.S.\ Mint.

% If Joe's flips were a representative sample, Mary and Will's empirical Bayes
% estimate might very well be a better point estimate for the parameter
% \myTheta{}.
% However, their empirical Bayes estimate of the probability of their hypothesis
% is incorrect and biased.
% This simple example shows how parameter estimation is fundamentally different
% from estimating the probability of a model.
% While empirically informed priors can be used to obtain well-behaved parameter
% estimators, using them for model choice is much less certain.


%CWL: This seems to specific, not sure about including
\section{Validation analyses}
% \highLight{Not sure where to put this section}.
% \highLight{This justifies presenting the unadjusted results in this paper.}
% \highLight{Is it worth including just for this purpose?}
Following \citet{Oaks2012}, we characterize the model-choice behavior of the
model-averaging approach of \citet{Hickerson2013} under the ideal conditions
where the prior is correct (i.e., the data are generated from parameters drawn
from the same prior distributions used in the analysis).
We used the same prior models as above ($M_1$--$M_5$;
Table~\ref{tabModelChoiceEmpirical}), and simulated 50,000 datasets under this
prior (10,000 from each model).
We used a simulated data structure of eight population pairs, with a single
1000 base-pair locus sampled from 10 individuals from each population.
We then analyzed each of these replicate datasets using the same prior with 2.5
million samples (500,000 from each of the five prior models), retaining 1000
posterior samples.
Our results are very similar to \citet{Oaks2012}, but we note that they
are not directly comparable as our simulations contained eight population
pairs rather than 10 (Figure~\ref{figValidationMCBehavior}).
We find that the approach of \citet{Hickerson2013} estimates the posterior
probability of divergence models reasonably well when all assumptions of the
method are met (i.e., the prior is correct) and the unadjusted posterior
estimates are used.
Similar to \cite{Oaks2012}, we find that the regression-adjusted estimates of
the model probabilities are biased.

\section{Additional clarifications from \citet{Hickerson2013}}

\subsection{Saturation of summary statistics}
\citet{Hickerson2013} claim the priors used by \citet{Oaks2012} ``cause much of
the explored parameter space to be beyond the threshold of saturation in most
mtDNA genes.'' To explore this possibility, we simulated datasets under prior
settings that match two of the three priors used by \citet{Oaks2012}:
$\meanDescendantTheta{} \sim U(0.0005, 0.04)$ and $\ancestralTheta{} \sim
U(0.0005, 0.02)$.
Under this prior, we randomly sample divergence-time parameters from a uniform
distribution of $U(0, 20)$ coalescent units, simulate datasets, and plot the
\divt{} values against the summary statistics calculated from the resulting
datasets (Figure~\ref{figSaturationPlot}).
Clearly, the priors used by \citet{Oaks2012} with upper limits on \divt{} of five
and 10 coalescent units suffered little to no effect from saturation.
Even at divergence times of 20 coalescent units, there is still signal in the
summary statistics used by \msb (Figure~\ref{figSaturationPlot}).
Thus, the assertion of \citet{Hickerson2013} does not apply to at least
two of the priors used by \citet{Oaks2012} and, as a result, does not
explain the bias they found.

\subsection{Graphical prior comparisons}
\citet{Hickerson2013} advocate the use graphical checks of prior models.
This prior-predictive approach entails generating a small number (1000) of
random samples from the prior and plotting the resulting summary statistics in
comparison to the observed statistics to see if they coincide (see Figure 1 of
\citet{Hickerson2013}).
% As we show above, this strategy can be misleading, because the resulting plots
% of this approach have little correlation with the appropriateness of priors.
Given the richness of the \msb model ($\approx 600$ parameters for the Philippine
dataset analyzed by \citet{Hickerson2013}), we do not expect that 1000
\emph{random} draws from the vast prior parameter space will yield data and
summary statistics consistent with the observed data.
In fact, when such random draws are tightly clustered around the observed
statistics, this can be an indication that the prior is over-fit, as we show in
the main text (Table~\ref{tabModelChoiceEmpirical} and Figure~S\ref{figPCA}).
Thus, using such plots to select priors should be avoided, and the use of
posterior-predictive analyses would be much more informative about the overall
fit of models.

\subsection{Differing utilities of \numt{} and \vmratio{} in \texttt{msBayes}}
The primary component of the \msb model is the vector of divergence
times for each of the taxon pairs,
$\divtvector = \{\divt{1}, \ldots, \divt{Y}\}$
\citep{Oaks2012}.
\citet{Hickerson2013} argue that the dispersion index of this vector,
\vmratio{}, is a better model-choice estimator than the number of 
divergence-time parameters within the vector,
\numt{}.
They present a plot of \numt{} against \vmratio{} (Fig.~S1 of
\citet{Hickerson2013}), which is essentially a plot of sample size versus
variance.
This plot shows, not surprisingly, that \vmratio{} has very little information
about the number of divergences among taxa.
Nonetheless, \citet{Hickerson2013} conclude \vmratio{} is more informative and
biogeographically relevant than \numt{}.
% We struggle to follow this logic.
However, all of the information about the temporal distribution of divergences
is contained within the divergence-time vector that \vmratio{} is summarizing.
Clearly, the number of divergence-time parameters within the vector and their
values is more informative than the variance (i.e., the dispersion index is not
a sufficient statistic for \divtvector).
\citet{Hickerson2013} also argue that \msb can estimate \vmratio{} much better
than \numt{}.
However, \citet{Oaks2012} demonstrate that even when all assumptions of the
model are met, \vmratio{} is a poor model-choice estimator (see plots B, D \& F
of Figure 4 in \citet{Oaks2012}), whereas \numt{} performs better.

Importantly, \vmratio{} is limited to estimating the probability of only a
single model (the one-divergence model), and thus its utility for model-choice
is very limited.
I.e., it can only be informative about the probability of whether there is one
divergence shared among the taxa ($\vmratio{} = 0.0$) or there is greater than
one divergence ($\vmratio{} > 0.0$).
As a result, not only is its model-choice utility limited, but it is also
very difficult to estimate.
\vmratio{} can range from zero to infinity, and the point density that it is
at its lower limit of zero will always be zero.
Thus, an arbitrary threshold (0.01 is used throughout the \msb literature) must
be chosen to make the probability of ``simultaneous'' divergence estimable.
Even with this arbitrary threshold, it is still not surprising to see that it
is numerically difficult to obtain reliable estimates of the probability that
\vmratio{} is ``near'' its lower limit of zero.
It is easier, less subjective, and more interpretable to estimate the
probability of the discrete parameter of the model, \numt{}, is at its lower
limit of one.
Thus, it is not surprising that \citet{Oaks2012} find that \numt{} is a better
estimator of model probability than \vmratio{}.

