%CWL:
%It may be important to point out the error made in their paper, but it can
%also be nit-picky, it depends on how the error impacts the overall message of
%our response.  We should strive to keep the response short and directed at the
%big picture items. If there is a way to still point out the error, then maybe
%some of this section should be used
\section*{An error in Hickerson et al.'s re-analysis of the Philippines data}
\citet{Hickerson2013} re-analyzed the dataset of \citet{Oaks2012} using a
model-averaging approach, where they placed a discrete uniform prior over eight
different prior models (see Table 1 of \citet{Hickerson2013}).
However, there is a fundamental error in the methodology of
\citet{Hickerson2013}: their model mixes different time units.

Each of the eight prior models used in the re-analysis by \citet{Hickerson2013}
has one of two priors on the mean size of the descendant populations of each
taxon pair:
$\meanDescendantTheta{} \sim U(0.0001, 0.1)$ or
$\meanDescendantTheta{} \sim U(0.0005, 0.04)$.
As described in \citet{Oaks2012}, the divergence time parameters of the model
implemented in \msb are scaled relative to a constant reference population
size, \myTheta{C}.
This reference population size is defined in terms of the upper limit of the
uniform prior on the mean size of the descendant populations,
\meanDescendantTheta{}, such that for the prior $\meanDescendantTheta{} \sim
U(\uniformMin{\meanDescendantTheta{}},\uniformMax{\meanDescendantTheta{}})$,
$\myTheta{C} = \uniformMax{\meanDescendantTheta{}}/2$.
Thus, the model used by \citet{Hickerson2013} mixes two different units of
time.
In other words, some of their prior and posterior samples are in units of
$0.05/\mutationRate$ generations, whereas others are in units of
$0.02/\mutationRate$ generations.

The fact that their posterior samples are in different units makes the results
of \citet{Hickerson2013} difficult to interpret, and renders their
regression-adjusted results invalid.
A fundamental assumption of regression is that all of the values of the
response variable are in the same units.
Thus, the results in sections ``Using ABC Model Comparison to Weight
Alternative Priors for the Philippine Vertebrate Data'' and ``Improved Sampling
Efficiency by Prior Weighting Supports Asynchronous and Recent Divergence for
the Philippines Vertebrate Data'' and presented in Figure 2 of
\citet{Hickerson2013} should be disregarded.
The error is easily illustrated by re-plotting their results with the different
time units indicated (Figure~\ref{figJointPosteriorHickerson}).

%CWL: This seems to specific, not sure about including
\section*{Validation analyses}
% \highLight{Not sure where to put this section}.
% \highLight{This justifies presenting the unadjusted results in this paper.}
% \highLight{Is it worth including just for this purpose?}
Following \citet{Oaks2012}, we characterize the model-choice behavior of the
model-averaging approach of \citet{Hickerson2013} under the ideal conditions
where the prior is correct (i.e., the data are generated from parameters drawn
from the same prior distributions used in the analysis).
We used the same prior models as above ($M_1$--$M_5$;
Table~\ref{tabModelChoiceEmpirical}), and generated 50,000 pseudo-replicate
datasets under this prior (10,000 from each model).
We used a simulated data structure of eight population pairs, with a single
1000 base-pair locus sampled from 10 individuals from each population.
We then analyzed each of these replicate datasets using the same prior,
retaining 1000 posterior samples.
Our results are very similar to \citet{Oaks2012}, but we note that they
are not directly comparable as our simulations contained eight population
pairs rather than 10 (Figure~\ref{figValidationMCBehavior}).
We find that the approach of \citet{Hickerson2013} estimates the posterior
probability of divergence models reasonably well when all assumptions of the
method are met (i.e., the prior is correct) and the unadjusted posterior
estimates are used.
Similar to \cite{Oaks2012}, we find that the regression-adjusted estimates of
the model probabilities are biased.


\section*{Differing utilities of \numt{} and \vmratio{} in \msb}
The main parameter of the \msb model is the vector of divergence
times for each of the taxon pairs,
$\divtvector = \{\divt{1}, \ldots, \divt{Y}\}$
\citep{Oaks2012}.
\citet{Hickerson2013} argue that the dispersion index of this vector,
\vmratio{}, is a better model-choice estimator than the number of 
divergence time parameters within the vector,
\numt{}.
They present a plot of \numt{} against \vmratio{} (Fig.~S1 of
\citet{Hickerson2013}), which is simply a plot of sample size versus variance.
This plot shows, not surprisingly, that \vmratio{} has very little information
about the number of divergences among taxa.
Nonetheless, \citet{Hickerson2013} conclude \vmratio{} more informative and
biogeographically relevant than \numt{}.
% We struggle to follow this logic.
Certainly the maximum information is contained within the divergence time
vector that \vmratio{} is summarizing.
Clearly, the number of divergence time parameters within the vector and their
values is more informative than its variance (i.e., the dispersion index is not
a sufficient statistic for \divtvector).

\citet{Hickerson2013} also argue that ``\msb can estimate \vmratio{} much
better than \numt{}.''
However, \msb is a model-choice estimator, and hence the goal is to estimate the
posterior probability of divergence models.
\citet{Oaks2012} demonstrate that even when all assumptions of the model are
met, \vmratio is a poor model-choice estimator (see plots B, D \& F of Figure
4), whereas \numt{} performs better.
Furthermore, \vmratio{} is limited to estimating the probability of only a
single divergence model (the one divergence model), and thus its utility for
model choice is extremely limited.

The model-choice utility of \vmratio{} is limited to the probability that
this continuous statistic, which can range from zero to infinity, is at its lower
limit of zero. In theory, this point density will always be zero, thus an
arbitrary threshold (0.01 is used throughout the \msb literature) must 
be chosen to make the probability estimable.
However, it is still not surprising to see that it is numerically difficult to
obtain reliable estimates of the probability that the continuous \vmratio{}
statistic is ``near'' its limit of zero.
It is much easier, less subjective, and more interpretable to estimate
the probability of the discrete parameter of the model, \numt{}, is
at its lower limit of one.
Thus, it is not surprising that \citet{Oaks2012} find that \numt{} is a better
estimator of model probability than \vmratio{}.

\section*{Other clarifications}

\subsection*{Graphical prior comparisons}
\citet{Hickerson2013} advocate the use of what they call graphical checks of
prior models.
This entails generating a small number (1000) of random samples from the prior
and plotting the resulting summary statistics in comparison to the observed
statistics to see if they coincide (see Figure 1 of \citet{Hickerson2013}).
As we show above, this strategy can be misleading, because the resulting plots
of this approach have little correlation with the appropriateness of priors.
Given the richness of the \msb model ($\approx 600$ parameters for the Philippine
dataset analyzed by \citet{Hickerson2013}), we do not expect that 1000
\emph{random} draws from the vast prior parameter space will yield data and
summary statistics consistent with the observed data.
In fact, when such random draws are tightly clustered around the observed
statistics, this can be an indication that the prior is over-fit, as we show
above (Table~\ref{tabModelChoiceEmpirical} and Figure~\ref{figPCA}).
Thus, using such plots to select priors should be avoided, and the use of
posterior predictive analyses would be much more informative about the overall
fit of models.

\subsection*{Saturation of summary statistics}
\citet{Hickerson2013} claim the priors used by \citet{Oaks2012} ``cause much of
the explored parameter space to be beyond the threshold of saturation in most
mtDNA genes.'' To explore this possibility, we simulated datasets under prior
settings that match two of the three priors used by \citet{Oaks2012}:
$\meanDescendantTheta{} \sim U(0.0005, 0.04)$ and $\ancestralTheta{} \sim
U(0.0005, 0.02)$.
Under this prior, we draw divergence time parameters from a uniform
distribution of $U(0, 20)$, simulate datasets, and plot the \divt{} values
against the summary statistics calculated from the resulting datasets
(Figure~\ref{figSaturationPlot}).
Clearly, the priors used by \citet{Oaks2012} with upper limits on \divt{} of five
and 10 suffered from little to no effect from saturation.
Even at divergence times of 20 coalescent units, there is still signal in the
summary statistics used by \msb (Figure~\ref{figSaturationPlot}).
Thus, the assertion of \citet{Hickerson2013} does not apply to at least
two of the priors used by \citet{Oaks2012} and, as a result, does not
explain the bias they found.
