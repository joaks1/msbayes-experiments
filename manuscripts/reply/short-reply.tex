
%&<latex>
\documentclass[letterpaper,12pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% preamble %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{../utils/preamble.tex}
\input{../utils/macros.tex}

\newcommand{\msTitle}{Nuh uh: A reply to Hickerson et al.
\xspace}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\doublespacing
\raggedright
\setlength{\parindent}{0.5in}
\begin{linenumbers}

\begin{titlepage}
    \begin{flushleft}
        \sffamily

        \MakeUppercase{\large\bfseries \msTitle}

        \vspace{12pt}
        \textbf{Running head:} \MakeUppercase{Approximate Bayesian model
        choice}

        \vspace{12pt}
        Jamie R.\ Oaks$^{1,2}$, Jeet Sukumaran$^{3}$, Jacob A.\
        Esselstyn$^{4}$, Charles W.\ Linkem$^{5}$, Cameron D.\
        Siler$^{6}$, and Rafe M.\ Brown$^{1}$

        \bigskip
        $^1$\emph{Department of Ecology and Evolutionary Biology,
            Biodiversity Institute,
            University of Kansas,
            Lawrence, Kansas 66045}\\[.1in]
        $^3$\emph{Department of Biology,
            Duke University,
            Durham, North Carolina 27708} \\[.1in]
        $^4$\emph{Museum of Natural Science,
            Louisiana State University,
            119 Foster Hall,
            Baton Rouge, Louisiana 70803}\\[.1in]
        $^5$\emph{Department of Biology,
            University of Washington,
            Seattle, Washington 98195}\\[.1in]
        $^6$\emph{Sam Noble Museum,
            Department of Biology,
            University of Oklahoma,
            Norman, Oklahoma 73072}\\[.1in]
        $^2$\emph{Corresponding author} (\href{mailto:joaks1@gmail.com}{\tt
        joaks1@gmail.com})\\

    \end{flushleft}
\end{titlepage}

{\sffamily
    \noindent\textbf{ABSTRACT} \\
    \noindent Biogeographers frequently seek to explain population and species
    differentiation on geographical phenomena.
    Establishing that a set of population splitting events occurred
    at the same time can be a potentially persuasive argument that a set of taxa
    are affected by the same geographic events.
    Unfortunately, estimating divergence times precisely when one lacks
    precise information about the rate of molecular evolution is a notoriously
    difficult problem in evolutionary biology.
    \citep{Huang2011} introduced an approximate Bayesian approach which can be
    used to estimate the probabilities of models in which multiple sets of taxa
    diverge at the same time. 
    This method (implemented in the software \msb) does not require a common
    set of character data that links all of the taxa under investigation, but
    does require the relative mutation rates among the taxa and loci to be
    assumed \emph{a priori}.
    Recently, \citet{Oaks2012} used this model-choice framework to 
    study 22 pairs of vertebrate lineages which are distributed across
    the Philippines; they also studied the behavior of the \msb approach
    using computer simulations.
    \citet{Oaks2012} found that the model was very sensitive to the 
    prior that is chosen and that the inference method had low power
    to detect variation in divergences times.
    These results were not surprising, in light of a rich statistical literature
    that shows that the marginal likelihood of a model is quite sensitive to the
    use of vague priors.
    Because this sensitivity to prior assumptions affects the crucial insights 
    that a researcher who employs \msb seeks to gain, \citet{Oaks2012} recommended
    that users of the approach should carefully assess the robustness of their 
    conclusions to different priors.
    According to \citet{Hickerson2013}, the lack of robustness in \msb analyses
    was due to excessively broad priors on divergence times leading to 
    inadequate numbers of simulation replicates.
    They proposed a new model-averaging approach that uses narrow, empirically informed uniform
    priors.
    Here we demonstrate that the approach of \citet{Hickerson2013} is
    dangerous in the sense that the empirically-derived priors often
    exclude from consideration the true values of the models' parameters.
    On a more fundamental level, we question the value of adopting an 
    empirical Bayesian stance for this model-choice problem.
    The robust Bayesian approach of conducting analyses under a variety
    of priors can reveal prior sensitivity and communicate which assumptions
    underlie model inference.
    Furthermore, simulations provide insight into the temporal resolution of
    the method, which in turn helps guide appropriate interpretation of
    results.

    \vspace{12pt}
    \noindent\textbf{KEY WORDS: Approximate Bayesian computation; Bayesian
        model choice; empirical Bayes} 
}

\newpage
\msb
\citep{Huang2011} uses approximate Bayesian computation (ABC) to
estimate the distribution of divergence times among co-distributed pairs of
taxa.
It approximates a posterior probability over models that range from a single
divergence-time parameter (i.e., simultaneous divergence of all pairs of taxa)
to the fully generalized model in which each pair of taxa diverged at a
unique time.
A full description of the model is given in \citet{Oaks2012}.
Frequently, the exact values of the divergence times are hard 
to determine because researchers typically lack precise knowledge 
of the rate at which substitutions occur.
So the exact divergence time is often viewed as a nuisance parameter, and
researchers focus on the number of divergence events.
The papers of \citet{Oaks2012} and \citet{Hickerson2013} are in agreement
on the fundamental methodological point about the model selection performed in \msb:
\begin{itemize}
   \item The use of vague priors on the divergence time parameters can lead to
       support models with few divergence events shared across taxa. Thus, the
       primary inference enabled by the approach is very sensitive to the
       priors on divergence times.
\end{itemize}

This is not surprising given the rich statistical literature that shows that
model marginal likelihoods are quite sensitive to the priors used in Bayesian
model selection
\citep[e.g.,][]{Jeffreys1935,Lindley1957}.
Accordingly, \citet{Oaks2012} suggest the primary cause of the spurious support
for models with few divergence parameters is the greater marginal likelihoods
of these models under vague uniform priors, relative to more parameter-rich
models.
Models with more divergence-time parameters integrate over \emph{much} greater
parameter space with low probability of producing the data, yet relatively high
prior density imposed by the uniformly distributed prior on divergence times.
Note, this is a statistical issue, extrinsic to \msb, and the numerical
approximation machinery of \msb could be sound.

\citet{Hickerson2013} suggest the bias is the result of a numerical issue
intrinsic to \msb, concluding the method's rejection algorithm is inefficient
and will be biased toward models with fewer divergence-time parameters, because
the parameter space of these models are more densely sampled relative to models
with more divergence-time parameters.
We agree that sampling error is present in all numerical Bayesian estimation
methods, however, we show here that it is not a major contributer to the
biases found by \citet{Oaks2012}.
We present results of additional analyses that, when combined with the results
of \citet{Oaks2012}, strongly suggest that \msb prefers models with few
divergence-time parameters, because they have greater likelihoods when
averaged over broad uniform priors on divergences times.

% We show that this argument is based on empirical Bayesian reasoning and
% questionable assumptions, and does not explain the bias of the method found in
% many of the analyses of \citet{Oaks2012}. 

% There are also differences between the papers regarding potential solutions to
% the bias.
\citet{Oaks2012} suggest alternative prior probability distributions on
divergence-times and other nuisance parameters could increase the marginal
likelihoods of models with more divergence-time parameters and thus reduce
spurious support for models of temporally clustered divergences.
Alternatively, \citet{Hickerson2013} present an approach that uses empirically
informed uniform priors and Bayesian model-averaging in an attempt to accommodate
uncertainty in selecting priors.
% \citet{Hickerson2013} champion this strategy as a means of avoiding the
% pitfalls raised by \citet{Oaks2012}.
In general, we agree with the use of Bayesian model averaging to obtain a
posterior estimate marginalized over uncertainty in prior choice.
However, we question whether adding an additional dimension of model choice
that samples over empirically informed uniform priors is a fruitful solution
for a model-choice method that is highly sensitive to priors.

In this paper, we discuss the potential theoretical and practical
considerations of using empirically informed priors for Bayesian model choice.
Furthermore, we evaluate the empirical Bayesian model-averaging approach of
\citet{Hickerson2013} as a potential solution to the biases of \msb revealed by
\citet{Oaks2012}.
In their re-analysis of the dataset of \citet{Oaks2012}, \citet{Hickerson2013}
made an error by mixing different units of time, which makes the results
presented in their response difficult to interpret (see Supporting Information
for details).
Even when avoiding this error, we find their empirical Bayesian model-averaging
implementation of \msb remains biased toward clustered divergence models.
Furthermore, the approach provides another means by which the method can
``escape'' models with large parameter space, which manifests in the preference
of models that exclude from consideration the true values of the model's
parameters.
Our results, when combined with those of \citet{Oaks2012}, suggest that it is
difficult to choose a uniformly distributed prior on divergence times that is
broad enough to confidently contain the true values of parameters while being
narrow enough to avoid spurious support of models with less parameter space.
However, recent work shows more flexible probability distributions without a
hard upper bound (e.g., gamma) can accommodate prior uncertainty in divergence
times without inhibiting the marginal likelihoods of models with more
divergence-time parameters, and as a result, increase the method's power to
detect temporal variation in divergences
\citet{Oaks2014dpp}.


\section*{The potential implications of empirical Bayesian model choice}
\citet{Hickerson2013} repeatedly refer to the prior distributions used by
\citet{Oaks2012} for divergence-time parameters as ``poorly selected.''
However, this is misleading, as \citet{Oaks2012} discuss in detail how they
selected their prior to reflect the large amount of uncertainty about the
timing of divergences across all 22 of the taxa in their study (see section
``Specifying and simulating the joint prior'' of \citet{Oaks2012}).
\citet{Oaks2012} further discuss how the use of uniform prior distributions in
\msb for many of the model's parameters requires investigators to select broad
priors to avoid excluding the truth \textit{a priori}, resulting in too much
density in improbable regions of parameter space.

\citet{Hickerson2013} suggest that \citet{Oaks2012} should have used their
data to inform their prior on divergence times (i.e., an empirical Bayesian
approach).
However, \citet{Oaks2012} did use empirically informed priors to mimic
empirical situations in which a large amount of prior information is available;
they did so to assess the prior sensitivity of the method and to determine if
uniform priors in such situations are narrow enough to avoid spurious support
of models with few divergence-time parameters.
The results of \citet{Oaks2012} and \citet{Hickerson2013} show the method is
highly sensitive to the prior on divergence times.
Furthermore, \citet{Oaks2012} found the method remained biased toward clustered
divergences under the informed priors.

The main argument of \citet{Hickerson2013} is that the priors used by
\citet{Oaks2012} were not informative enough.
They suggest a very narrow, highly informed uniform prior on divergence times
is necessary to avoid the method's preference for models with few
divergence-time parameters.
Such an empirical Bayesian approach to model selection raises some theoretical
and practical concerns, some of which were discussed by \citet{Oaks2012} (see
the last paragraph of ``Assessing prior sensitivity of \msb'' in
\citet{Oaks2012}); we expand on this here.

\subsection*{Theoretical implications of empirical priors for Bayesian model
choice}
\begin{linenomath}
Bayesian inference is a method of inductive learning in which Bayes' rule is
used to update our beliefs about a model $M$ as new information becomes
available.
If we let \allParameterValues represent the set of all possible parameter
values for model $M$, we can define a prior distribution for all $\theta \in
\allParameterValues$ such that $p(\theta \given M)$ describes our belief that
any given \myTheta{} is the true value of the parameter.
If we let \allDatasets represent all possible datasets then we can 
define a sampling model for all $\theta \in
\allParameterValues$ and $\alignment{}{} \in \allDatasets$ such that
$p(\alignment{}{} | \theta, M)$ measures our belief that any dataset \alignment{}{}
will be generated by any state \myTheta{} of model $M$.
After collecting a new dataset \alignment{i}{}, we can use Bayes' rule to
calculate the posterior distribution
\begin{equation}
    p(\myTheta{} \given \alignment{i}{}, M) = \frac{p(\alignment{i}{} \given
    \myTheta{}, M)p(\myTheta{} \given M)}{p(\alignment{i}{} \given M)},
    \label{eq:bayesrule}
\end{equation}
as a measure of our beliefs after seeing the new information, where
\begin{equation}
    p(\alignment{i}{} \given M) = \int_{\myTheta{}} p(\alignment{i}{} \given
    \myTheta{}, M)p(\myTheta{} \given M) d\myTheta{}
    \label{eq:marginallikelhiood}
\end{equation}
is the marginal likelihood of the model.
\end{linenomath}

This is an elegant method of updating our beliefs as data are accumulated.
However, this all hinges on the fact that the prior ($p(\myTheta{} \given M)$)
is defined for all possible parameter values independently of the new data
being analyzed.
Any other datasets or external information can safely be used to inform our
beliefs about $p(\myTheta{} \given M)$.
However, if the same data are used to both inform the prior and calculate the
posterior, the prior becomes conditional on the data, and Bayes' rule breaks
down.

Thus, empirical Bayes methods have an uncertain theoretical basis and
do not yield a valid posterior distribution from Bayes' rule \citep[e.g.,
empirical Bayesian estimates of the posterior are often too narrow, off-center,
and incorrectly shaped;][]{Morris1983,Laird1987,Carlin1990,Efron2013}.
This is not to say that empirical Bayesian approaches are not useful.
Empirical Bayes is a well-studied branch of Bayesian statistics that has given
rise to many inference methods that provide means of parameter estimation that
often exhibit favorable frequentist properties.
Furthermore, many post-hoc correction methods have been developed for
estimating confidence-intervals from empirical Bayes estimates of
distributions that often exhibit well-behaved frequentist coverage
probabilities
\citep{Morris1983,Laird1987,Laird1989, Carlin1990,Hwang2009}.

Whereas empirical Bayes approaches can provide powerful methods for parameter
estimation, a theoretical justification for empirical Bayes approaches to model
choice is questionable.
In Bayesian model choice, the primary goal is not to estimate parameters, but
to estimate the relative probabilities of candidate models.
In a simple example where two candidate models, $M_1$ and $M_2$, are being
compared, the goal is to estimate the posterior probabilities of these two
models.
Again, we can use Bayes' rule to calculate this as
\begin{equation}
    p(M_1 \given \alignment{i}{}) = \frac{p(\alignment{i}{} \given
    M_1)p(M_1)}{p(\alignment{i}{} \given M_1)p(M_1) + p(\alignment{i}{} \given
    M_2)p(M_2)}.
    \label{eq:bayesmodelchoice}
\end{equation}
By comparing Equations \ref{eq:bayesrule} and \ref{eq:bayesmodelchoice}, we
see fundamental differences between Bayesian parameter estimation and
model choice.
In Equation \ref{eq:bayesrule}, we see that the posterior density of any
state $\myTheta{}$ of the model, is the prior density updated by the
probability of the data given the state $\myTheta{}$ (the likelihood of
$\myTheta{}$).
The integral over the entire parameter space of the model, which is defined by
the priors, only appears as a normalizing constant in the denominator.
Thus, as long as the prior distribution contains the values of $\myTheta{}$
that maximize the probability of the data (i.e., the maximum likelihood) and
the data are strongly informative relative to the prior, the values of the
parameters that maximize the posterior distribution will be relatively robust
to prior choice, even if the posterior distribution is technically incorrect
due to using the data to inform the priors.
This is why empirical Bayes can work well for estimating the values of model
parameters.

% Unlike parameter estimates, posterior probabilities represent a summary of the
% entire posterior distribution.
% Thus, given that empirical Bayesian posterior distributions are not accurate,
% there is no guarantee regarding the accuracy of probabilities summarized from
% them.

However, if we look at Equation~\ref{eq:bayesmodelchoice}, we see that in
Bayesian model choice it is now the \emph{marginal} likelihood of a model that
updates the prior to yield the model's posterior probability.
Thus the integral over the entire parameter space of the likelihood weighted by
the prior probability density is no longer a normalizing constant, but rather
is what informs the posterior probability of that model from the data.
As a result, Bayesian model choice tends to be much more sensitive to priors
than parameter estimation.

Another important difference of Bayesian model choice illustrated by
Equation~\ref{eq:bayesmodelchoice} is that the value of interest, the
posterior probability of a model, is not a function of \myTheta{}, because
it is integrated out of the marginal likelihoods of the candidate models.
Thus, unlike parameter estimates, the estimated posterior probability of a
model is a single value (rather than a distribution) lacking a measure of
posterior uncertainty.
Also, unlike model parameters, the posterior probabilities of candidate
models have no clear true values.
% In some simple situations we might suspect that one model is correct and the
% rest are wrong, but in most applications, we know that all the models under
% consideration are oversimplifications of reality.
Model posterior probabilities are inherently measures of our belief in the
models after our prior beliefs are updated by the data being analyzed.

This complicates the meaning of model posterior probabilities when Bayes' rule
is violated by informing priors with the same data to be analyzed.
For empirical Bayesian parameter estimation, the objective is that by giving the
data more weight relative to the prior, the posterior distribution will be
peaked near the true value(s) of the model's parameter(s).
There is no such justification for empirical Bayesian model choice, because
there are no true values for the model probabilities being estimated.
By using the data twice, we fail to account for prior uncertainty and mislead
our posterior beliefs in the models being compared; we will be over confident
in some models and under confident in others.

These distinctions between Bayesian parameter estimation and model choice can
be illustrated with a simple example.
Let us say we are interested in the fairness of a particular coin, and we
denote the unknown probability of it landing heads as \myTheta{}.
More specifically, we are interested in the probability of two models, $M_1$
and $M_2$.
In both models the outcomes of flipping the coin are assumed to be binomially
distributed, but under $M_1$ the coin is weighted toward landing heads (i.e.,
$\myTheta{} > 0.5)$), whereas under $M_2$, the coin is weighted toward landing
tails (i.e., $\myTheta{} < 0.5$).
We already have data from flipping a different coin 20 times that landed both
heads and tails 10 times, and so we decide to use these data in specifying a
beta prior on fairness of the new coin of $beta(a=10, b=10)$
(Figure~\ref{figCoinFlip}).
We collect data by flipping the coin $N=10$ times, $y=3$ of which land heads.
Given the beta distribution is a conjugate prior for a binomial likelihood, the
posterior distribution has the nice analytical form $\theta \given y,N \sim
beta(a + y, b + N - y)$, which for the new dataset is simply $beta(13, 17)$
(Figure~\ref{figCoinFlip}).
The maximum a posteriori (MAP) estimate of the probability of heads is 0.429,
and following Equation~\ref{eq:marginallikelhiood} the marginal likelihoods of
our models of interest are
\begin{equation}
    p(y=3, N=10 \given M_1) = \int_{0.5}^{1} p(y=3, N=10 \given
    \myTheta{}, M_1)p(\myTheta{} \given M_1) d\myTheta{} \approx 0.029,
\end{equation}
and
\begin{equation}
    p(y=3, N=10 \given M_2) = \int_{0}^{0.5} p(y=3, N=10 \given
    \myTheta{}, M_2)p(\myTheta{} \given M_2) d\myTheta{} \approx 0.097.
\end{equation}
Given the models have equal probability under our prior, we can calculate the
posterior probability of Model 1 as
\begin{equation}
    p(M_1 \given y=3, N=10) = \frac{p(y=3, N=10 \given M_1)}{p(y=3, N=10 \given
    M_1) + p(y=3, N=10 \given M_2)} \approx 0.23.
\end{equation}
This is the correct posterior probability of Model 1 given our prior
and data.

To give the data more weight relative to the prior, we could use it twice, and
calculate an empirical Bayes estimate using a prior of $beta(13,17)$.  This
results in a ``posterior'' distribution of $beta(16, 24)$
(Figure~\ref{figCoinFlip}), with a MAP estimate of 0.395, and $p(M_1 \given
y=3, N=10) = 0.10$.
The estimated posterior distribution of the parameter, and resulting MAP
estimate, is similar whether or not an empirically informed prior is used.
However, the posterior probability of Model 1 is very sensitive to the
empirical prior, decreasing by 56\%.
By using the empirically informed prior, we ignored prior uncertainty, leading
to an underestimate of our posterior uncertainty (Figure~\ref{figCoinFlip}).
While this did not greatly affect our estimate of \myTheta{}, it misled us
to be overconfident in Model 2.
% Our mode estimate of \myTheta{} under the empirical prior might be closer
% to the true value than our posterior mode.
% Let us assume the truth of the matter is that the coin under study has a broad
% flat edge, and as a result lands on its edge at a certain frequency.
% Hence, both of our models are incorrect (i.e., the outcomes of flipping the
% coin are not binomially distributed).



% This can be demonstrated with a simple, albeit contrived, example.
% Let us say that principal investigator Mary and her new postdoc Will are
% interested in the hypothesis that the mass of George Washington's periwig
% renders the quarter dollar of the United States unfair.
% That is to say their null hypothesis is that the probability of a US
% quarter landing heads when tossed is less than 0.5 ($\theta < 0.5$).
% They can certainly evaluate this hypothesis, they can have undergraduate
% worker, Joe, flip the coin for them while they tabulate the results.
% But being Bayesians, before they call Joe into the lab, they agree on a
% prior probability to place on the set of all possible probabilities that
% the quarter will land heads when it is flipped.
% Given that neither of them have a quarter, and their prior knowledge that Joe
% moonlights as a magician, and is notorious for performing coin and card tricks
% in the lab, they suspect there is a good chance that the quarter Joe uses will
% be either two-headed or two-tailed.
% So, knowing that the beta distribution is the conjugate prior for a binomial
% likelihood, they decide to use a $beta(a=0.5, b=0.5)$ prior distribution
% (Figure~\ref{figCoinFlip}).

% Mary calls Joe into the lab, confirms that he has a quarter, and tells him to
% begin flipping it.
% After five tosses, four of which was heads, Joe decides that academics are
% crazy and leaves the lab to pursue a major in theatre.  Mary and Will, both
% being computational biologists, are satisfied with their
% empirical dataset of $y = 4$ heads out of $N = 5$ trials.
% They know from the conjugacy of the beta prior, that the posterior distribution
% has the nice analytical form $\theta|y,N \sim beta(a + y, b + N - y)$, which
% in this case is simply $beta(4.5, 1.5)$; this is the true posterior distribution
% of $\theta$ given their prior belief and data (Figure~\ref{figCoinFlip}).
% This allows them to plug these values into the beta cumulative distribution
% function to determine that the posterior probability of their hypothesis is
% $p(\theta < 0.5 | y=1, N=5) = 0.088$.
% Given their prior belief and dataset, this indeed is the correct posterior
% probability of the hypothesis, and Mary and Will should now update their
% posterior belief accordingly.

% However, reflecting upon the results of their experiment
% (Figure~\ref{figCoinFlip}), Mary and Will regret their choice of prior.
% Their prior looks very ``poorly selected,'' and if they had only known that the
% coin was fair before the data were collected, they would have selected a much
% better prior.
% Clearly, from their results, they should have used a prior centered
% around 0.2; their data suggest a prior of $beta(4.5, 1.5)$ would have been
% much ``better.''
% Rather than resort to flipping the coin five more times, Mary and Will decide
% to redo their analysis using the much better ``prior'' of $p(\theta) \sim
% beta(4.5, 1.5)$.
% This gives a ``posterior'' of $\theta|y,N \sim beta(8.5, 2.5)$, and a
% probability of their hypothesis of $p(\theta < 0.5 | y=1, N=5) = 0.026$.
% Now convinced that the US quarter is unfair, albeit not due to the mass of
% President Washington's head as they hypothesized, Will begins composing an
% e-mail of complaint addressed to the U.S.\ Mint.

% If Joe's flips were a representative sample, Mary and Will's empirical Bayes
% estimate might very well be a better point estimate for the parameter
% \myTheta{}.
% However, their empirical Bayes estimate of the probability of their hypothesis
% is incorrect and biased.
% This simple example shows how parameter estimation is fundamentally different
% from estimating the probability of a model.
% While empirically informed priors can be used to obtain well-behaved parameter
% estimators, using them for model choice is much less certain.

\subsection*{Practical concerns about empirically informed uniform priors for
    Bayesian model choice}
% \cwlnote{}{Should this be part of the empirical Bayes section, or something
%     else?}

% \cwlnote{}{It seems like this section can be boiled down to ``empirical Bayes
%     approaches for model choice when using uniform priors can exclude the
%     $truth$ from the prior, which is a violation of Bayes theorem" or something
%     like that. Picking apart what they calculated and we calculated seems like
%     too much detail. I would try to simplify how this is being presented. This
%     can lead into how the use of more appropriate prior distributions should be
%     used.}
In addition to the theoretical concerns discussed above, there are practical
problems with using narrow, empirically informed, uniform priors for a method
that has already been shown to be biased toward models with less parameter
space.
The results of Hickerson et al.'s \citeyear{Hickerson2013} reanalysis of the
Philippine dataset strongly favored models with the narrowest, empirically
informed prior on divergence time, and thus their model-averaged posterior
estimates are dominated by models $M_1$ and $M_2$ (see Table 1 of
\citet{Hickerson2013}).
This is concerning, because the narrowest \divt{} prior used by
\citet{Hickerson2013} ($\divt{} \sim U(0,0.1)$) likely excludes the true
divergence times for at least some of the Philippine taxa, a major problem when
using uniform priors.
\citet{Hickerson2013} set this prior to match the 95\% highest posterior
density (HPD) interval for the mean divergence time estimated under one of the
priors used by \citet{Oaks2012} (see Tables 2 and 3 of \citet{Oaks2012}).
Given this interval estimate is for the \emph{mean} divergence time across all
22 taxa, it may be inappropriate to set this as the limit on the prior, because
some of the taxon pairs are expected to have diverged at times older than the
upper limit.
Furthermore, this prior is \emph{excluded} from the 95\% HPD interval estimates
of the mean divergence time under the other two priors explored by
\citet{Oaks2012} (under these priors the 95\% HPD is approximately 0.3--0.6;
see Table~6 of \citet{Oaks2012}).
The strong preference for the questionable priors on divergence times (their
$M_1$ and $M_2$) in the model-averaged results of \citet{Hickerson2013} suggest
their approach is biased toward models with less parameter space, and as a
consequence, is biased toward estimating model-averaged posteriors dominated by
models that exclude true values of parameters of the model.
We explored this possibility in two ways.
First, we re-analyzed the Philippines dataset using the model-averaging approach
of \citet{Hickerson2013}, but set one of prior models with a uniform prior on
divergence times that is unrealistically narrow, and almost certainly excludes
most, if not all, of the true divergence times of the 22 taxon pairs.
If small likelihoods of large models cause the method to prefer models with
less parameter space, we expect \msb will preferentially sample from this
incorrect model yielding a posterior that is incorrect (i.e., the
model-averaged posterior will be dominated by an incorrect model that excludes
the truth).
Second, we generated simulated datasets for which the divergence times are
drawn from an exponential distribution and applied the approach of
\citet{Hickerson2013} to each of them to see how often the method excludes the
truth.

\subsubsection*{Re-analyses of the Philippines dataset using empirical Bayesian
model averaging}

For our re-analyses of the Philippines dataset we followed the model-averaging
approach of \citet{Hickerson2013}, but with a reduced set of prior models to
avoid their error of mixing units of time (see SI for details).
We used five prior models, all of which had priors on population sizes of
$\meanDescendantTheta{} \sim U(0.0001, 0.1)$ and $\ancestralTheta{} \sim
U(0.0001, 0.05)$.
Following \citet{Hickerson2013}, each of these models had the following
priors on divergence time parameters:
$M_1$, $\divt{} \sim U(0, 0.1)$;
$M_2$, $\divt{} \sim U(0, 1)$;
$M_3$, $\divt{} \sim U(0, 5)$;
$M_4$, $\divt{} \sim U(0, 10)$; and
$M_5$, $\divt{} \sim U(0, 20)$.
We simulated $1\e6$ random samples from each of the models for a total of
$5\e6$ prior samples.
For each model, we retained the 10,000 samples with the smallest Euclidean
distance from the observed summary statistics, standardizing the statistics
using the prior means and standard deviations of the given model.
From the remaining 50,000 samples, we then retained the 10,000 samples with the
smallest Euclidean distance from the observed summary statistics, this time
standardizing the statistics using the prior means and standard deviations
across all five models.
We then repeated this analysis twice, replacing the $M_1$ model with
$M_{1A}$ and $M_{1B}$, which differ only by having priors on divergence
times of $\divt{} \sim U(0, 0.01)$ and $\divt{} \sim U(0, 0.001)$,
respectively.
While we suspect the prior of $\divt{} \sim U(0, 0.1)$ used by
\citet{Hickerson2013} likely excludes the true divergence times of at least
some of the 22 taxa, we are nearly certain that these narrower priors are
incorrect and exclude most, if not all, of the divergence times of the
Philippine taxa.

Our results show that the model-averaging approach of \citet{Hickerson2013}
does not reduce the method's support of models with less parameter space.
Rather, the method strongly prefers the prior model with the narrowest
distribution on divergence times across all three of our analyses, even when
this model is almost certainly incorrect and excludes the true divergence times
of the Philippine taxa (Table~\ref{tabModelChoiceEmpirical}).

However, \citet{Hickerson2013} vetted the priors used in their model-averaging
approach via ``graphical checks,'' in which the summary statistics from 1000
random samples of each prior model are plotted along the first two orthogonal
axes of a principle component analysis (see Figure 1 of \citet{Hickerson2013}).
To determine if such prior predictive analyses would indicate the $M_{1A}$ and
$M_{1B}$ models are problematic, we performed these graphical checks on our
prior models.
Unfortunately, these prior predictive checks provide no warning that these
priors are too narrow (Figure~S\ref{figPCA}).
Rather, the graphs suggest these incorrect priors are ``better fit''
(Figure~S\ref{figPCA}A--C) than the valid priors similar to those used by
\citet{Oaks2012} (Figure~S\ref{figPCA}D--F).

Given that the results of \citet{Hickerson2013} strongly prefer the models with
the narrowest prior on divergence times, it seems quite likely that their
model-averaged results are dominated by models that exclude at least some of
the 22 true divergence times, making their results difficult to interpret.


\subsubsection*{Simulation-based assessment of Hickerson et al.'s
\citeyear{Hickerson2013} model averaging over empirical priors}

To better quantify the propensity of Hickerson et al.'s
(\citeyear{Hickerson2013}) approach to exclude the truth, we simulated 1000
datasets in which the divergence times for the 22-population pairs are drawn
randomly from an exponential distribution with a mean of 0.5 ($\divt{} \sim
Exp(2)$).
All other parameters were identically distributed as the $M_1$--$M_5$ models
(Table~\ref{tabModelChoiceEmpirical}).
We then repeated the analysis described above using $1\e6$ random samples from
prior models $M_1$, $M_2$, $M_3$, $M_4$, and $M_5$, retaining 1000 posterior
samples for each of the 1000 simulated datasets.

For each simulation replicate, we estimated the Bayes factor in favor
of excluding the truth as the ratio of the posterior to prior odds of
excluding the true value of at least one parameter.
Whenever the Bayes factor preferred a model excluding the truth we counted the
number of the 22 true divergence times that were excluded by the preferred
model.
Our results show that the model-averaging approach of \citet{Hickerson2013}
favors a model that excludes the true values of parameters in 97\% of the
replicates (90\% with GLM-regression adjustment), excluding up to 21 of the 22
true divergence times (Figure~\ref{figExclusionSimTau}).
We also used the mode estimate of the preferred model for each replicate to
estimate the number of true parameter values excluded, which produced very
similar results.
Importantly, the posterior probability of excluding at least one true parameter
value is very high in nearly all of the replicates
(Figure~\ref{figExclusionSimProb}).
Using a Bayes factor of greater than 10 as a criterion for strong support, 66\%
of the replicates (87\% with GLM-regression adjustment) strongly support the
exclusion of true values (Figure~\ref{figExclusionSimProb}).

The results of the preceding empirical and simulation-based analyses clearly
demonstrate the risk of using narrow, empirically guided uniform priors in a
Bayesian model-averaging framework.
The consequence of this approach is obtaining a model-averaged posterior
estimate that is heavily weighted toward incorrect models that exclude true
values of model parameters.


\subsection*{Additional thoughts on empirical priors in Bayesian model choice}
The preceding sections are not a general critique of Bayesian model averaging.
Rather, model averaging can provide an elegant way of incorporating
model uncertainty in Bayesian inference.
However, when averaging over models with narrow and broad uniform priors in
situations where the likelihood density is not expected to be uniformly
distributed, the posterior could be dominated by models that exclude from
consideration the true values of parameters due to the larger marginal
likelihoods of the models that are integrated over less space with high prior
weight yet low likelihood.

Given the theoretical and practical issues with empirical Bayes approaches to
Bayesian model choice, it is clear why one should use caution before overly
criticizing an investigator's choice of priors after having seen the resulting
posterior.
%In a strictly Bayesian world, this is cheating.
As discussed by \citet{Oaks2012}, prior to analyzing the data, there was a
large amount of uncertainty regarding the divergence times of the 22 population
pairs under study.
Two of these pairs represent distinct species, and the taxonomy of many groups
in the Philippines has repeatedly been shown to mask deeply divergent lineages
\citep{RafeDiesmosAlcala2008,Linkem2010,Siler2010,Welton2010,Siler2011HerpMonographs,
Siler2011,Siler2012,RafeStuart2012,LinkemBrown2013,Rafe2013AREES,Siler2014kikuchii}.
% \citet{Oaks2012} also discuss how uniform priors on postive real-valued parameters make
% it difficult for investigators to express their prior uncertainty without
% putting high prior density in regions of improbable parameter space.
When limited to uniformly distributed priors, the alternative to priors that
reflect prior uncertainty, as shown above, is to risk excluding the true values
one seeks to estimate.
However continuous distributions perhaps more appropriate as priors for
positive real-valued parameters have been shown to greatly reduce spurious
support for clustered divergence models while allowing prior uncertainty to be
accommodated \citep{Oaks2014dpp}.






\section*{Assessing the power of the model-averaging approach of
    \citet{Hickerson2013}}
While our results above clearly demonstrate the risks inherent to the empirical
Bayesian model-choice approach used by \citet{Hickerson2013}, one could justify
such risk if the method does indeed increase the power of the method and thus
decrease bias toward clustered divergence models.
We assess this possibility using simulation-based analyses to evaluate the power
of Hickerson et al.'s (\citeyear{Hickerson2013}) approach.
Following \citet{Oaks2012}, we simulated 1000 datasets with \divt{} for each of
the 22 population pairs randomly drawn from a uniform distribution, $U(0,
\divt{max})$, where \divt{max} was set to: 0.2, 0.4, 0.6, 0.8, 1.0, and 2.0, in
\globalcoalunit generations.
All other parameters were identically distributed as the prior.
We use the same $5\e6$ samples from the same set of five prior models as above
($M_1$, $M_2$, $M_3$, $M_4$, and $M_5$).
As described above, for each simulated dataset, we approximated the posterior
by retaining 1000 samples from the prior with the smallest Euclidean distance
from the true summary statistics.
In total, we analyzed 6000 replicate datasets, retaining 1000 model-averaged
posterior samples for each of them.

We find that the approach of \citet{Hickerson2013} struggles to estimate the
variance of divergence times (\vmratio{}) across most of the \divt{max}
we simulated, whether evaluating the unadjusted
(Figure~\ref{figPowerAccOmegaMedian}) or GLM-adjusted
(Figure~\ref{figPowerAccOmegaModeGLM}) posterior estimates.
The method only estimates \vmratio{} relatively well when the simulated
distribution of divergence times is identical to one of the prior models
(Figures~\ref{figPowerAccOmegaMedian}E \& \ref{figPowerAccOmegaModeGLM}E).
This is consistent with the findings of \citet{Oaks2012}  and
\citet{Hickerson2013} that \msb is highly sensitive to the prior distribution
deviating from the true, underlying distribution of the data.

Furthermore, our results demonstrate that the approach of \citet{Hickerson2013}
consistently infers highly clustered divergences across all the \divt{max} we
simulated (Figure~\ref{figPowerNumExcluded}).
The method is most likely to infer the extreme case of a single divergence
event shared across 22 taxa when populations diverged randomly over the past
\globalcoalunit generations.
When divergences are random over the past $8\globalpopsize$ generations, the
most likely inference is only two divergence events, and a single
divergence is still estimated in more than 10\% of the replicates.
It is interesting to note that as \divt{max} increases, but before the
estimates are finally pulled away from $\numt{} = 1$, the distribution of
\numt{} estimates closely mirror the U-shaped prior on divergence models
used by \msb (see Figure~\ref{figPowerNumExcluded}E and Oaks et al.'s
(\citeyear{Oaks2012}) Figure 5B).
This suggests this U-shaped
prior coupled with the small marginal likelihoods of models with many
divergence parameters, is a major cause of the method's bias toward clustered
divergence models; this is consistent with \citet{Oaks2012} and confirmed by
\citet{Oaks2014dpp}.

Looking at our simulation results in terms of the posterior probability of the
dispersion index of divergence times supporting the extreme case of one
divergence event (i.e., $p(\vmratio{} < 0.01 \given \ssSpace)$), we find the
method strongly supports one divergence in greater than 27\% of the replicates
across all the \divt{max} we simulated (Figure~\ref{figPowerOmegaProb}).
Following \citet{Hickerson2013}, we use a Bayes factor of greater than 10 as
the criterion for incorrect inference of a single divergence event.
By this criterion, there is strong support for a single divergence event in
more than 90\% of the replicates when divergences are random over the past
$2.4\globalpopsize$ generations, and more than 60\% when over the past
$3.2\globalpopsize$ generations or less (Figure~\ref{figPowerOmegaProb}).

Our results show that the empirical Baysian model-averaging approach leads to
spuriously strong support for the extreme case of a single divergence event
when populations diverged randomly over the last $8\globalpopsize$ generations.
To put this on the scale roughly consistent with a vertebrate mitochondrial
locus, assuming a mutation rate of $2\e{-8}$ per site per generation, this
translates to 5 million generations.
Assuming a mutation rate consistent with nuclear loci of $1\e{-9}$, this is 100
million generations.

Also, the results of our power analyses further demonstrate the propensity of
Hickerson et al.'s (\citeyear{Hickerson2013}) approach to exclude true
parameter values.
Across all but one of the \divt{max} we simulated, in a large proportion of
replicates the method favors a model that excludes the truth, and across many
of the \divt{max} the preferred model will exclude a large proportion of the
true divergence-time values (Figure~\ref{figPowerNumExcluded}).
Importantly, the posterior probability of excluding at least on true divergence
value is also quite high across many of the \divt{max}
(Figure~\ref{figPowerProbExclusion}).
Only when the data are identically distributed as one of the prior models does
the method avoid excluding the truth more than 5\% of the time
(Figure~\ref{figPowerNumExcluded}E).
Again, this demonstrates the method's sensitivity to priors.

We find the model-averaging approach employing empirically informed uniform
priors lacks power to detect random variation in divergence times over a scale
of $8\globalpopsize$ generations or more, which roughly translates to millions
of generations for mitochondrial loci.
As a result, it is difficult to justify the risk of the approach to to exclude
regions of parameter space containing the truth.


%CWL: I would suggest removing this section
\section*{The use of power analysis to guide applications of \msb}
\citet{Hickerson2013} presented a power analysis of \msb under a narrow uniform
divergence-time prior of 0--1 coalescent units ago.
They found that under these prior conditions \msb can, assuming a
per-site rate of $1.92\e{-8}$ mutations per generation, detect multiple
divergence events among 18 taxa when the true divergences were random over
hundreds of thousands of generations or more.
\citet{Oaks2012} performed similar power analyses under three uniform
divergence-time priors as narrow as 0--5 coalescent units, and found the
method was able to detect multiple events among 22 when divergences were random
over millions of generations.
As recommended by \citet{Oaks2012}, it is important that investigators perform
power analyses to determine the method's power for their dataset, and decide if
\msb has sufficient power to address their hypotheses; in the case of the
Philippines dataset, it did not.
When doing so, it is important to consider what prior conditions are
relevant to their empirical system.
A divergence-time prior of 0--5 coalescent units is quite narrow considering
that this expresses the prior belief that all 22 taxon pairs diverged within
this window.
Certainly, it is a rare for there to be enough \emph{a priori} information to
be certain that all taxa diverged within the last 4$\globalpopsize$ generations
(i.e., 0--1 coalescent units).
Also, it seems unlikely that when such prior information is available that
being able to detect multiple divergences on the scale of hundreds of thousands
of generations will add much insight about the evolutionary history of the
taxa; assuming a rate of mutation consistent with nuclear loci ($1\e{-9}$),
this translates to a temporal resolution of 3 million generations or more.

As noted by \citet{Oaks2012}, inferring more than one divergence time
shared across all taxa does not confirm the method is working well when
analyzing data generated under random temporal variation in divergences (e.g.,
an inference of two divergence events could be biogeographically interesting
yet spurious).
Thus, it is important that investigators not limit their assessment of method's
power to only differentiating inferences of one event or more (i.e. $\numt{} =
1$ versus $\numt{} > 1$).
Rather, looking at the distribution of estimates, as in \citet{Oaks2012},
provides much more information about the behavior of the method.



\section*{The causes of support for models of co-divergence}
To determine how best to improve the behavior of \msb, it is important to
determine the mechanism by which broad uniform priors cause spurious support
for clustered models of divergence.
It is well established that vague priors can be problematic in Bayesian model
selection.
Models that integrate over more parameter space characterized by low
probability of producing the data and relatively high prior density will have
smaller marginal likelihoods \citep{Jeffreys1935,Lindley1957}.
Given the uniformly distributed priors on divergence times (and other nuisance
parameters) employed in \msb, models with more divergence parameters will be
forced to integrate over \emph{much} greater parameter space, all with equal
prior density, and much of it with low likelihood.
In light of this fundamental statistical issue, it is not surprising that the
method tends to support simple models.

However, \citet{Hickerson2013} conclude that the bias is due to insufficient
prior sampling.
They argue the widest of the three priors used by \citet{Oaks2012} would
infrequently produce samples with many independent young divergence times.
However, their argument assumes the gene divergence times presented
in \citet{Oaks2012} were estimated without error.
These estimates were intended to only provide a very rough comparison of the
gene divergence times across the 22 taxa.
These analyses assumed an arbitrary strict mutation rate of $2\e{-8}$ for all
taxa, and are, of course, subject to estimation error.
Furthermore, the branch-length units of the gene trees are in millions of
years, whereas the divergence time prior of \msb is in generations, thus
\citet{Hickerson2013} make the implicit assumption that all 22 Philippine taxa
have a generation time of one year.
The argument of \citet{Hickerson2013} that divergence time estimates of
\citet{Oaks2012} ``should set an upper bound on their prior for \divt{}'' seems
questionable, especially given our findings presented above regarding the
behavior of \msb when empirically informed uniform priors are employed.

Even if we assume the gene divergence times are estimated without error,
Hickerson et al.'s \citeyear{Hickerson2013} argument only applies to one of the
three different priors used by \citet{Oaks2012}.
The narrowest prior on divergence times used by \citet{Oaks2012} ($U(0, 5
mya)$) closely mirrors the range of estimates of gene-divergence times.
Applying Hickerson et al.'s (\citeyear{Hickerson2013}) probability sampling
probability argument demonstrates this prior is densely populated with samples
with large numbers of divergence parameters with values younger than the
estimated gene divergence estimates.
Thus, if insufficient prior sampling is to blame for the bias, it should be
much reduced under the narrow prior on \divt{}.
However, the magnitude of the bias is very similar across all three priors
explored by \citet{Oaks2012}.
\citet{Hickerson2013} point out a case where the narrow prior performs
slightly better (panel L of Figures S32, S37, and S38 of \citet{Oaks2012}).
However, it is important to note that these results suffered from a bug
in \msb, and there are many cases after \citet{Oaks2012} corrected the 
bug where the narrow prior performs slightly worse (see panels D--J of
Figures 3 and S12).

To disentangle whether model likelihoods or insufficient prior sampling is to
blame for the method's spurious support for simple models, we must look at the
different predictions made by these two phenomena.
One example, as discussed by \citet{Oaks2012}, is that insufficient prior
sampling should create large variance among posterior estimates, and thus it
should cause analyses to be highly sensitive to the number of samples drawn
from the prior.
Furthermore, if the sampling error causes bias toward models with less
parameter space, as suggested by \citet{Hickerson2013}, we expect to see
support for these models decrease as sampling increases.
\citet{Oaks2012} do not see such sensitivity when they compare prior sample
sizes of $2\e6$, $5\e6$, and $10^7$.

To explore this prediction further, we repeat the analysis of the Philippines
dataset under the intermediate prior used by \citet{Oaks2012} ($\divt{} \sim
U(0, 10)$, $\meanDescendantTheta{} \sim (0.0005, 0.04)$, $\ancestralTheta{}
\sim (0.0005, 0.02)$), using a very large prior sample size of $10^8$.
When we look at the trace of the estimates of the dispersion index of
divergence times (\vmratio{}) as the prior samples accumulate
(Figure~\ref{figSamplingError}) we see no trend in either the unadjusted or
GLM-regression-adjusted estimates, as predicted by \citet{Hickerson2013}.
While sampling error is always a reality, it does not appear to be playing a
large role in the results of \citet{Oaks2012} or presented above.

As discussed by \citet{Oaks2012}, a straightforward prediction if marginal
likelihoods are causing the preference for simple models is that the bias should
disappear as the model generating the data converges to the prior.
\citet{Oaks2012} tested this prediction by performing 100,000 simulations to
assess the model-choice behavior of \msb when the prior model is correct.
The results confirm the prediction as \msb actually tends to underestimate
the probability of the one-divergence model (see Figure 4 of
\citet{Oaks2012}).
We confirmed this same behavior for the model-averaging approach used by
\citet{Hickerson2013} (see SI text and Figure~\ref{figValidationMCBehavior}.
These results are not clearly predicted if insufficient sampling was causing
the bias.
Even when the prior is correct, due to the discrete uniform prior on
\numt{} implemented in \msb, models with larger numbers of divergence
events (and thus greater parameter space) will still be less densely
sampled than those with fewer divergence events \citep{Oaks2012}.
Thus, the results of the simulations of \citet{Oaks2012} are more consistent
with the fundamental statistical issue of the sensitivity of the marginal
likelihoods to priors.

Furthermore, the results presented herein demonstrate the model-averaging
approach of \citet{Hickerson2013} prefers models with narrower \divt{}
priors (Table~\ref{tabModelChoiceEmpirical} and
Figs.~\labelcref{figExclusionSimTau,figExclusionSimProb,figPowerNumExcluded,figPowerProbExclusion})
and models with fewer \divt{} parameters (Figs.~\ref{figPowerPsiMode} \&
\ref{figPowerOmegaProb}).
In all of these analyses, each of the prior models have the same number of
samples.
Thus, while sampling error will always be present in any numerical Bayesian
approximation method, insufficient sampling is an unlikely explanation for
support of prior models with less parameters space.
While analyses that sample each model proportional to their parameter space
could be explored, it is clear that the marginal likelihoods under broad
uniform priors on divergence times will be greater for models with fewer
divergence-time dimensions.




\section*{General thoughts on the model of \msb}
Our findings are not very surprising in light of difficult inference problem
\msb seeks to tackle.
To get a sense of this task, let us tally up all the free parameters of the
\msb model as applied to the dataset of \citet{Oaks2012}.
% While this approach is trivial to implement, it is not a trivial change
% to the \msb model.
% To better understand the reason for this, it might help to step back and get a
% sense of the scale of the \msb model.
% To do this, we will use the dataset of 22 vertebrate taxon pairs of
% \citet{Oaks2012} as an example.
% Following the model description and notation of \citet{Oaks2012}, let us tally
% up all of the free parameters in the \msb model.
Under the simplest model in
\msb (i.e., assuming no migration and
no intra-locus recombination), the number of parameters for each
taxon pair include:
The population sizes of the ancestral and descendant populations
(\ancestralTheta{}, \descendantTheta{1}{}, \descendantTheta{2}{}),
the magnitude of population contraction in each of the descendant
populations (\bottleScalar{1}{} and \bottleScalar{2}{}) and the
timing of these contractions (\bottleTime{}), and the $N-1$ node heights
(coalescent times) of the gene tree that gave rise to the $N$ gene
copies sampled from both populations of the pair.
% Because we only have a single locus for each taxon, there are no locus-specific
% \myTheta{}-scaling parameters (\locusMutationRateScalar{}) or
% \locusRateHetShapeParameter shape parameter for the gamma prior distribution on
% \locusMutationRateScalar{}.
Lastly, there are between one and 22 divergence time parameters \divt{} in
the vector \divtvector.
Overall, under the simplest model in \msb, for the dataset of \citet{Oaks2012}
with 22 taxon pairs, there are 581--602 free parameters (depending on the
number of divergence-time parameters in \divtvector).
Furthermore, under this rich model, the method is estimating the probability of
1002 divergence models \citep[i.e., the number of integer partitions of
$Y=22$;][]{Oaks2012}.

This is a very difficult inference problem, especially considering that all the
information in the sequence alignment of each taxon pair is distilled into four
summary statistics:
$\pi$ \citep{Tajima1983}, $\theta_W$ \citep{Watterson1975}, $\pi_{net}$
\citep{Takahata1985}, and $SD(\pi-\theta_W)$ \citep{Tajima1989}.
That gives us a total of 88 summary statistics (four from each of the 22 taxon
pairs), which contain minimal information about many of the $\approx 600$
parameters in the model.
More summary statistics can be used in \msb, but most are highly correlated
with these four statistics, and thus contribute little additional information
about the sequence data.
The large number of parameters and divergence models relative to summary
statistics is undoubtedly a key reason the method lacks robustness to prior
conditions.
Robustness is an important characteristic of a method to gauge its
applicability to real-world data, because we know the model and priors will be
wrong to some degree.

The model-averaging approach of \citet{Hickerson2013} adds an additional
dimension of model choice to the model.
They expand the model to sample over eight prior models.
This adds an additional free parameter to the model and, more importantly,
forces the model to sample over 8016 unique models.
While this approach is trivial to implement, it is a non-trivial extension of
the model.
Given the method's very large number of free parameters and models to sample
relative to the information used from the data, is likely a major reason the
method is so sensitive to priors, and tends to support models with the least
parameter space when used with uniform priors.
In theory, the model-averaging approach of \citet{Hickerson2013} is very
appealing.
It leverages a great strength of Bayesian statistical procedures, namely the
ability to obtain marginalized estimates that incorporate uncertainty in
nuisance parameters.
However, when averaging over both narrow-empirical and diffuse uniform priors
for a parameter that is expected to have a very non-uniform likelihood density,
and in the context of a model-choice method that is highly sensitive to priors,
it is not too surprising to find undesirable behavior.

The recommendations of \citet{Oaks2012} for mitigating the lack of robustness
of \msb are similar to those of \citet{Hickerson2013}, but avoid the
need for imposing an additional dimension of model choice.
\citet{Oaks2012} suggest that uniform priors may not be appropriate for many
parameters of the \msb model, and recommend the use of probability
distributions from the exponential family.
If we look at the prior distribution on divergence-time parameters imposed by
the model-averaging approach of \citet{Hickerson2013} we see it is a mixture of
overlapping uniforms with lower limits of zero (Figure~\ref{figMCTauPrior}).
This looks very much like an exponential distribution, except that in any state
of the model, all the divergence-time parameters are restricted to the hard
bounds of one of the uniform distributions.
Thus, it seems more appropriate to simply place a gamma prior (the exponential
being a special case of the gamma) on divergence times.
This would capture the prior uncertainty that \citet{Hickerson2013} are
suggesting for divergence times (Figure~\ref{figMCTauPrior}) while avoiding
costly model-averaging and the constraint that all divergence times must fall
within the hard bounds of the current model state.
It also would allow an investigator to place the majority of the prior density
in regions of parameter space they believe, \emph{a priori}, are most
plausible, but still capture uncertainty in the tails of distributions with low
density.
Most importantly, we have found the use of more flexible prior distributions in
place of uniforms improves the power of the method to detect temporal variation
in divergences and reduces the bias toward models of clustered divergences
\citep{Oaks2014dpp}.
% This seems like a natural solution to the problem of skewed marginal
% likelihoods and/or sampling error, which are both likely exacerbated by the
% fact that uniform distributions are the only choice of prior for many of the
% parameters of the \msb model \citep[\divt{}, \ancestralTheta{},
% \descendantTheta{1}{}, \descendantTheta{2}{}, \bottleTime{},
% \bottleScalar{1}{}, \bottleScalar{2}{}, \locusRateHetShapeParameter,
% \migrationRate{}, \recombinationRate;][]{Oaks2012}.

% Importantly, this strategy is easy to implement.
% Given that ABC methods only need to draw random values from prior distributions
% (hence there are no difficult prior and proposal ratios to calculate, etc.) it
% is easy to use the most appropriate distributions on parameters.
% Also, this approach reduces the temptation of empirically guided priors.
% The investigator can place the majority of the prior density in regions of
% parameter space they believe, \emph{a priori}, are most plausible, but still
% capture uncertainty in the tails of distributions with low density.
% As discussed in \citet{Oaks2012} the use of uniform priors necessitate
% broad priors to avoid excluding the truth \emph{a priori}.
% Thus, to avoid the behavior of the method under these uniform priors, using the
% data is a very tempting, albeit risky, option \citep{Hickerson2013}.
% More appropriate prior distributions would alleviate this issue.





\section*{Conclusions}
We demonstrate how the approximate Bayesian model choice method implemented in
\msb can be strongly biased away from models with greater parameter space.
As suggested by \citet{Oaks2012}, this is likely caused by the use of uniform
priors on most of the model's parameters.
Uniform distributions necessitate the use of broad priors that place high prior
density in unlikely regions of parameter space, less the risk of excluding the
truth \emph{a priori}.
These broad priors reduce the marginal likelihoods of models with greater parameter
space, either due to more divergence time parameters or broader prior
distributions on those parameters.
We show that the empirical Bayesian model-averaging approach of
\citet{Hickerson2013} does not mitigate this bias, but rather causes it to
manifest by sampling predominantly from models that may exclude the true values
of the parameters.

Whether or not one chooses to use empirically informed priors, our results
suggest that it is difficult to choose an uniformly distributed prior on
divergence times that is broad enough to confidently contain the true values of
parameters while being narrow enough to avoid bias toward models with less
parameter space.
While we do not assume ``that all previous \msb results are invalid,'' as
suggested by \citet{Hickerson2013}, we do conservatively recommend that the
common inference of temporally clustered divergences \citep{Barber2010,
    Bell2012, Carnaval2009, Chan2011, Daza2010, Hickerson2006, Huang2011,
Lawson2010, Leache2007, Plouviez2009, Stone2012, Voje2009}, when not
accompanied with the necessary analyses to assess the robustness and temporal
resolution of such results, should be treated with caution, because the method
has been shown to spuriously infer clustered divergences over a range of prior
conditions.
Fortunately, alternative prior probability distributions allow prior
uncertainty to be accommodated while avoiding excessive prior density in
regions of low likelihood, which greatly improves the power of the method
\citet{Oaks2014dpp}.

% Based on our results, we discourage the use of empirically informed uniform
% priors in Bayesian model choice, especially when combined with model-averaging
% \ldots


The work presented herein follows the principles of Open Notebook Science.
All aspects of the work were recorded in real-time via version-control software
and are publicly available at
\href{https://github.com/joaks1/msbayes-experiments}{https://github.com/joaks1/msbayes-experiments}.
All information necessary to reproduce our results is provided there.


\section*{Acknowledgments}
J.\ Oaks thanks the National Science Foundation for supporting this work (DEB
1011423 and DBI 1308885).
J.\ Oaks was also supported by the University of Kansas (KU) Office of Graduate
Studies, Society of Systematic Biologists, Sigma Xi Scientific Research
Society, KU Department of Ecology and Evolutionary Biology, and the KU
Biodiversity Institute.

\bibliography{../bib/references}
% \bibliography{references}

%% LIST OF FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\singlespacing

\renewcommand\listfigurename{Figure Captions}
\cftsetindents{fig}{0cm}{2.2cm}
\renewcommand\cftdotsep{\cftnodots}
\setlength\cftbeforefigskip{10pt}
\cftpagenumbersoff{fig}
\listoffigures


\end{linenumbers}

%% TABLES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\singlespacing

\input{reply-tables.tex}

\clearpage

%% FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\input{reply-figures.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SUPPORTING INFO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{page}{1}

\singlespacing

% PUT MAIN TEXT CITATION HERE
{
\renewcommand{\refname}{\noindent\MakeUppercase{\LARGE\sffamily\upshape supporting information}}
\begin{thebibliography}{1}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }

\bibitem[{Oaks et~al.(2014)Oaks, Sukumaran, Esselstyn, Linkem, Siler, Brown,
  and Holder}]{Oaks2013reply}
Oaks, J.~R., J.~Sukumaran, J.~A.\ Esselstyn, C.~W.\ Linkem, C.~D.\ Siler,
    R.~M.\ Brown, and M.~T.\ Holder,
  % 2013.
\newblock \msTitle
% \newblock Evolution 67:991--1010.

\end{thebibliography}
}

\doublespacing
\input{reply-si-main.tex}

\newpage
\singlespacing

\input{reply-si-figures.tex}

\end{document}

